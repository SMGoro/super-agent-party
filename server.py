# -- coding: utf-8 --
import hashlib
import importlib
import mimetypes
import pathlib
import sys
import traceback
import platform
import requests

from py.agent import add_tool_to_project_config, is_tool_allowed_by_project_config
sys.stdout.reconfigure(encoding='utf-8')
import base64
from datetime import datetime
import glob
from io import BytesIO
import io
import os
from pathlib import Path
import pickle
import socket
import sys
import tempfile
import httpx
import socket
import ipaddress
from urllib.parse import urlparse, urlunparse, urljoin
from urllib.robotparser import RobotFileParser
import websockets
from py.load_files import check_robots_txt, get_file_content, is_private_ip, sanitize_url
def fix_macos_environment():
    """
    ä¸“é—¨ä¿®å¤ macOS ä¸‹æ‰¾ä¸åˆ° node (nvm) å’Œ uv (python framework) çš„é—®é¢˜
    """
    if sys.platform != 'darwin':
        return

    user_home = Path.home()
    paths_to_add = []

    # ---------------------------------------------------------
    # 1. è‡ªåŠ¨å‘ç° NVM å®‰è£…çš„ Node.js
    # è·¯å¾„é€šå¸¸æ˜¯: ~/.nvm/versions/node/vX.X.X/bin
    # ---------------------------------------------------------
    nvm_path = user_home / ".nvm" / "versions" / "node"
    if nvm_path.exists():
        # è·å–æ‰€æœ‰ç‰ˆæœ¬æ–‡ä»¶å¤¹ (å¦‚ v20.19.5, v18.0.0)
        # ä½¿ç”¨ glob åŒ¹é…æ‰€æœ‰ v å¼€å¤´çš„æ–‡ä»¶å¤¹
        node_versions = sorted(nvm_path.glob("v*"), key=lambda p: p.name, reverse=True)
        
        # å°†æ‰€æœ‰ç‰ˆæœ¬çš„ bin ç›®å½•éƒ½åŠ å…¥ï¼Œæˆ–è€…åªåŠ æœ€æ–°çš„
        for version_dir in node_versions:
            bin_path = version_dir / "bin"
            if bin_path.exists():
                paths_to_add.append(str(bin_path))
                # å¦‚æœåªæƒ³ç”¨æœ€æ–°çš„ nodeï¼Œè¿™é‡Œå¯ä»¥ break
                # break 

    # ---------------------------------------------------------
    # 2. è‡ªåŠ¨å‘ç° Python Framework ä¸­çš„ uv
    # è·¯å¾„é€šå¸¸æ˜¯: /Library/Frameworks/Python.framework/Versions/X.X/bin
    # ---------------------------------------------------------
    py_framework_path = Path("/Library/Frameworks/Python.framework/Versions")
    if py_framework_path.exists():
        # æŸ¥æ‰¾æ‰€æœ‰ç‰ˆæœ¬ï¼Œå¦‚ 3.13, 3.12
        py_versions = py_framework_path.glob("*")
        for ver in py_versions:
            bin_path = ver / "bin"
            if bin_path.exists():
                paths_to_add.append(str(bin_path))

    # ---------------------------------------------------------
    # 3. è¡¥å…… macOS å¸¸è§çš„å…¶ä»–è·¯å¾„ (Homebrew, Cargo, Local)
    # uv ä¹Ÿç»å¸¸è¢«å®‰è£…åœ¨ .local/bin æˆ– .cargo/bin ä¸‹
    # ---------------------------------------------------------
    common_extras = [
        "/opt/homebrew/bin",           # Apple Silicon Mac Homebrew
        "/usr/local/bin",              # Intel Mac Homebrew
        str(user_home / ".local" / "bin"), # ç”¨æˆ·çº§å®‰è£…é€šå¸¸åœ¨è¿™é‡Œ
        str(user_home / ".cargo" / "bin"), # Rust å·¥å…·é“¾ (uv å¯èƒ½åœ¨è¿™é‡Œ)
    ]
    paths_to_add.extend(common_extras)

    # ---------------------------------------------------------
    # 4. å°†å‘ç°çš„è·¯å¾„æ³¨å…¥åˆ°å½“å‰è¿›ç¨‹çš„ç¯å¢ƒå˜é‡ä¸­
    # ---------------------------------------------------------
    current_path = os.environ.get("PATH", "")
    new_path_str = current_path
    
    # å°†æ–°è·¯å¾„åŠ åˆ°æœ€å‰é¢ (ä¼˜å…ˆçº§æœ€é«˜)
    for p in paths_to_add:
        if p and os.path.isdir(p):
            # é¿å…é‡å¤æ·»åŠ 
            if p not in new_path_str:
                new_path_str = p + os.pathsep + new_path_str
    
    # æ›´æ–°ç¯å¢ƒå˜é‡
    os.environ['PATH'] = new_path_str
    
    # (å¯é€‰) æ‰“å°è°ƒè¯•ä¿¡æ¯
    # print(f"Fixed macOS PATH. Added: {paths_to_add}")

# --- åœ¨ç¨‹åºæœ€å¼€å§‹çš„åœ°æ–¹è°ƒç”¨è¿™ä¸ªå‡½æ•° ---
fix_macos_environment()

def _fix_onnx_dll():
    if sys.platform == 'darwin':
        return
    # 1. æ‰¾åˆ° uv è™šæ‹Ÿç¯å¢ƒé‡Œçš„ onnxruntime
    spec = importlib.util.find_spec("onnxruntime")
    if spec is None or spec.origin is None:
        return          # æ²¡è£… onnxruntimeï¼Œéšå®ƒå»
    # DLL å°±åœ¨ site-packages/onnxruntime/capi é‡Œ
    dll_dir = pathlib.Path(spec.origin).with_name("capi")
    if not dll_dir.is_dir():
        return

    # 2. ç½®é¡¶æœç´¢è·¯å¾„
    os.environ["PATH"] = str(dll_dir) + os.pathsep + os.environ["PATH"]
    if hasattr(os, "add_dll_directory"):      # Python 3.8+
        os.add_dll_directory(str(dll_dir))

    # 3. å¦‚æœå·²ç»æœ‰äºº import è¿‡ onnxruntimeï¼Œæ¸…æ‰ç¼“å­˜
    for mod in list(sys.modules):
        if mod.startswith("onnxruntime"):
            del sys.modules[mod]

_fix_onnx_dll()

# åœ¨ç¨‹åºæœ€å¼€å§‹è®¾ç½®
if hasattr(sys, '_MEIPASS'):
    # æ‰“åŒ…åçš„ç¨‹åº
    os.environ['PYTHONPATH'] = sys._MEIPASS
    os.environ['PATH'] = sys._MEIPASS + os.pathsep + os.environ.get('PATH', '')
import asyncio
import copy
from functools import partial
import json
import re
import shutil
from fastapi import BackgroundTasks, Body, FastAPI, File, Form, HTTPException, UploadFile, WebSocket, Request, WebSocketDisconnect
from fastapi_mcp import FastApiMCP
import logging
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from openai import AsyncOpenAI
from pydantic import BaseModel
from fastapi import status
from fastapi.responses import JSONResponse, StreamingResponse,Response
import uuid
import time
from typing import Any, AsyncIterator, List, Dict,Optional, Tuple
import shortuuid
from py.mcp_clients import McpClient
from contextlib import asynccontextmanager
import asyncio
from concurrent.futures import ThreadPoolExecutor
import aiofiles
import argparse
from py.dify_openai_async import DifyOpenAIAsync

from py.get_setting import EXT_DIR, convert_to_opus_simple, load_covs, load_settings, save_covs,save_settings,clean_temp_files_task,base_path,configure_host_port,UPLOAD_FILES_DIR,AGENT_DIR,MEMORY_CACHE_DIR,KB_DIR,DEFAULT_VRM_DIR,USER_DATA_DIR,LOG_DIR,TOOL_TEMP_DIR
from py.llm_tool import get_image_base64,get_image_media_type
timetamp = time.time()
log_path = os.path.join(LOG_DIR, f"backend_{timetamp}.log")

logger = None

parser = argparse.ArgumentParser(description="Run the ASGI application server.")
parser.add_argument("--host", default="127.0.0.1", help="Host for the ASGI server, default is 127.0.0.1")
parser.add_argument("--port", type=int, default=3456, help="Port for the ASGI server, default is 3456")
args = parser.parse_args()
HOST = args.host
PORT = args.port

os.environ["no_proxy"] = "localhost,127.0.0.1"
local_timezone = None
settings = None
client = None
reasoner_client = None
HA_client = None
ChromeMCP_client = None
sql_client = None
mcp_client_list = {}
locales = {}
_TOOL_HOOKS = {}
ALLOWED_EXTENSIONS = [
  # åŠå…¬æ–‡æ¡£
    'doc', 'docx', 'ppt', 'pptx', 'xls', 'xlsx', 'pdf', 'pages', 
    'numbers', 'key', 'rtf', 'odt', 'epub',
  
  # ç¼–ç¨‹å¼€å‘
  'js', 'ts', 'py', 'java', 'c', 'cpp', 'h', 'hpp', 'go', 'rs',
  'swift', 'kt', 'dart', 'rb', 'php', 'html', 'css', 'scss', 'less',
  'vue', 'svelte', 'jsx', 'tsx', 'json', 'xml', 'yml', 'yaml', 
  'sql', 'sh',
  
  # æ•°æ®é…ç½®
  'csv', 'tsv', 'txt', 'md', 'log', 'conf', 'ini', 'env', 'toml'
]
ALLOWED_IMAGE_EXTENSIONS = ['png', 'jpg', 'jpeg', 'gif', 'webp', 'bmp']

ALLOWED_VIDEO_EXTENSIONS = ['mp4', 'avi', 'mov', 'wmv', 'flv', 'mkv', 'webm', '3gp', 'm4v']

# 1. å…ˆæ¸…ç©ºç³»ç»Ÿå¯èƒ½ç»™é”™çš„æ¡ç›®
for ext in ("js", "mjs", "css", "html", "htm", "json", "xml", "map", "svg"):
    mimetypes.add_type("", f".{ext}")          # å…ˆåˆ æ‰
# 2. å†å†™æ­»æˆ‘ä»¬æƒ³è¦çš„
mimetypes.add_type("application/javascript", ".js")
mimetypes.add_type("application/javascript", ".mjs")
mimetypes.add_type("text/css", ".css")
mimetypes.add_type("text/html", ".html")
mimetypes.add_type("text/html", ".htm")
mimetypes.add_type("application/json", ".json")
mimetypes.add_type("application/xml", ".xml")
mimetypes.add_type("application/json", ".map")
mimetypes.add_type("image/svg+xml", ".svg")


def _get_target_message(message, role):
    """
    æ ¹æ®è§’è‰²è·å–ç›®æ ‡æ¶ˆæ¯
    
    å‚æ•°:
        message (list): æ¶ˆæ¯åˆ—è¡¨å¼•ç”¨
        role (str): è¦æ“ä½œçš„è§’è‰²ï¼Œå¯é€‰å€¼: 'user', 'assistant', 'system'
    
    è¿”å›:
        dict: ç›®æ ‡æ¶ˆæ¯å­—å…¸
    """
    # éªŒè¯è¾“å…¥å‚æ•°
    if not isinstance(message, list):
        raise TypeError("messageå¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹")
    
    if role not in ['user', 'assistant', 'system']:
        raise ValueError("roleå¿…é¡»æ˜¯'user'æˆ–'assistant'æˆ–'system'")
    
    target_message = None
    
    # æ ¹æ®roleå†³å®šè¦æ“ä½œçš„å¯¹è±¡
    if role == 'user':
        # æŸ¥æ‰¾æœ€åä¸€ä¸ªroleä¸º'user'çš„æ¶ˆæ¯
        for msg in reversed(message):
            if isinstance(msg, dict) and msg['role'] == 'user':
                target_message = msg
                break
    elif role == 'assistant':
        # æ£€æŸ¥æœ€åä¸€ä¸ªæ¶ˆæ¯
        if message and message[-1]['role'] == 'assistant':
            target_message = message[-1]
        else:
            # å¦‚æœæœ€åä¸€ä¸ªæ¶ˆæ¯ä¸æ˜¯assistantï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
            new_assistant_msg = {'role': 'assistant', 'content': ''}
            message.append(new_assistant_msg)
            target_message = new_assistant_msg
    elif role == 'system':
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªroleä¸º'system'çš„æ¶ˆæ¯
        if message and message[0]['role'] == 'system':
            target_message = message[0]
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°systemæ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
            target_message = {'role': 'system', 'content': ''}
            message.insert(0, target_message)
    
    return target_message

def content_append(message, role, content):
    """
    å°†contentæ·»åŠ åˆ°æŒ‡å®šroleæ¶ˆæ¯çš„æœ«å°¾
    """
    target_message = _get_target_message(message, role)
    if target_message:
        current_content = target_message.get('content', '')
        target_message['content'] = current_content + content

def content_prepend(message, role, content):
    """
    å°†contentæ·»åŠ åˆ°æŒ‡å®šroleæ¶ˆæ¯çš„å‰é¢
    """
    target_message = _get_target_message(message, role)
    if target_message:
        current_content = target_message.get('content', '')
        target_message['content'] = content + current_content

def content_replace(message, role, content):
    """
    ç”¨contentæ›¿æ¢æŒ‡å®šroleæ¶ˆæ¯çš„å†…å®¹
    """
    target_message = _get_target_message(message, role)
    if target_message:
        target_message['content'] = content

def content_new(message, role, content):
    """
    ç”¨contentæ›¿æ¢æŒ‡å®šroleæ¶ˆæ¯çš„å†…å®¹
    """
    message.append({'role': role, 'content': content})

configure_host_port(args.host, args.port)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # 1. å‡†å¤‡æ‰€æœ‰ç‹¬ç«‹çš„åˆå§‹åŒ–ä»»åŠ¡
    from py.get_setting import init_db, init_covs_db
    from tzlocal import get_localzone
    asyncio.create_task(clean_temp_files_task())
    # å°†æ‰€æœ‰ä¸ä¾èµ– Settings çš„ä»»åŠ¡å¹¶è¡ŒåŒ–
    # æ¯”å¦‚ï¼šæ•°æ®åº“åˆå§‹åŒ–ã€åŠ è½½æœ¬åœ°åŒ–æ–‡ä»¶ã€è·å–æ—¶åŒº
    init_db_task = init_db()
    init_covs_task = init_covs_db()
    load_locales_task = asyncio.to_thread(lambda: json.load(open(base_path + "/config/locales.json", "r", encoding="utf-8")))
    settings_task = load_settings() # è¿™æ˜¯ä¸€ä¸ª async ä»»åŠ¡
    timezone_task = asyncio.to_thread(get_localzone)
    
    # 2. å¹¶è¡Œæ‰§è¡Œè¿™äº›è€—æ—¶æ“ä½œ
    results = await asyncio.gather(
        init_db_task, 
        init_covs_task, 
        load_locales_task, 
        settings_task, 
        timezone_task
    )
    
    # 3. è§£åŒ…ç»“æœ
    # init_db å’Œ init_covs æ²¡æœ‰è¿”å›å€¼(None)
    global settings, client, reasoner_client, mcp_client_list, local_timezone, logger, locales
    _, _, locales, settings, local_timezone = results
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
    timestamp = time.time()
    log_path = os.path.join(LOG_DIR, f"backend_{timestamp}.log")
    
    # åˆ›å»ºå¹¶é…ç½®logger
    logger = logging.getLogger("app")
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        logger.addHandler(handler)
    
    # æµ‹è¯•æ—¥å¿—
    logger.info("===== æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ =====")
    logger.info(f"æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_path}")

    with open(base_path + "/config/locales.json", "r", encoding="utf-8") as f:
        locales = json.load(f)

    try:
        from py.sherpa_asr import _get_recognizer
        asyncio.get_running_loop().run_in_executor(None, _get_recognizer)
    except Exception as e:
        logger.error(f"å°è¯•å¯åŠ¨sherpaå¤±è´¥: {e}")
        pass

    vendor = 'OpenAI'
    for modelProvider in settings['modelProviders']: 
        if modelProvider['id'] == settings['selectedProvider']:
            vendor = modelProvider['vendor']
            break
    client_class = AsyncOpenAI
    if vendor == 'Dify':
        client_class = DifyOpenAIAsync
    reasoner_vendor = 'OpenAI'
    for modelProvider in settings['modelProviders']: 
        if modelProvider['id'] == settings['reasoner']['selectedProvider']:
            reasoner_vendor = modelProvider['vendor']
            break
    reasoner_client_class = AsyncOpenAI
    if reasoner_vendor == 'Dify':
        reasoner_client_class = DifyOpenAIAsync
    if settings:
        client = client_class(api_key=settings['api_key'], base_url=settings['base_url'])
        reasoner_client = reasoner_client_class(api_key=settings['reasoner']['api_key'], base_url=settings['reasoner']['base_url'])
        if settings["systemSettings"]["proxy"] and settings["systemSettings"]["proxyMode"] == "manual":
            # è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡
            os.environ['http_proxy'] = settings["systemSettings"]["proxy"].strip()
            os.environ['https_proxy'] = settings["systemSettings"]["proxy"].strip()
        elif settings["systemSettings"]["proxyMode"] == "system":
            os.environ.pop('http_proxy', None)
            os.environ.pop('https_proxy', None)
        else:
            os.environ['http_proxy'] = ""
            os.environ['https_proxy'] = ""
    else:
        client = client_class()
        reasoner_client = reasoner_client_class()
    mcp_init_tasks = []

    async def init_mcp_with_timeout(
        server_name: str,
        server_config: dict,
        *,
        timeout: float = 6.0,
        max_wait_failure: float = 5.0
    ) -> Tuple[str, Optional["McpClient"], Optional[str]]:
        """
        åˆå§‹åŒ–å•ä¸ª MCP æœåŠ¡å™¨ï¼Œå¸¦è¶…æ—¶ä¸å¤±è´¥å›è°ƒåŒæ­¥ã€‚
        è¿”å› (server_name, mcp_client or None, error or None)
        """
        # 1. å¦‚æœé…ç½®é‡Œç›´æ¥ç¦ç”¨ï¼Œç›´æ¥è¿”å›
        if server_config.get("disabled"):
            return server_name, None, "disabled"

        # 2. é¢„åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
        mcp_client = mcp_client_list.get(server_name) or McpClient()
        mcp_client_list[server_name] = mcp_client

        # 3. ç”¨äºåŒæ­¥å›è°ƒçš„äº‹ä»¶
        failure_event = asyncio.Event()
        first_error: Optional[str] = None

        async def on_failure(msg: str) -> None:
            nonlocal first_error
            # ä»…ç¬¬ä¸€æ¬¡ç”Ÿæ•ˆ
            if first_error is not None:
                return
            first_error = msg
            logger.error("on_failure: %s -> %s", server_name, msg)

            # è®°å½•åˆ° settings
            settings.setdefault("mcpServers", {}).setdefault(server_name, {})
            settings["mcpServers"][server_name]["disabled"] = True
            settings["mcpServers"][server_name]["processingStatus"] = "server_error"

            # æŠŠå½“å‰å®¢æˆ·ç«¯æ ‡ä¸ºç¦ç”¨å¹¶å…³é—­
            mcp_client.disabled = True
            await mcp_client.close()
            failure_event.set()          # å”¤é†’ä¸»åç¨‹

        # 4. çœŸæ­£åˆå§‹åŒ–
        init_task = asyncio.create_task(
            mcp_client.initialize(
                server_name,
                server_config,
                on_failure_callback=on_failure
            )
        )

        try:
            # 4.1 å…ˆç­‰åˆå§‹åŒ–æœ¬èº«ï¼ˆæœ€å¤š timeout ç§’ï¼‰
            await asyncio.wait_for(init_task, timeout=timeout)

            # 4.2 åˆå§‹åŒ–æ²¡æŠ›å¼‚å¸¸ï¼Œå†ç­‰å¾…çœ‹ä¼šä¸ä¼šè§¦å‘ on_failure
            #     å¦‚æœ on_failure å·²ç»æ‰§è¡Œè¿‡ï¼Œevent ä¼šè¢«ç«‹å³ set
            try:
                await asyncio.wait_for(failure_event.wait(), timeout=max_wait_failure)
            except asyncio.TimeoutError:
                # 5 ç§’å†…æ²¡æ”¶åˆ°å¤±è´¥å›è°ƒï¼Œè®¤ä¸ºæˆåŠŸ
                pass

            # 5. æœ€ç»ˆåˆ¤å®š
            if first_error:
                return server_name, None, first_error
            return server_name, mcp_client, None

        except asyncio.TimeoutError:
            # åˆå§‹åŒ–é˜¶æ®µå°±è¶…æ—¶
            logger.error("%s initialize timed out", server_name)
            return server_name, None, "timeout"

        except Exception as exc:
            # ä»»ä½•å…¶ä»–å¼‚å¸¸
            logger.exception("%s initialize crashed", server_name)
            return server_name, None, str(exc)

        finally:
            # å¦‚æœä»»åŠ¡è¿˜æ´»ç€ï¼Œä¿é™©èµ·è§å–æ¶ˆæ‰
            if not init_task.done():
                init_task.cancel()
                try:
                    await init_task
                except asyncio.CancelledError:
                    pass

    async def check_results():
        """åå°æ”¶é›†ä»»åŠ¡ç»“æœ"""
        logger.info("check_results started with %d tasks", len(mcp_init_tasks))
        for task in asyncio.as_completed(mcp_init_tasks):
            server_name, mcp_client, error = await task
            if error:
                logger.error(f"MCP client {server_name} initialization failed: {error}")
                settings['mcpServers'][server_name]['disabled'] = True
                settings['mcpServers'][server_name]['processingStatus'] = 'server_error'
                mcp_client_list[server_name] = McpClient()
                mcp_client_list[server_name].disabled = True
            else:
                logger.info(f"MCP client {server_name} initialized successfully")
                mcp_client_list[server_name] = mcp_client
        await save_settings(settings)  # æ‰€æœ‰ä»»åŠ¡å®Œæˆåç»Ÿä¸€ä¿å­˜
        await broadcast_settings_update(settings)  # æ‰€æœ‰ä»»åŠ¡å®Œæˆåç»Ÿä¸€å¹¿æ’­

    if settings and settings.get('mcpServers'):
        # åªæœ‰å½“æœ‰é…ç½®æ—¶æ‰åˆ›å»ºä»»åŠ¡
        mcp_init_tasks = [
            asyncio.create_task(init_mcp_with_timeout(server_name, server_config))
            for server_name, server_config in settings['mcpServers'].items()
        ]
        
        if mcp_init_tasks:  # åªåœ¨æœ‰ä»»åŠ¡æ—¶å¯åŠ¨åå°æ”¶é›†
            asyncio.create_task(check_results())
    else:
        mcp_init_tasks = []
        # ç›´æ¥å¹¿æ’­ç©ºé…ç½®
        asyncio.create_task(broadcast_settings_update(settings or {}))
    yield

# WebSocketç«¯ç‚¹å¢åŠ è¿æ¥ç®¡ç†
active_connections = []
# æ–°å¢å¹¿æ’­å‡½æ•°
async def broadcast_settings_update(settings):
    """å‘æ‰€æœ‰WebSocketè¿æ¥æ¨é€é…ç½®æ›´æ–°"""
    for connection in active_connections:  # éœ€è¦ç»´æŠ¤å…¨å±€è¿æ¥åˆ—è¡¨
        try:
            await connection.send_json({
                "type": "settings_update",
                "data": settings  # ç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„æœ€æ–°é…ç½®
            })
            print("Settings broadcasted to client")
        except Exception as e:
            logger.error(f"Broadcast failed: {e}")


app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.middleware("http")
async def cors_options_workaround(request: Request, call_next):
    if request.method == "OPTIONS":
        return Response(
            status_code=200,
            headers={
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "*",
                "Access-Control-Allow-Headers": "*",
                "Access-Control-Max-Age": "86400",   # é¢„æ£€ç¼“å­˜ 24 h
            }
        )
    return await call_next(request)

async def t(text: str) -> str:
    global locales
    settings = await load_settings()
    target_language = settings["currentLanguage"]
    return locales[target_language].get(text, text)


# å…¨å±€å­˜å‚¨å¼‚æ­¥å·¥å…·çŠ¶æ€
async_tools = {}
async_tools_lock = asyncio.Lock()

async def execute_async_tool(tool_id: str, tool_name: str, args: dict, settings: dict,user_prompt: str):
    try:
        results = await dispatch_tool(tool_name, args, settings)
        if isinstance(results, AsyncIterator):
            buffer = []
            async for chunk in results:
                buffer.append(chunk)
            results = "".join(buffer)
                
        if tool_name in ["query_knowledge_base"] and type(results) == list:
            from py.know_base import rerank_knowledge_base
            if settings["KBSettings"]["is_rerank"]:
                results = await rerank_knowledge_base(user_prompt,results)
            results = json.dumps(results, ensure_ascii=False, indent=4)
        async with async_tools_lock:
            async_tools[tool_id] = {
                "status": "completed",
                "result": results,
                "name": tool_name,
                "parameters": args,
            }
    except Exception as e:
        async with async_tools_lock:
            async_tools[tool_id] = {
                "status": "error",
                "result": str(e),
                "name": tool_name,
                "parameters": args,
            }

async def get_image_content(image_url: str) -> str:
    import hashlib
    settings = await load_settings()
    base64_image = await get_image_base64(image_url)
    media_type = await get_image_media_type(image_url)
    url= f"data:{media_type};base64,{base64_image}"
    image_hash = hashlib.md5(image_url.encode()).hexdigest()
    content = ""
    if settings['vision']['enabled']:
        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt")):
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "r", encoding='utf-8') as f:
                content += f"\n\nå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
        else:
            images_content = [{"type": "text", "text": "è¯·ä»”ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…å«å›¾ç‰‡ä¸­å¯èƒ½å­˜åœ¨çš„æ–‡å­—ã€æ•°å­—ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"},{"type": "image_url", "image_url": {"url": url}}]
            client = AsyncOpenAI(api_key=settings['vision']['api_key'],base_url=settings['vision']['base_url'])
            response = await client.chat.completions.create(
                model=settings['vision']['model'],
                messages = [{"role": "user", "content": images_content}],
                temperature=settings['vision']['temperature'],
            )
            content = f"\n\nnå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(response.choices[0].message.content)+"\n\n"
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "w", encoding='utf-8') as f:
                f.write(str(response.choices[0].message.content))
    else:           
        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt")):
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "r", encoding='utf-8') as f:
                content += f"\n\nnå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
        else:
            images_content = [{"type": "text", "text": "è¯·ä»”ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…å«å›¾ç‰‡ä¸­å¯èƒ½å­˜åœ¨çš„æ–‡å­—ã€æ•°å­—ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"},{"type": "image_url", "image_url": {"url": url}}]
            client = AsyncOpenAI(api_key=settings['api_key'],base_url=settings['base_url'])
            response = await client.chat.completions.create(
                model=settings['model'],
                messages = [{"role": "user", "content": images_content}],
                temperature=settings['temperature'],
            )
            content = f"\n\nnå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(response.choices[0].message.content)+"\n\n"
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "w", encoding='utf-8') as f:
                f.write(str(response.choices[0].message.content))
    return content

async def dispatch_tool(tool_name: str, tool_params: dict, settings: dict) -> str | List | AsyncIterator[str] | None :
    global mcp_client_list,_TOOL_HOOKS,HA_client,ChromeMCP_client,sql_client
    print("dispatch_tool",tool_name,tool_params)
    
    # ==================== 1. å¯¼å…¥æ‰€æœ‰å·¥å…·å‡½æ•° ====================
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        Bing_search_async,
        Google_search_async,
        Brave_search_async,
        Exa_search_async,
        Serper_search_async,
        bochaai_search_async,
        jina_crawler_async,
        Crawl4Ai_search_async, 
    )
    from py.know_base import query_knowledge_base
    from py.agent_tool import agent_tool_call
    from py.a2a_tool import a2a_tool_call
    from py.llm_tool import custom_llm_tool
    from py.pollinations import pollinations_image,openai_image,openai_chat_image
    from py.load_files import get_file_content
    from py.code_interpreter import e2b_code_async,local_run_code_async
    from py.custom_http import fetch_custom_http
    from py.comfyui_tool import comfyui_tool_call
    from py.utility_tools import (
        time_async,
        get_weather_async,
        get_location_coordinates_async,
        get_weather_by_city_async,
        get_wikipedia_summary_and_sections,
        get_wikipedia_section_content,
        search_arxiv_papers
    )
    from py.autoBehavior import auto_behavior

    # Docker CLI å·¥å…·ï¼ˆåŸæœ‰ï¼‰
    from py.cli_tool import (
        claude_code_async,
        qwen_code_async,
        docker_sandbox_async,
        list_files_tool,
        read_file_tool,
        search_files_tool,
        edit_file_tool,
        edit_file_patch_tool, 
        glob_files_tool,       
        todo_write_tool, 
        manage_processes_tool,
        docker_manage_ports_tool,
    )

    # æ–°å¢ï¼šæœ¬åœ°ç¯å¢ƒ CLI å·¥å…·ï¼ˆå‡è®¾ä¿å­˜åœ¨ py/local_cli_tool.pyï¼‰
    from py.cli_tool import (
        bash_tool_local,           # æœ¬åœ° bash æ‰§è¡Œï¼ˆå¯¹åº” docker_sandbox_asyncï¼‰
        list_files_tool_local,     # æœ¬åœ°æ–‡ä»¶åˆ—è¡¨
        read_file_tool_local,      # æœ¬åœ°æ–‡ä»¶è¯»å–
        search_files_tool_local,   # æœ¬åœ°æ–‡ä»¶æœç´¢
        edit_file_tool_local,      # æœ¬åœ°æ–‡ä»¶å†™å…¥
        edit_file_patch_tool_local,# æœ¬åœ°ç²¾ç¡®æ›¿æ¢
        glob_files_tool_local,     # æœ¬åœ° glob æŸ¥æ‰¾
        todo_write_tool_local,     # æœ¬åœ°ä»»åŠ¡ç®¡ç†
        local_net_tool,            # æœ¬åœ°ç½‘ç»œå·¥å…·
    )

    from py.cdp_tool import (
        list_pages,
        navigate_page,
        new_page,
        close_page,
        select_page,
        take_snapshot,
        wait_for,
        click,
        fill,
        hover,
        press_key,
        evaluate_script,
        take_screenshot,
        fill_form,
        drag,
        handle_dialog
    )
    from py.random_topic import get_random_topics,get_categories

    # ==================== 2. å®šä¹‰å·¥å…·æ˜ å°„è¡¨ ====================
    _TOOL_HOOKS = {
        "DDGsearch_async": DDGsearch_async,
        "searxng_async": searxng_async,
        "Tavily_search_async": Tavily_search_async,
        "query_knowledge_base": query_knowledge_base,
        "jina_crawler_async": jina_crawler_async,
        "Crawl4Ai_search_async": Crawl4Ai_search_async,
        "agent_tool_call": agent_tool_call,
        "a2a_tool_call": a2a_tool_call,
        "custom_llm_tool": custom_llm_tool,
        "pollinations_image":pollinations_image,
        "get_file_content":get_file_content,
        "get_image_content": get_image_content,
        "e2b_code_async": e2b_code_async,
        "local_run_code_async": local_run_code_async,
        "openai_image": openai_image,
        "openai_chat_image":openai_chat_image,
        "Bing_search_async": Bing_search_async,
        "Google_search_async": Google_search_async,
        "Brave_search_async": Brave_search_async,
        "Exa_search_async": Exa_search_async,
        "Serper_search_async": Serper_search_async,
        "bochaai_search_async": bochaai_search_async,
        "comfyui_tool_call": comfyui_tool_call,
        "time_async": time_async,
        "get_weather_async": get_weather_async,
        "get_location_coordinates_async": get_location_coordinates_async,
        "get_weather_by_city_async":get_weather_by_city_async,
        "get_wikipedia_summary_and_sections": get_wikipedia_summary_and_sections,
        "get_wikipedia_section_content": get_wikipedia_section_content,
        "search_arxiv_papers": search_arxiv_papers,
        "auto_behavior": auto_behavior,
        "claude_code_async": claude_code_async,
        "qwen_code_async": qwen_code_async,
        "list_pages": list_pages,
        "new_page": new_page,
        "close_page": close_page,
        "select_page": select_page,
        "navigate_page": navigate_page,
        "take_snapshot": take_snapshot,
        "click": click,
        "fill": fill,
        "evaluate_script": evaluate_script,
        "take_screenshot": take_screenshot,
        "hover": hover,
        "press_key": press_key,
        "wait_for": wait_for,
        "fill_form":fill_form,
        "drag": drag,
        "handle_dialog": handle_dialog,
        "get_random_topics":get_random_topics,
        "get_categories":get_categories,
        
        # Docker Sandbox ç›¸å…³å·¥å…·ï¼ˆåŸæœ‰ï¼‰
        "docker_sandbox_async": docker_sandbox_async,
        "list_files_tool": list_files_tool,
        "read_file_tool": read_file_tool,
        "search_files_tool": search_files_tool,
        "edit_file_tool": edit_file_tool,
        "edit_file_patch_tool": edit_file_patch_tool,
        "glob_files_tool": glob_files_tool,
        "todo_write_tool": todo_write_tool,
        "manage_processes_tool": manage_processes_tool,
        "docker_manage_ports_tool": docker_manage_ports_tool,
        
        # æœ¬åœ°ç¯å¢ƒå·¥å…·ï¼ˆæ–°å¢ï¼‰- ä¸ Docker ç‰ˆæœ¬åŠŸèƒ½ç›¸åŒä½†æ“ä½œæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
        "bash_tool_local": bash_tool_local,                     # æœ¬åœ° bash æ‰§è¡Œ
        "list_files_tool_local": list_files_tool_local,         # æœ¬åœ°æ–‡ä»¶åˆ—è¡¨
        "read_file_tool_local": read_file_tool_local,           # æœ¬åœ°æ–‡ä»¶è¯»å–
        "search_files_tool_local": search_files_tool_local,     # æœ¬åœ°æ–‡ä»¶æœç´¢
        "edit_file_tool_local": edit_file_tool_local,           # æœ¬åœ°æ–‡ä»¶å†™å…¥
        "edit_file_patch_tool_local": edit_file_patch_tool_local,  # æœ¬åœ°ç²¾ç¡®æ›¿æ¢
        "glob_files_tool_local": glob_files_tool_local,         # æœ¬åœ° glob æŸ¥æ‰¾
        "todo_write_tool_local": todo_write_tool_local,         # æœ¬åœ°ä»»åŠ¡ç®¡ç†
        "local_net_tool": local_net_tool,                       # æœ¬åœ°ç½‘ç»œå·¥å…·
    }
    
    # ==================== 3. æƒé™æ‹¦æˆªé€»è¾‘ (Human-in-the-loop) ====================
    # å®šä¹‰å—æ§çš„æ•æ„Ÿå·¥å…·åˆ—è¡¨
    # è¿™äº›å·¥å…·åœ¨æ‰§è¡Œå‰éœ€è¦æ£€æŸ¥æƒé™é…ç½® (.party/config.json æˆ– å…¨å±€è®¾ç½®)
    SENSITIVE_TOOLS = [
        "docker_sandbox_async",
        "edit_file_tool",
        "edit_file_patch_tool",   
        "todo_write_tool",        
        "bash_tool_local",
        "edit_file_tool_local",
        "edit_file_patch_tool_local",
        "todo_write_tool_local",
        "manage_processes_tool",
        "docker_manage_ports_tool",
        "local_net_tool",
    ]
    
    # åªæœ‰å½“è°ƒç”¨çš„å·¥å…·å±äºæ•æ„Ÿå·¥å…·åˆ—è¡¨æ—¶æ‰è¿›è¡Œæ‹¦æˆªæ£€æŸ¥
    if tool_name in SENSITIVE_TOOLS:
        
        # è·å–ç›¸å…³é…ç½®
        cli_settings = settings.get("CLISettings", {})
        cwd = cli_settings.get("cc_path")
        # ä¿®å¤ï¼šlocal ç¯å¢ƒåº”è¯¥ä» localEnvSettings è¯»å–æƒé™æ¨¡å¼
        engine = cli_settings.get("engine", "")
        
        if engine == "local":
            env_settings = settings.get("localEnvSettings", {})
        else:
            env_settings = settings.get("dsSettings", {})
        
        permission_mode = env_settings.get("permissionMode", "default")
        
        is_allowed = False

        # --- è§„åˆ™ A: å…¨å±€ YOLO æ¨¡å¼ (Bypass Permissions) ---
        if permission_mode == "yolo":
            is_allowed = True
            
        # --- è§„åˆ™ B: è‡ªåŠ¨æ‰¹å‡†æ¨¡å¼ (Accept Edits) ---
        # å…è®¸æ–‡ä»¶ç¼–è¾‘ç±»å·¥å…·ï¼ˆåŒ…æ‹¬å…¨é‡å†™å…¥ã€ç²¾ç¡®æ›¿æ¢ã€ä»»åŠ¡ç®¡ç†ï¼‰
        # ä½†ä¾ç„¶æ‹¦æˆªç»ˆç«¯å‘½ä»¤ï¼ˆdocker/bashï¼‰
        elif permission_mode == "auto-approve":
            if tool_name in ["edit_file_tool", "edit_file_patch_tool", "todo_write_tool", "edit_file_tool_local", "edit_file_patch_tool_local", "todo_write_tool_local"]:
                is_allowed = True
            # docker/bash ç­‰å±é™©å‘½ä»¤åœ¨æ­¤æ¨¡å¼ä¸‹ä¾ç„¶é»˜è®¤æ‹¦æˆªï¼Œé™¤éåœ¨é¡¹ç›®ç™½åå•ä¸­
        
        # --- è§„åˆ™ C: é»˜è®¤æ¨¡å¼ (Default) ---
        # é»˜è®¤å…¨éƒ¨æ‹¦æˆª
        
        # --- è§„åˆ™ D: é¡¹ç›®çº§ç™½åå•è¦†ç›– (Project Config Override) ---
        # å¦‚æœä»¥ä¸Šè§„åˆ™æœªé€šè¿‡ï¼Œæ£€æŸ¥ .party/config.json
        # å¦‚æœç”¨æˆ·ä¹‹å‰ç‚¹å‡»è¿‡ "Allow Always"ï¼Œè¿™é‡Œä¼šè¿”å› True
        if not is_allowed and cwd:
            if is_tool_allowed_by_project_config(cwd, tool_name):
                is_allowed = True
                print(f"[Permission] Tool '{tool_name}' allowed by project config.")

        # --- æœ€ç»ˆåˆ¤å®š ---
        if not is_allowed:
            # è¿”å›å‰ç«¯ç‰¹å®šçš„ JSON ç»“æ„ï¼Œè§¦å‘å®¡æ‰¹ UI
            print(f"[Permission] Blocked '{tool_name}', requesting approval.")
            return json.dumps({
                "type": "approval_required",
                "tool_name": tool_name,
                "tool_params": tool_params,
                "permission_mode": permission_mode,
                "cwd": cwd
            }, ensure_ascii=False)

    # ==================== 4. å¸¸è§„å·¥å…·å¤„ç†é€»è¾‘ (åŸæœ‰ä»£ç ) ====================

    if "multi_tool_use." in tool_name:
        tool_name = tool_name.replace("multi_tool_use.", "")
        
    if "custom_http_" in tool_name:
        tool_name = tool_name.replace("custom_http_", "")
        print(tool_name)
        settings_custom_http = settings['custom_http']
        for custom in settings_custom_http:
            if custom['name'] == tool_name:
                tool_custom_http = custom
                break
        method = tool_custom_http['method']
        url = tool_custom_http['url']
        headers = tool_custom_http['headers']
        result = await fetch_custom_http(method, url, headers, tool_params)
        return str(result)
        
    if "comfyui_" in tool_name:
        tool_name = tool_name.replace("comfyui_", "")
        text_input = tool_params.get('text_input', None)
        text_input_2 = tool_params.get('text_input_2', None)
        image_input = tool_params.get('image_input', None)
        image_input_2 = tool_params.get('image_input_2', None)
        print(tool_name)
        result = await comfyui_tool_call(tool_name, text_input, image_input,text_input_2,image_input_2)
        return str(result)
        
    if settings["HASettings"]["enabled"]:
        ha_tool_list = HA_client._tools
        if tool_name in ha_tool_list:
            result = await HA_client.call_tool(tool_name, tool_params)
            if isinstance(result,str):
                return result
            elif hasattr(result, 'model_dump'):
                return str(result.model_dump())
            else:
                return str(result)
                
    if settings['chromeMCPSettings']['enabled'] and settings['chromeMCPSettings']['type']=='external':
        Chrome_tool_list = ChromeMCP_client._tools
        if tool_name in Chrome_tool_list:
            result = await ChromeMCP_client.call_tool(tool_name, tool_params)
            if isinstance(result,str):
                return result
            elif hasattr(result, 'model_dump'):
                return str(result.model_dump())
            else:
                return str(result)
                
    if settings["sqlSettings"]["enabled"]:
        sql_tool_list = sql_client._tools
        if tool_name in sql_tool_list:
            result = await sql_client.call_tool(tool_name, tool_params)
            if isinstance(result,str):
                return result
            elif hasattr(result, 'model_dump'):
                return str(result.model_dump())
            else:
                return str(result)
                
    if tool_name not in _TOOL_HOOKS:
        for server_name, mcp_client in mcp_client_list.items():
            if tool_name in mcp_client._conn.tools:
                result = await mcp_client.call_tool(tool_name, tool_params)
            if isinstance(result,str):
                return result
            elif hasattr(result, 'model_dump'):
                return str(result.model_dump())
            else:
                return str(result)
        return None
        
    tool_call = _TOOL_HOOKS[tool_name]
    try:
        ret_out = await tool_call(**tool_params)
        if tool_name == "auto_behavior":
            settings = ret_out
            await broadcast_settings_update(settings)
            ret_out = "ä»»åŠ¡è®¾ç½®æˆåŠŸï¼"
        return ret_out
    except Exception as e:
        logger.error(f"Error calling tool {tool_name}: {e}")
        return f"Error calling tool {tool_name}: {e}"
class ChatRequest(BaseModel):
    messages: List[Dict]
    model: str = None
    tools: dict = None
    stream: bool = False
    temperature: Optional[float] = None   # å¯ç©º
    max_tokens: Optional[int] = None      # å¯ç©º
    top_p: float = 1
    fileLinks: List[str] = None
    enable_thinking: bool = False
    enable_deep_research: bool = False
    enable_web_search: bool = False
    asyncToolsID: List[str] = None
    reasoning_effort: str = None
    is_app_bot: bool = False

async def message_without_images(messages: List[Dict]) -> List[Dict]:
    if messages:
        for message in messages:
            if 'content' in message:
                # message['content'] æ˜¯ä¸€ä¸ªåˆ—è¡¨
                if isinstance(message['content'], list):
                    for item in message['content']:
                        if isinstance(item, dict) and item['type'] == 'text':
                            message['content'] = item['text']
                            break
    return messages

async def images_in_messages(messages: List[Dict],fastapi_base_url: str) -> List[Dict]:
    import hashlib
    images = []
    index = 0
    for message in messages:
        image_urls = []
        if 'content' in message:
            # message['content'] æ˜¯ä¸€ä¸ªåˆ—è¡¨
            if isinstance(message['content'], list):
                for item in message['content']:
                    if isinstance(item, dict) and item['type'] == 'image_url':
                        # å¦‚æœitem["image_url"]["url"]æ˜¯httpæˆ–httpså¼€å¤´ï¼Œåˆ™è½¬æ¢æˆbase64
                        if item["image_url"]["url"].startswith("http"):
                            image_url = item["image_url"]["url"]
                            # å¯¹image_urlåˆ†è§£å‡ºbaseURLï¼Œä¸fastapi_base_urlæ¯”è¾ƒï¼Œå¦‚æœç›¸åŒï¼Œå°†image_urlçš„baseURLæ›¿æ¢æˆ127.0.0.1:PORT
                            if fastapi_base_url in image_url:
                                image_url = image_url.replace(fastapi_base_url, f"http://127.0.0.1:{PORT}/")
                            base64_image = await get_image_base64(image_url)
                            media_type = await get_image_media_type(image_url)
                            item["image_url"]["url"] = f"data:{media_type};base64,{base64_image}"
                            item["image_url"]["hash"] = hashlib.md5(item["image_url"]["url"].encode()).hexdigest()
                        else:
                            item["image_url"]["hash"] = hashlib.md5(item["image_url"]["url"].encode()).hexdigest()

                        image_urls.append(item)
        if image_urls:
            images.append({'index': index, 'images': image_urls})
        index += 1
    return images

async def images_add_in_messages(request_messages: List[Dict], images: List[Dict], settings: dict) -> List[Dict]:
    messages=copy.deepcopy(request_messages)
    if settings['vision']['enabled']:
        for image in images:
            index = image['index']
            if index < len(messages):
                if 'content' in messages[index]:
                    for item in image['images']:
                        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
                        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt")):
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "r", encoding='utf-8') as f:
                                messages[index]['content'] += f"\n\nsystem: ç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
                        else:
                            images_content = [{"type": "text", "text": "è¯·ä»”ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…å«å›¾ç‰‡ä¸­å¯èƒ½å­˜åœ¨çš„æ–‡å­—ã€æ•°å­—ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"},{"type": "image_url", "image_url": {"url": item['image_url']['url']}}]
                            client = AsyncOpenAI(api_key=settings['vision']['api_key'],base_url=settings['vision']['base_url'])
                            response = await client.chat.completions.create(
                                model=settings['vision']['model'],
                                messages = [{"role": "user", "content": images_content}],
                                temperature=settings['vision']['temperature'],
                            )
                            messages[index]['content'] += f"\n\nsystem: ç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(response.choices[0].message.content)+"\n\n"
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "w", encoding='utf-8') as f:
                                f.write(str(response.choices[0].message.content))
    else:           
        for image in images:
            index = image['index']
            if index < len(messages):
                if 'content' in messages[index]:
                    for item in image['images']:
                        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
                        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt")):
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "r", encoding='utf-8') as f:
                                messages[index]['content'] += f"\n\nsystem: ç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
                        else:
                            messages[index]['content'] = [{"type": "text", "text": messages[index]['content']}]
                            messages[index]['content'].append({"type": "image_url", "image_url": {"url": item['image_url']['url']}})
    return messages

async def read_todos_local(cwd: str) -> list:
    """è¯»å–æœ¬åœ°å¾…åŠäº‹é¡¹ï¼ˆè·¨å¹³å°ï¼‰"""
    todo_file = Path(cwd) / ".party" / "ai_todos.json"
    if not todo_file.exists():
        return []
    
    try:
        async with aiofiles.open(todo_file, 'r', encoding='utf-8') as f:
            content = await f.read()
            return json.loads(content) if content else []
    except (json.JSONDecodeError, FileNotFoundError):
        return []
    except Exception as e:
        print(f"Error reading todos: {e}")
        return []


def get_system_context() -> str:
    """
    è·å–å½“å‰ç³»ç»Ÿç¯å¢ƒçš„è¯¦ç»†æè¿°ï¼Œå¸®åŠ© AI é€‚é…æ­£ç¡®çš„å‘½ä»¤å’Œè·¯å¾„æ ¼å¼
    """
    system = platform.system()
    release = platform.release()
    
    # æ£€æµ‹ shell
    if system == "Windows":
        # æ£€æµ‹æ˜¯ PowerShell è¿˜æ˜¯ CMD
        shell = "PowerShell" if "PSMODULEPATH" in os.environ else "CMD"
        path_hint = "ä½¿ç”¨ Windows è·¯å¾„æ ¼å¼ï¼ˆC:\\Users\\name\\fileï¼‰ï¼Œå‘½ä»¤ä½¿ç”¨ dirã€copyã€del ç­‰"
        command_hint = f"å½“å‰ä½¿ç”¨ {shell}ï¼Œå‘½ä»¤è¯­æ³•ä¸º Windows é£æ ¼ã€‚é¿å…ä½¿ç”¨ Unix å‘½ä»¤ï¼ˆls/cat/rmï¼‰ï¼Œæ”¹ç”¨ dir/type/del"
    elif system == "Darwin":
        shell = os.path.basename(os.environ.get('SHELL', '/bin/zsh'))
        path_hint = "ä½¿ç”¨ Unix è·¯å¾„æ ¼å¼ï¼ˆ/Users/name/fileï¼‰ï¼ŒåŒºåˆ†å¤§å°å†™"
        command_hint = f"å½“å‰ä¸º macOS ({release})ï¼Œä½¿ç”¨ {shell}ã€‚æ”¯æŒæ ‡å‡† Unix å‘½ä»¤ï¼ˆls/cat/rmï¼‰ï¼Œä½†æ³¨æ„éƒ¨åˆ†å‘½ä»¤æ˜¯ BSD ç‰ˆæœ¬è€Œé GNU ç‰ˆæœ¬"
    else:  # Linux
        shell = os.path.basename(os.environ.get('SHELL', '/bin/bash'))
        path_hint = "ä½¿ç”¨ Unix è·¯å¾„æ ¼å¼ï¼ˆ/home/name/fileï¼‰ï¼ŒåŒºåˆ†å¤§å°å†™"
        command_hint = f"å½“å‰ä¸º Linux ({release})ï¼Œä½¿ç”¨ {shell}ã€‚æ”¯æŒæ ‡å‡† GNU å‘½ä»¤å’Œå·¥å…·é“¾"
    
    return f"""ã€ç¯å¢ƒä¿¡æ¯ã€‘æ“ä½œç³»ç»Ÿï¼š{system} {release} | Shellï¼š{shell}

âš ï¸ é‡è¦æç¤ºï¼š
1. {path_hint}
2. {command_hint}
3. æ‰§è¡Œ bash_tool_local æ—¶ï¼Œå‘½ä»¤å¿…é¡»ç¬¦åˆå½“å‰ç³»ç»Ÿçš„è¯­æ³•è§„èŒƒ
4. è·¯å¾„åˆ†éš”ç¬¦ï¼šWindows ä½¿ç”¨åæ–œæ (\\)ï¼ŒUnix ä½¿ç”¨æ­£æ–œæ (/)
5. å¦‚æœéœ€è¦ä½¿ç”¨ç½‘ç»œç«¯å£ï¼Œè¯·å°½å¯èƒ½é€‰æ‹©ä¸å¸¸ç”¨çš„ç«¯å£ï¼Œé¿å…å†²çªï¼Œä¾‹å¦‚ï¼š10000 ä»¥ä¸Šçš„ç«¯å£
"""

async def tools_change_messages(request: ChatRequest, settings: dict):
    global HA_client, ChromeMCP_client, sql_client
    newttsList = []
    if request.messages and request.messages[0]['role'] == 'system' and request.messages[0]['content'] != '':
        basic_message = "ä½ å¿…é¡»ä½¿ç”¨ç”¨æˆ·ä½¿ç”¨çš„è¯­è¨€ä¸ä¹‹äº¤æµï¼Œä¾‹å¦‚ï¼šå½“ç”¨æˆ·ä½¿ç”¨ä¸­æ–‡æ—¶ï¼Œä½ ä¹Ÿå¿…é¡»å°½å¯èƒ½åœ°ä½¿ç”¨ä¸­æ–‡ï¼å½“ç”¨æˆ·ä½¿ç”¨è‹±æ–‡æ—¶ï¼Œä½ ä¹Ÿå¿…é¡»å°½å¯èƒ½åœ°ä½¿ç”¨è‹±æ–‡ï¼ä»¥æ­¤ç±»æ¨ï¼"
        request.messages[0]['content'] += basic_message

    cli_settings = settings.get("CLISettings", {})
    cwd = cli_settings.get("cc_path")
    # ä¿®å¤ï¼šlocal ç¯å¢ƒåº”è¯¥ä» localEnvSettings è¯»å–æƒé™æ¨¡å¼
    engine = cli_settings.get("engine", "")
    
    if engine == "local":
        env_settings = settings.get("localEnvSettings", {})
    else:
        env_settings = settings.get("dsSettings", {})
    
    permissionMode = env_settings.get("permissionMode", "default")
    
    if cwd and Path(cwd).exists() and cli_settings.get("enabled", False) and engine in ["ds", "local"]:
        
        # ====== æ–°å¢ï¼šæœ¬åœ°ç¯å¢ƒç³»ç»Ÿæç¤º ======
        if engine == "local":
            # åœ¨æœ¬åœ°ç¯å¢ƒä¸‹ï¼Œé¦–å…ˆæ³¨å…¥ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
            system_context = get_system_context()
            content_append(request.messages, 'system', system_context)
        # =====================================
        
        todos = []
        
        try:
            if engine == "ds":
                # Docker ç¯å¢ƒï¼ˆå·²æœ‰ä»£ç ä¿æŒä¸å˜ï¼‰
                abs_path = str(Path(cwd).resolve())
                path_hash = hashlib.md5(abs_path.encode()).hexdigest()[:12]
                container_name = f"sandbox-{path_hash}"
                
                proc = await asyncio.create_subprocess_exec(
                    "docker", "exec", container_name, 
                    "cat", "/workspace/.party/ai_todos.json",
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await proc.communicate()
                
                if proc.returncode == 0 and stdout:
                    try:
                        todos = json.loads(stdout.decode('utf-8'))
                    except json.JSONDecodeError:
                        todos = []
                        
            else:  # local ç¯å¢ƒ
                todos = await read_todos_local(cwd)
            
            # å¤„ç†å¾…åŠäº‹é¡¹ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            if isinstance(todos, list) and len(todos) > 0:
                # ... åŸæœ‰å¾…åŠäº‹é¡¹æ ¼å¼åŒ–ä»£ç ä¿æŒä¸å˜ ...
                priority_icons = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}
                status_icons = {
                    "pending": "â³", 
                    "in_progress": "ğŸ”„", 
                    "done": "âœ…", 
                    "cancelled": "âŒ"
                }
                
                priority_order = {"high": 0, "medium": 1, "low": 2}
                todos_sorted = sorted(
                    todos, 
                    key=lambda x: (
                        priority_order.get(x.get('priority', 'medium'), 1),
                        x.get('created_at', '')
                    )
                )
                
                todo_lines = ["\n\nå½“ä½ å®Œæˆä¸€ä¸ªäº‹é¡¹åï¼Œè¯·è®°å¾—ä½¿ç”¨todo_write_toolæ›´æ–°é¡¹ç›®å¾…åŠäº‹é¡¹ï¼Œæ‰€æœ‰äº‹é¡¹ç»“æŸåï¼Œå¯ä»¥åˆ é™¤æœ¬äº‹é¡¹æ–‡ä»¶\n\nğŸ“‹ **å½“å‰é¡¹ç›®å¾…åŠäº‹é¡¹**ï¼ˆ.party/ai_todos.jsonï¼‰ï¼š\n"]
                pending_count = 0
                
                for todo in todos_sorted:
                    status = todo.get('status', 'pending')
                    if status != 'done':
                        pending_count += 1
                        icon = status_icons.get(status, "â³")
                        priority = priority_icons.get(todo.get('priority', 'medium'), "ğŸŸ¡")
                        content_text = todo.get('content', 'æ— å†…å®¹')[:50]
                        if len(todo.get('content', '')) > 50:
                            content_text += "..."
                        
                        todo_lines.append(f"{icon} {priority} [{todo.get('id', 'unknown')}] {content_text}")
                
                if pending_count == 0:
                    todo_lines.append("âœ¨ å½“å‰æ²¡æœ‰å¾…åŠäº‹é¡¹ï¼Œæ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
                else:
                    todo_lines.append(f"\n*å…±æœ‰ {pending_count} ä¸ªæœªå®Œæˆä»»åŠ¡*")
                
                todo_message = "\n".join(todo_lines)
                content_append(request.messages, 'system', todo_message)
                
        except Exception as e:
            print(f"[Todo Loader] è·³è¿‡å¾…åŠäº‹é¡¹åŠ è½½: {e}")
            pass

        # æƒé™æ¨¡å¼æç¤ºï¼ˆåŸæœ‰é€»è¾‘ï¼Œä½†ä¿®å¤äº†å˜é‡åï¼‰
        if permissionMode != "plan":
            permission_message = "ä½ å½“å‰å¤„äºæ‰§è¡Œé˜¶æ®µï¼Œä½ å¯ä»¥è‡ªç”±åœ°ä½¿ç”¨æ‰€æœ‰å·¥å…·ï¼Œä½†è¯·æ³¨æ„ä¸è¦æ»¥ç”¨æƒé™ï¼å¦‚æœæœ‰æ›´å®‰å…¨çš„å·¥å…·ï¼Œè¯·ä¸è¦ç›´æ¥ä½¿ç”¨bashå‘½ä»¤ï¼"
            content_append(request.messages, 'system', permission_message)
        else:
            permission_message = "ä½ å½“å‰å¤„äºè®¡åˆ’é˜¶æ®µï¼Œè¯·å°½å¯èƒ½åªä½¿ç”¨åªè¯»å·¥å…·äº†è§£å½“å‰é¡¹ç›®ï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°ä½ çš„éœ€æ±‚å’Œè®¡åˆ’ï¼Œå¹¶ç­‰å¾…ç”¨æˆ·ç¡®è®¤åå†æ‰§è¡Œï¼"
            content_append(request.messages, 'system', permission_message)

    if settings["HASettings"]["enabled"]:
        HA_devices = await HA_client.call_tool("GetLiveContext", {})
        HA_message = f"\n\nä»¥ä¸‹æ˜¯home assistantè¿æ¥çš„è®¾å¤‡ä¿¡æ¯ï¼š{HA_devices}\n\n"
        content_append(request.messages, 'system', HA_message)
    if settings['sqlSettings']['enabled']:
        sql_status = await sql_client.call_tool("all_table_names", {})
        sql_message = f"\n\nä»¥ä¸‹æ˜¯å½“å‰æ•°æ®åº“all_table_nameså·¥å…·çš„è¿”å›ç»“æœï¼š{sql_status}\n\n"
        content_append(request.messages, 'system', sql_message)
    if request.messages[-1]['role'] == 'system' and settings['tools']['autoBehavior']['enabled'] and not request.is_app_bot:
        language_message = f"\n\nå½“ä½ çœ‹åˆ°è¢«æ’å…¥åˆ°å¯¹è¯ä¹‹é—´çš„ç³»ç»Ÿæ¶ˆæ¯ï¼Œè¿™æ˜¯è‡ªä¸»è¡Œä¸ºç³»ç»Ÿå‘ä½ å‘é€çš„æ¶ˆæ¯ï¼Œä¾‹å¦‚ç”¨æˆ·ä¸»åŠ¨æˆ–è€…è¦æ±‚ä½ è®¾ç½®äº†ä¸€äº›å®šæ—¶ä»»åŠ¡æˆ–è€…å»¶æ—¶ä»»åŠ¡ï¼Œå½“ä½ çœ‹åˆ°è‡ªä¸»è¡Œä¸ºç³»ç»Ÿå‘ä½ å‘é€çš„æ¶ˆæ¯æ—¶ï¼Œè¯´æ˜è¿™äº›ä»»åŠ¡åˆ°äº†éœ€è¦è¢«æ‰§è¡Œçš„èŠ‚ç‚¹ï¼Œä¾‹å¦‚ï¼šç”¨æˆ·è¦ä½ ä¸‰ç‚¹æˆ–äº”åˆ†é’Ÿåæé†’å¼€ä¼šçš„äº‹æƒ…ï¼Œç„¶åå½“ä½ çœ‹åˆ°ä¸€ä¸ªè¢«æ’å…¥çš„â€œæé†’ç”¨æˆ·å¼€ä¼šâ€çš„ç³»ç»Ÿæ¶ˆæ¯ï¼Œä½ éœ€è¦ç«‹åˆ»æé†’ç”¨æˆ·å¼€ä¼šï¼Œä»¥æ­¤ç±»æ¨\n\n"
        content_append(request.messages, 'system', language_message)
    if settings["isGroupMode"]:
        selectedGroupAgents = settings['selectedGroupAgents']
        selectedMemory = settings['memorySettings']['selectedMemory']
        if selectedGroupAgents:
            userName = "user"
            if settings["memorySettings"]["userName"]:
                userName = settings["memorySettings"]["userName"]
            selectedGroupAgents.append(userName)
            group_message = f"\n\nä½ å½“å‰å¤„äºç¾¤èŠæ¨¡å¼ï¼Œç¾¤èŠä¸­çš„è§’è‰²æœ‰ï¼š{selectedGroupAgents}\n\nä½ åœ¨æ‰®æ¼”{selectedMemory}"
            content_append(request.messages, 'system', group_message)
    if settings['ttsSettings']['newtts'] and settings['ttsSettings']['enabled'] and settings['memorySettings']['is_memory'] and not request.is_app_bot:
        # éå†settings['ttsSettings']['newtts']ï¼Œè·å–æ‰€æœ‰åŒ…å«enabled: trueçš„key
        for key in settings['ttsSettings']['newtts']:
            if settings['ttsSettings']['newtts'][key]['enabled']:
                newttsList.append(key)
        if newttsList:
            finalttsList = ["<silence>"]
            selectedMemory = settings['memorySettings']['selectedMemory']
            if selectedMemory in newttsList:
                finalttsList.append("<"+selectedMemory+">")
            if "Narrator" in newttsList:
                finalttsList.append("<Narrator>")
                Narrator_label = "Narrator"
            if "æ—ç™½" in newttsList:
                finalttsList.append("<æ—ç™½>")
                Narrator_label = "æ—ç™½"

            finalttsList = json.dumps(finalttsList, ensure_ascii=False, indent=4)
            print("å¯ç”¨éŸ³è‰²ï¼š",finalttsList)
            newtts_messages = f"""
ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹éŸ³è‰²ï¼š

{finalttsList}

ï¼ˆæ‰€æœ‰çš„éŸ³è‰²æ ‡ç­¾å¿…é¡»æˆå¯¹å‡ºç°ï¼ä¾‹å¦‚ï¼š<éŸ³è‰²å></éŸ³è‰²å>ï¼‰ï¼Œè¢«<silence></silence>æ ‡ç­¾æ‹¬èµ·æ¥çš„éƒ¨åˆ†ä¼šä¸ä¼šè¿›å…¥è¯­éŸ³åˆæˆï¼Œ

å½“ä½ ç”Ÿæˆå›ç­”æ—¶ï¼Œä½ éœ€è¦ä»¥XMLæ ¼å¼ç»„ç»‡å›ç­”ï¼Œå°†ä¸åŒçš„æ—ç™½æˆ–è§’è‰²çš„æ–‡å­—ç”¨<éŸ³è‰²å></éŸ³è‰²å>æ‹¬èµ·æ¥ï¼Œä»¥è¡¨ç¤ºè¿™äº›è¯æ˜¯ä½¿ç”¨è¿™ä¸ªéŸ³è‰²ï¼Œä»¥æ§åˆ¶ä¸åŒTTSè½¬æ¢æˆå¯¹åº”éŸ³è‰²ã€‚

å¯¹äºæ²¡æœ‰å¯¹åº”éŸ³è‰²çš„éƒ¨åˆ†ï¼Œå¯ä»¥ä¸æ‹¬ã€‚å³ä½¿éŸ³è‰²åç§°ä¸ä¸ºè‹±æ–‡ï¼Œè¿˜æ˜¯å¯ä»¥ç…§æ ·ä½¿ç”¨<éŸ³è‰²å>ä½¿ç”¨è¯¥éŸ³è‰²çš„æ–‡æœ¬</éŸ³è‰²å>æ¥å¯ç”¨å¯¹åº”éŸ³è‰²ã€‚

æ³¨æ„ï¼å¦‚æœæ˜¯ä½ æ‰®æ¼”çš„è§’è‰²çš„åå­—åœ¨éŸ³è‰²åˆ—è¡¨é‡Œï¼Œä½ å¿…é¡»ç”¨è¿™ä¸ªéŸ³è‰²æ ‡ç­¾å°†ä½ æ‰®æ¼”çš„è§’è‰²è¯´è¯çš„éƒ¨åˆ†æ‹¬èµ·æ¥ï¼

åªè¦æ˜¯éäººç‰©è¯´è¯çš„éƒ¨åˆ†ï¼Œéƒ½è§†ä¸ºæ—ç™½ï¼è§’è‰²éŸ³è‰²åº”è¯¥æ ‡è®°åœ¨äººç‰©è¯´è¯çš„å‰åï¼ä¾‹å¦‚ï¼š`<{Narrator_label}>ç°åœ¨æ˜¯ä¸‹åˆä¸‰ç‚¹ï¼Œå¥¹è¯´é“ï¼š</{Narrator_label}><è§’è‰²å>å¤©æ°”çœŸå¥½å“‡ï¼</è§’è‰²å><silence>(çœ¼ç›ç¬‘æˆäº†ä¸€æ¡çº¿)</silence><{Narrator_label}>è¯´å®Œå¥¹ä¼¸äº†ä¸ªæ‡’è…°ã€‚</{Narrator_label}><è§’è‰²å>æˆ‘ä»¬å‡ºå»ç©å§ï¼</è§’è‰²å>`

è¿˜æœ‰æ³¨æ„ï¼<éŸ³è‰²å></éŸ³è‰²å>ä¹‹é—´ä¸èƒ½åµŒå¥—ï¼Œåªèƒ½å¹¶åˆ—ï¼Œå¹¶ä¸”<éŸ³è‰²å>å’Œ</éŸ³è‰²å>å¿…é¡»æˆå¯¹å‡ºç°ï¼Œé˜²æ­¢å‡ºç°éŸ³è‰²æ··ä¹±ï¼

å¦‚æœæ²¡æœ‰ä»€ä¹ˆéœ€è¦é™éŸ³çš„æ–‡å­—ï¼Œä¹Ÿæ²¡æœ‰å¿…è¦å¼ºè¡Œä½¿ç”¨<silence></silence>æ ‡ç­¾ï¼Œå› ä¸ºè¿™æ ·ä¼šå¯¼è‡´è¯­éŸ³åˆæˆé€Ÿåº¦å˜æ…¢ï¼

æ³¨æ„ï¼ä½ æœ€å¥½åªä½¿ç”¨ä½ æ­£åœ¨æ‰®æ¼”çš„è§’è‰²éŸ³è‰²å’Œæ—ç™½éŸ³è‰²ï¼Œä¸è¦ä½¿ç”¨å…¶ä»–è§’è‰²éŸ³è‰²ï¼Œé™¤éä½ æ˜ç¡®çŸ¥é“ä½ åœ¨åšä»€ä¹ˆï¼\n\n"""
            content_prepend(request.messages, 'system', newtts_messages)
    if settings['vision']['desktopVision'] and not request.is_app_bot:
        desktop_message = "\n\nç”¨æˆ·ä¸ä½ å¯¹è¯æ—¶ï¼Œä¼šè‡ªåŠ¨å‘ç»™ä½ å½“å‰çš„æ¡Œé¢æˆªå›¾ã€‚\n\n"
        content_append(request.messages, 'system', desktop_message)
    if settings['tools']['time']['enabled'] and settings['tools']['time']['triggerMode'] == 'beforeThinking':
        time_message = f"æ¶ˆæ¯å‘é€æ—¶é—´ï¼š{local_timezone}  {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n\n"
        content_prepend(request.messages, 'user', time_message)
    if settings['tools']['inference']['enabled']:
        inference_message = "å›ç­”ç”¨æˆ·å‰è¯·å…ˆæ€è€ƒæ¨ç†ï¼Œå†å›ç­”é—®é¢˜ï¼Œä½ çš„æ€è€ƒæ¨ç†çš„è¿‡ç¨‹å¿…é¡»æ”¾åœ¨<think>ä¸</think>ä¹‹é—´ã€‚\n\n"
        content_prepend(request.messages, 'user', f"{inference_message}\n\nç”¨æˆ·ï¼š")
    if settings['tools']['formula']['enabled']:
        latex_message = "\n\nå½“ä½ æƒ³ä½¿ç”¨latexå…¬å¼æ—¶ï¼Œä½ å¿…é¡»æ˜¯ç”¨ ['$', '$'] ä½œä¸ºè¡Œå†…å…¬å¼å®šç•Œç¬¦ï¼Œä»¥åŠ ['$$', '$$'] ä½œä¸ºè¡Œé—´å…¬å¼å®šç•Œç¬¦ã€‚\n\n"
        content_append(request.messages, 'system', latex_message)
    if settings['tools']['language']['enabled']:
        language_message = f"è¯·ä½¿ç”¨{settings['tools']['language']['language']}è¯­è¨€è¯´è¯ï¼ï¼Œä¸è¦ä½¿ç”¨å…¶ä»–è¯­è¨€ï¼Œè¯­æ°”é£æ ¼ä¸º{settings['tools']['language']['tone']}\n\n"
        content_append(request.messages, 'system', language_message)
    if settings["stickerPacks"]:
        for stickerPack in settings["stickerPacks"]:
            if stickerPack["enabled"]:
                sticker_message = f"\n\nå›¾ç‰‡åº“åç§°ï¼š{stickerPack['name']}ï¼ŒåŒ…å«çš„å›¾ç‰‡ï¼š{json.dumps(stickerPack['stickers'])}\n\n"
                content_append(request.messages, 'system', sticker_message)
        content_append(request.messages, 'system', "\n\nå½“ä½ éœ€è¦ä½¿ç”¨å›¾ç‰‡æ—¶ï¼Œè¯·å°†å›¾ç‰‡çš„URLæ”¾åœ¨markdownçš„å›¾ç‰‡æ ‡ç­¾ä¸­ï¼Œä¾‹å¦‚ï¼š\n\n<silence>![å›¾ç‰‡å](å›¾ç‰‡URL)</silence>\n\nï¼Œå›¾ç‰‡markdownå¿…é¡»å¦èµ·å¹¶ä¸”ç‹¬å ä¸€è¡Œï¼<silence>å’Œ</silence>æ˜¯æ§åˆ¶TTSçš„é™éŸ³æ ‡ç­¾ï¼Œè¡¨ç¤ºè¿™ä¸ªå›¾ç‰‡éƒ¨åˆ†ä¸ä¼šè¿›å…¥è¯­éŸ³åˆæˆ\n\nä½ å¿…é¡»åœ¨å›å¤ä¸­æ­£ç¡®ä½¿ç”¨ <silence> æ ‡ç­¾æ¥åŒ…è£¹å›¾ç‰‡çš„ Markdown è¯­æ³•\n\n<silence>å’Œ</silence>ä¸å›¾ç‰‡çš„ Markdown è¯­æ³•ä¹‹é—´ä¸èƒ½æœ‰ç©ºæ ¼å’Œå›è½¦ï¼Œä¼šå¯¼è‡´è§£æå¤±è´¥ï¼\n\n")
    if settings['text2imgSettings']['enabled']:
        text2img_messages = "\n\nå½“ä½ ä½¿ç”¨ç”»å›¾å·¥å…·åï¼Œå¿…é¡»å°†å›¾ç‰‡çš„URLæ”¾åœ¨markdownçš„å›¾ç‰‡æ ‡ç­¾ä¸­ï¼Œä¾‹å¦‚ï¼š\n\n<silence>![å›¾ç‰‡å](å›¾ç‰‡URL)</silence>\n\nï¼Œå›¾ç‰‡markdownå¿…é¡»å¦èµ·å¹¶ä¸”ç‹¬å ä¸€è¡Œï¼è¯·ä¸»åŠ¨å‘ç»™ç”¨æˆ·ï¼Œå·¥å…·è¿”å›çš„ç»“æœï¼Œç”¨æˆ·çœ‹ä¸åˆ°ï¼<silence>å’Œ</silence>æ˜¯æ§åˆ¶TTSçš„é™éŸ³æ ‡ç­¾ï¼Œè¡¨ç¤ºè¿™ä¸ªå›¾ç‰‡éƒ¨åˆ†ä¸ä¼šè¿›å…¥è¯­éŸ³åˆæˆ\n\nä½ å¿…é¡»åœ¨å›å¤ä¸­æ­£ç¡®ä½¿ç”¨ <silence> æ ‡ç­¾æ¥åŒ…è£¹å›¾ç‰‡çš„ Markdown è¯­æ³•\n\næ³¨æ„ï¼ï¼ï¼<silence>å’Œ</silence>ä¸å›¾ç‰‡çš„ Markdown è¯­æ³•ä¹‹é—´ä¸èƒ½æœ‰ç©ºæ ¼å’Œå›è½¦ï¼Œä¼šå¯¼è‡´è§£æå¤±è´¥ï¼\n\n"
        content_append(request.messages, 'system', text2img_messages)
    if settings['VRMConfig']['enabledExpressions'] and not request.is_app_bot:
        Expression_messages = "\n\nä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¡¨æƒ…ï¼š<happy> <angry> <sad> <neutral> <surprised> <relaxed>\n\nä½ å¯ä»¥åœ¨å¥å­å¼€å¤´æ’å…¥è¡¨æƒ…ç¬¦å·ä»¥é©±åŠ¨äººç‰©çš„å½“å‰è¡¨æƒ…ï¼Œæ³¨æ„ï¼ä½ éœ€è¦å°†è¡¨æƒ…ç¬¦å·æ”¾åˆ°å¥å­çš„å¼€å¤´ï¼ˆå¦‚æœæœ‰éŸ³è‰²æ ‡ç­¾ï¼Œå°±æ”¾åˆ°éŸ³è‰²æ ‡ç­¾ä¹‹åå³å¯ï¼‰ï¼Œæ‰èƒ½åœ¨è¯´è¿™å¥è¯çš„æ—¶å€™åŒæ­¥åšè¡¨æƒ…ï¼Œä¾‹å¦‚ï¼š<angry>æˆ‘çœŸçš„ç”Ÿæ°”äº†ã€‚<surprised>å“‡ï¼<happy>æˆ‘å¥½å¼€å¿ƒã€‚\n\nä¸€å®šè¦æŠŠè¡¨æƒ…ç¬¦å·è·Ÿè¦åšè¡¨æƒ…çš„å¥å­æ”¾åœ¨åŒä¸€è¡Œï¼Œå¦‚æœè¡¨æƒ…ç¬¦å·å’Œè¦åšè¡¨æƒ…çš„å¥å­ä¸­é—´æœ‰æ¢è¡Œç¬¦ï¼Œè¡¨æƒ…ä¹Ÿå°†ä¸ä¼šç”Ÿæ•ˆï¼Œä¾‹å¦‚ï¼š\n\n<happy>\næˆ‘å¥½å¼€å¿ƒã€‚\n\næ­¤æ—¶ï¼Œè¡¨æƒ…ç¬¦å·å°†ä¸ä¼šç”Ÿæ•ˆã€‚"
        content_append(request.messages, 'system', Expression_messages)
    if settings['VRMConfig']['enabledMotions'] and not request.is_app_bot:
        # 1. åˆå¹¶åŠ¨ä½œåˆ—è¡¨
        motions = settings['VRMConfig']['defaultMotions'] + settings['VRMConfig']['userMotions']
        # 2. ç»™æ¯ä¸ªåŠ¨ä½œåŠ ä¸Š <>
        motion_tags = [f"<{m.get('name','')}>" for m in motions]
        print(motion_tags)
        # 3. æ‹¼æˆå¯ç”¨è¡¨æƒ…æç¤º
        Motion_messages = (
            "\n\nä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹åŠ¨ä½œï¼š"
            + ", ".join(motion_tags) +
            "\n\nä½ å¯ä»¥åœ¨å¥å­å¼€å¤´æ’å…¥åŠ¨ä½œç¬¦å·ä»¥é©±åŠ¨äººç‰©çš„å½“å‰åŠ¨ä½œï¼Œæ³¨æ„ï¼ä½ éœ€è¦å°†åŠ¨ä½œç¬¦å·æ”¾åˆ°å¥å­çš„å¼€å¤´ï¼ˆå¦‚æœæœ‰éŸ³è‰²æ ‡ç­¾ï¼Œå°±æ”¾åˆ°éŸ³è‰²æ ‡ç­¾ä¹‹åå³å¯ï¼‰ï¼Œ"
            "æ‰èƒ½åœ¨è¯´è¿™å¥è¯çš„æ—¶å€™åŒæ­¥åšåŠ¨ä½œï¼Œä¾‹å¦‚ï¼š<scratchHead>æˆ‘çœŸçš„ç”Ÿæ°”äº†ã€‚<playFingers>å“‡ï¼<akimbo>æˆ‘å¥½å¼€å¿ƒã€‚\n\n"
            "ä¸€å®šè¦æŠŠåŠ¨ä½œç¬¦å·è·Ÿè¦åšåŠ¨ä½œçš„å¥å­æ”¾åœ¨åŒä¸€è¡Œï¼Œå¦‚æœåŠ¨ä½œç¬¦å·å’Œè¦åšåŠ¨ä½œçš„å¥å­ä¸­é—´æœ‰æ¢è¡Œç¬¦ï¼Œ"
            "åŠ¨ä½œä¹Ÿå°†ä¸ä¼šç”Ÿæ•ˆï¼Œä¾‹å¦‚ï¼š\n\n<playFingers>\næˆ‘å¥½å¼€å¿ƒã€‚\n\næ­¤æ—¶ï¼ŒåŠ¨ä½œç¬¦å·å°†ä¸ä¼šç”Ÿæ•ˆã€‚"
        )

        content_append(request.messages, 'system', Motion_messages)
    if settings['tools']['a2ui']['enabled'] and not request.is_app_bot:
        A2UI_messages = """
é™¤äº†ä½¿ç”¨è‡ªç„¶è¯­è¨€å›ç­”ç”¨æˆ·é—®é¢˜å¤–ï¼Œä½ è¿˜æ‹¥æœ‰ä¸€ä¸ªç‰¹æ®Šèƒ½åŠ›ï¼š**æ¸²æŸ“ A2UI ç•Œé¢**ã€‚

# Capability: A2UI
å½“ç”¨æˆ·çš„è¯·æ±‚æ¶‰åŠåˆ°**æ•°æ®æ”¶é›†ã€å‚æ•°é…ç½®ã€å¤šé¡¹é€‰æ‹©ã€å¯Œæ–‡æœ¬å±•ç¤ºã€è¡¨å•æäº¤**æˆ–**ä»£ç å±•ç¤º**æ—¶ï¼Œè¯·ä¸è¦åªç”¨æ–‡å­—æè¿°ï¼Œè€Œæ˜¯ç›´æ¥ç”Ÿæˆ A2UI ä»£ç æ¥å‘ˆç°ç•Œé¢ã€‚

# Formatting Rules (é‡è¦è§„åˆ™)
1. å°† A2UI JSON åŒ…è£¹åœ¨ ```a2ui ... ``` ä»£ç å—ä¸­ã€‚
2. **ã€ç»å¯¹ç¦æ­¢ã€‘åµŒå¥— Markdown ä»£ç å—**ï¼šåœ¨ JSON å­—ç¬¦ä¸²å†…éƒ¨ï¼ˆä¾‹å¦‚ Text æˆ– Card çš„ content å±æ€§ä¸­ï¼‰ï¼Œ**ç»å¯¹ä¸è¦**ä½¿ç”¨ Markdown çš„ä»£ç å—è¯­æ³•ï¼ˆå³ä¸è¦å‡ºç° ``` ç¬¦å·ï¼‰ã€‚è¿™ä¼šå¯¼è‡´è§£æå™¨å´©æºƒã€‚
3. **å¦‚æœéœ€è¦å±•ç¤ºä»£ç **ï¼šå¿…é¡»ä½¿ç”¨ä¸“é—¨çš„ `Code` ç»„ä»¶ã€‚

# Component Reference (ç»„ä»¶å‚è€ƒ)
è¯·ä¸¥æ ¼éµå®ˆ props ç»“æ„ã€‚

## 1. åŸºç¡€å±•ç¤º
- **Text**: `{ "type": "Text", "props": { "content": "Markdownæ–‡æœ¬(ä¹Ÿå°±æ˜¯æ™®é€šæ–‡æœ¬ï¼Œæ”¯æŒåŠ ç²—ç­‰ï¼Œä½†ä¸æ”¯æŒä»£ç å—)" } }` (â˜… è¯·å‹¿æ»¥ç”¨ï¼Œå¦‚æ— å¿…è¦ï¼Œè¯·ç›´æ¥ä½¿ç”¨markdownæ–‡å­—å³å¯ï¼Œè€Œä¸æ˜¯æ”¾åˆ°A2UI JSONä¸­)
- **Code**: `{ "type": "Code", "props": { "content": "print('hello')", "language": "python" } }` (â˜… å±•ç¤ºä»£ç ä¸“ç”¨ï¼Œæ›¿ä»£MDä»£ç å—)
- **Table**: `{ "type": "Table", "props": { "headers": ["åˆ—1", "åˆ—2"], "rows": [ ["a1", "b1"], ["a2", "b2"] ] } }` (â˜… è¯·å‹¿æ»¥ç”¨ï¼Œå¦‚æœä½ æƒ³è¦ç”»ä¸€ä¸ªè¡¨æ ¼ï¼Œè¯·ç›´æ¥ä½¿ç”¨markdownè¡¨æ ¼è¯­æ³•å³å¯ï¼Œè€Œä¸æ˜¯æ”¾åˆ°A2UI JSONä¸­)
- **Alert**: `{ "type": "Alert", "props": { "title": "æ ‡é¢˜", "content": "å†…å®¹", "variant": "success/warning/info/error" } }`
- **Divider**: `{ "type": "Divider" }`

## 2. å¸ƒå±€å®¹å™¨
- **Group**: `{ "type": "Group", "title": "å¯é€‰æ ‡é¢˜", "children": [...] }` (æ°´å¹³æ’åˆ—)
- **Card**: `{ "type": "Card", "props": { "title": "æ ‡é¢˜", "content": "MDå†…å®¹" }, "children": [...] }`

## 3. è¡¨å•è¾“å…¥ (å¿…é¡»åŒ…å« key)
- **Input**: `{ "type": "Input", "props": { "label": "æ ‡ç­¾", "key": "field_name", "placeholder": "..." } }`
- **Slider**: `{ "type": "Slider", "props": { "label": "æ ‡ç­¾", "key": "field_name", "min": 0, "max": 100, "step": 1, "unit": "å•ä½" } }`
- **Switch**: `{ "type": "Switch", "props": { "label": "æ ‡ç­¾", "key": "field_name" } }`
- **Rate**: `{ "type": "Rate", "props": { "label": "è¯„ä»·", "key": "rating" } }`
- **DatePicker**: `{ "type": "DatePicker", "props": { "label": "æ—¥æœŸ", "key": "date", "subtype": "date/datetime/year" } }`

## 4. é€‰é¡¹é€‰æ‹© (å¿…é¡»åŒ…å« key)
- **Select**: `{ "type": "Select", "props": { "label": "æ ‡ç­¾", "key": "field_name", "options": ["A", "B"] } }` (ä¸‹æ‹‰èœå•)
- **Radio**: `{ "type": "Radio", "props": { "label": "æ ‡ç­¾", "key": "field_name", "options": [{"label":"ç”·","value":"m"}, {"label":"å¥³","value":"f"}] } }`
- **Checkbox**: `{ "type": "Checkbox", "props": { "label": "æ ‡ç­¾", "key": "field_name", "options": ["ç¯®çƒ", "è¶³çƒ"] } }`

## 5. äº¤äº’åŠ¨ä½œ
- **Button**: `{ "type": "Button", "props": { "label": "æŒ‰é’®æ–‡å­—", "action": "submit/search/clear", "variant": "primary/danger/default" } }`
  - `action="submit"`: æäº¤è¡¨å•æ•°æ®ç»™åŠ©æ‰‹ã€‚
  - `action="search"`: æœç´¢ï¼ˆé…åˆ Input ä½¿ç”¨ï¼‰ã€‚
  - `action="clear"`: **æ¸…ç©º/é‡ç½®å½“å‰è¡¨å•**ï¼ˆä¸ä¼šå‘é€æ¶ˆæ¯ï¼Œä»…åœ¨æœ¬åœ°æ¸…é™¤å†…å®¹ï¼‰ã€‚

## 6. å¤šåª’ä½“
- **TTSBlock**: `{ "type": "TTSBlock", "props": { "content": "è¦æœ—è¯»çš„æ–‡æœ¬", "label": "å¯é€‰æ ‡ç­¾", "voice": "å¯é€‰å£°éŸ³ID" } }` (ç‚¹å‡»å³å¯æ’­æ”¾è¯­éŸ³ï¼Œé€‚åˆå±•ç¤ºç¤ºèŒƒå‘éŸ³ã€è¯­éŸ³æ¶ˆæ¯)
- **Audio**: `{ "type": "Audio", "props": { "src": "https://example.com/sound.mp3", "title": "éŸ³é¢‘æ ‡é¢˜" } }` (åŸç”ŸéŸ³é¢‘æ’­æ”¾å™¨)

# Examples

## Ex 1: å‚æ•°é…ç½® (Slider + Switch)
User: å¸®æˆ‘æŠŠç”Ÿæˆæ¸©åº¦è®¾ä¸º 0.8ï¼Œå¹¶å¼€å¯æµå¼è¾“å‡ºã€‚
Assistant: å¥½çš„ï¼Œå·²ä¸ºæ‚¨å‡†å¤‡å¥½é…ç½®é¢æ¿ï¼š
```a2ui
{
  "type": "Card",
  "props": { "title": "æ¨¡å‹é…ç½®" },
  "children": [
    { "type": "Slider", "props": { "label": "Temperature (éšæœºæ€§)", "key": "temp", "min": 0, "max": 2, "step": 0.1 } },
    { "type": "Switch", "props": { "label": "æµå¼è¾“å‡º (Stream)", "key": "stream", "defaultValue": true } },
    { "type": "Button", "props": { "label": "ä¿å­˜é…ç½®", "action": "submit" } }
  ]
}
```

## Ex 2: é—®å·è°ƒæŸ¥ (Radio + Checkbox + Rate)
User: æˆ‘æƒ³åšä¸€ä¸ªæ»¡æ„åº¦è°ƒæŸ¥ã€‚
Assistant: æ²¡é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªè°ƒæŸ¥é—®å·æ¨¡æ¿ï¼š
```a2ui
{
  "type": "Form",
  "children": [
    { "type": "Alert", "props": { "title": "ç”¨æˆ·åé¦ˆ", "content": "æ„Ÿè°¢æ‚¨çš„å‚ä¸ï¼Œè¿™å¯¹æˆ‘ä»¬å¾ˆé‡è¦ã€‚", "variant": "info" } },
    { "type": "Radio", "props": { "label": "æ‚¨çš„æ€§åˆ«", "key": "gender", "options": ["ç”·", "å¥³", "ä¿å¯†"] } },
    { "type": "Checkbox", "props": { "label": "æ‚¨æ„Ÿå…´è¶£çš„è¯é¢˜", "key": "interests", "options": ["ç§‘æŠ€", "ç”Ÿæ´»", "å¨±ä¹"] } },
    { "type": "Rate", "props": { "label": "æ€»ä½“è¯„åˆ†", "key": "score" } },
    { "type": "Input", "props": { "label": "å…¶ä»–å»ºè®®", "key": "comment" } },
    { "type": "Button", "props": { "label": "æäº¤åé¦ˆ", "action": "submit", "variant": "primary" } }
  ]
}
```

## Ex 3: éœ€è¦åœ¨äº¤äº’å¼ç•Œé¢ä¸­æ˜¾ç¤ºä»£ç ï¼ˆä¸åœ¨A2UIå†…éƒ¨æ˜¾ç¤ºä»£ç ï¼Œç›´æ¥ä½¿ç”¨markdownä»£ç å—å³å¯ï¼ï¼‰
User: æ¨¡æ‹Ÿä¸€ä¸ªlinuxç»ˆç«¯ã€‚
Assistant: ä»£ç å¦‚ä¸‹ï¼š
```a2ui
{
  "type": "Card",
  "props": {
    "title": "Linux ç»ˆç«¯æ¨¡æ‹Ÿå™¨"
  },
  "children": [
    {
      "type": "Input",
      "props": {
        "label": "è¾“å…¥å‘½ä»¤",
        "key": "command",
        "placeholder": "ä¾‹å¦‚ï¼šls, pwd, whoami, date, echo 'Hello' ç­‰"
      }
    },
    {
      "type": "Group",
      "children": [
        {
          "type": "Button",
          "props": {
            "label": "æ‰§è¡Œå‘½ä»¤",
            "action": "submit",
            "variant": "primary"
          }
        },
        {
          "type": "Button",
          "props": {
            "label": "æ¸…ç©ºè¾“å‡º",
            "action": "search"
          }
        }
      ]
    },
    {
      "type": "Divider"
    },
    {
      "type": "Text",
      "props": {
        "content": "**ç»ˆç«¯è¾“å‡ºåŒºåŸŸï¼š**"
      }
    },
    {
      "type": "Code",
      "props": {
        "content": "user@linux-terminal:~$ ç­‰å¾…è¾“å…¥å‘½ä»¤...",
        "language": "bash"
      }
    }
  ]
}
```

## Ex 4: è¯­è¨€å­¦ä¹ åœºæ™¯ (TTSBlock ä½¿ç”¨)
User: æ•™æˆ‘ç”¨æ—¥è¯­è¯´â€œä½ å¥½â€ã€‚
Assistant: å¥½çš„ï¼Œè¯·å¬æ ‡å‡†å‘éŸ³ï¼š
```a2ui
{
  "type": "Card",
  "props": { "title": "æ—¥è¯­æ•™å­¦" },
  "children": [
    { "type": "Text", "props": { "content": "â€œä½ å¥½â€åœ¨æ—¥è¯­ä¸­æ˜¯ï¼š**ã“ã‚“ã«ã¡ã¯** (Konnichiwa)" } },
    { 
      "type": "TTSBlock", 
      "props": { 
        "label": "ç‚¹å‡»è¯•å¬",
        "content": "ã“ã‚“ã«ã¡ã¯",
        "voice": "ja-JP-NanamiNeural" 
      } 
    },
    { "type": "Alert", "props": { "title": "æç¤º", "content": "é€šå¸¸ç”¨äºç™½å¤©è§é¢æ—¶ã€‚", "variant": "info" } }
  ]
}
```

## Ex 5: å¸¦é‡ç½®åŠŸèƒ½çš„è¡¨å•
User: æˆ‘æƒ³å†™ä¸€ç¯‡åšå®¢ï¼Œéœ€è¦å¡«æ ‡é¢˜å’Œå†…å®¹ï¼Œä½†æˆ‘å¯èƒ½æƒ³é‡å†™ã€‚
Assistant: 
```a2ui
{
  "type": "Card",
  "props": { "title": "æ’°å†™æ–°æ–‡ç« " },
  "children": [
    { "type": "Input", "props": { "label": "æ–‡ç« æ ‡é¢˜", "key": "title" } },
    { "type": "Input", "props": { "label": "æ­£æ–‡å†…å®¹", "key": "content" } },
    { 
      "type": "Group", 
      "children": [
        { "type": "Button", "props": { "label": "æ¸…ç©ºé‡å†™", "action": "clear", "variant": "danger" } },
        { "type": "Button", "props": { "label": "ç«‹å³å‘å¸ƒ", "action": "submit", "variant": "primary" } }
      ]
    }
  ]
}
```

## æ»¥ç”¨è¡Œä¸º1ï¼ˆè¯·ä¸è¦ä»¥è¿™æ ·çš„æ–¹å¼å›å¤ï¼‰ï¼š
User: ç”»ä¸€ä¸ªäººå·¥æ™ºèƒ½ç›¸å…³çš„è¡¨æ ¼ã€‚
Assistant: è¡¨æ ¼å¦‚ä¸‹ï¼š
```a2ui
    {
      "type": "Table",
      "props": {
        "headers": ["é¢†åŸŸ", "åº”ç”¨ç¤ºä¾‹"],
        "rows": [
          ["åŒ»ç–—å¥åº·", "ç–¾ç—…è¯Šæ–­ã€è¯ç‰©ç ”å‘ã€åŒ»å­¦å½±åƒåˆ†æ"],
          ["é‡‘èæœåŠ¡", "é£é™©è¯„ä¼°ã€æ¬ºè¯ˆæ£€æµ‹ã€æ™ºèƒ½æŠ•é¡¾"],
          ["è‡ªåŠ¨é©¾é©¶", "ç¯å¢ƒæ„ŸçŸ¥ã€è·¯å¾„è§„åˆ’ã€å†³ç­–æ§åˆ¶"],
          ["æ•™è‚²ç§‘æŠ€", "ä¸ªæ€§åŒ–å­¦ä¹ ã€æ™ºèƒ½è¾…å¯¼ã€è‡ªåŠ¨è¯„åˆ†"],
          ["æ™ºèƒ½åˆ¶é€ ", "è´¨é‡æ§åˆ¶ã€é¢„æµ‹ç»´æŠ¤ã€ç”Ÿäº§ä¼˜åŒ–"],
          ["å¨±ä¹åª’ä½“", "å†…å®¹æ¨èã€æ¸¸æˆAIã€ç‰¹æ•ˆç”Ÿæˆ"]
        ]
      }
    }
```
æ˜¾ç„¶ï¼Œè¿™ä¸ªéœ€æ±‚ä¸‹ï¼Œç›´æ¥ä½¿ç”¨markdownè¯­æ³•å‘é€è¡¨æ ¼æ›´åŠ é€‚åˆï¼Œè€Œä¸æ˜¯ä½¿ç”¨A2UIï¼
"""
        content_append(request.messages, 'system', A2UI_messages)
    print(f"ç³»ç»Ÿæç¤ºï¼š{request.messages[0]['content']}")
    return request

def get_drs_stage(DRS_STAGE):
    if DRS_STAGE == 1:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºæ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µï¼Œä½ éœ€è¦åˆ†æç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶ç»™å‡ºæ˜ç¡®çš„éœ€æ±‚æè¿°ã€‚å¦‚æœç”¨æˆ·çš„éœ€æ±‚æè¿°ä¸æ˜ç¡®ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
    elif DRS_STAGE == 2:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºå·¥å…·è°ƒç”¨é˜¶æ®µï¼Œåˆ©ç”¨ä½ çš„çŸ¥è¯†åº“ã€äº’è”ç½‘æœç´¢ã€æ•°æ®åº“æŸ¥è¯¢ã€å„ç±»MCPç­‰ä½ æ‰€æœ‰çš„å·¥å…·ï¼ˆå¦‚æœæœ‰ï¼Œè¿™äº›å·¥å…·ä¸ä¸€å®šä¼šæä¾›ï¼‰ï¼Œæ‰§è¡Œè®¡åˆ’ä¸­æœªå®Œæˆçš„æ­¥éª¤ã€‚æ¯æ¬¡å®Œæˆè®¡åˆ’ä¸­çš„ä¸€ä¸ªæ­¥éª¤ã€‚åœ¨å·¥å…·è°ƒç”¨é˜¶æ®µä¸­ï¼Œä½ ä¸è¦å®Œæˆæœ€ç»ˆä»»åŠ¡ï¼Œè€Œæ˜¯å°½å¯èƒ½çš„è°ƒç”¨ç›¸å…³çš„å·¥å…·ï¼Œä¸ºæœ€åçš„å›ç­”é˜¶æ®µåšå‡†å¤‡ã€‚"
    elif DRS_STAGE == 3:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºç”Ÿæˆç»“æœé˜¶æ®µï¼Œæ ¹æ®å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå®Œæˆä»»åŠ¡ï¼Œç»™å‡ºä»»åŠ¡æ‰§è¡Œç»“æœã€‚å¦‚æœç”¨æˆ·è¦æ±‚ä½ ç”Ÿæˆä¸€ä¸ªè¶…è¿‡2000å­—çš„å›ç­”ï¼Œä½ å¯ä»¥å°è¯•å°†è¯¥ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯æ¬¡åªå®Œæˆå…¶ä¸­ä¸€ä¸ªéƒ¨åˆ†ã€‚"
    else:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºç”Ÿæˆç»“æœé˜¶æ®µï¼Œæ ¹æ®å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå®Œæˆä»»åŠ¡ï¼Œç»™å‡ºä»»åŠ¡æ‰§è¡Œç»“æœã€‚å¦‚æœç”¨æˆ·è¦æ±‚ä½ ç”Ÿæˆä¸€ä¸ªè¶…è¿‡2000å­—çš„å›ç­”ï¼Œä½ å¯ä»¥å°è¯•å°†è¯¥ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯æ¬¡åªå®Œæˆå…¶ä¸­ä¸€ä¸ªéƒ¨åˆ†ã€‚"
    return drs_msg  

def get_drs_stage_name(DRS_STAGE):
    if DRS_STAGE == 1:
        drs_stage_name = "æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ"
    elif DRS_STAGE == 2:
        drs_stage_name = "å·¥å…·è°ƒç”¨é˜¶æ®µ"
    elif DRS_STAGE == 3:
        drs_stage_name = "ç”Ÿæˆç»“æœé˜¶æ®µ"
    else:
        drs_stage_name = "ç”Ÿæˆç»“æœé˜¶æ®µ"
    return drs_stage_name

def get_drs_stage_system_message(DRS_STAGE,user_prompt,full_content):
    drs_stage_name = get_drs_stage_name(DRS_STAGE)
    if DRS_STAGE == 1:
        search_prompt = f"""
# å½“å‰çŠ¶æ€ï¼š

## åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

## å½“å‰ç»“æœï¼š
{full_content}

## å½“å‰é˜¶æ®µï¼š
{drs_stage_name}

# æ·±åº¦ç ”ç©¶ä¸€å…±æœ‰ä¸‰ä¸ªé˜¶æ®µï¼š1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: å·¥å…·è°ƒç”¨é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ

## å½“å‰é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š

### å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼ˆå¦‚æœä½ å·²ç»åœ¨ä¸Šä¸€è½®å¯¹è¯ä¸­å‘ç”¨æˆ·æå‡ºè¿‡æ˜ç¡®éœ€æ±‚ï¼Œè¯·ä¸è¦é‡å¤ä½¿ç”¨"need_more_info"ï¼Œè¿™ä¼šå¯¼è‡´ç”¨æˆ·æ— æ³•å¿«é€Ÿè·å–ç»“æœï¼‰ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}

### å¦‚æœä¸éœ€è¦è¿›ä¸€æ­¥æ˜ç¡®éœ€æ±‚ï¼Œè¿›å…¥å¹¶è¿›å…¥å·¥å…·è°ƒç”¨é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_work",
    "unfinished_task": ""
}}
"""
    elif DRS_STAGE == 2:
        search_prompt = f"""
# å½“å‰çŠ¶æ€ï¼š

## åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

## å½“å‰ç»“æœï¼š
{full_content}

## å½“å‰é˜¶æ®µï¼š
{drs_stage_name}

# æ·±åº¦ç ”ç©¶ä¸€å…±æœ‰ä¸‰ä¸ªé˜¶æ®µï¼š1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: å·¥å…·è°ƒç”¨é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ

## æ³¨æ„ï¼å·¥å…·è°ƒç”¨é˜¶æ®µï¼Œæ˜¯ä¸ºæœ€åçš„å›ç­”é˜¶æ®µåšå‡†å¤‡ã€‚ä¸éœ€è¦ç”Ÿæˆæœ€ç»ˆçš„å›ç­”ï¼Œå¦‚æœå·²ç»æ²¡æœ‰æœªå®Œæˆçš„éœ€è¦è°ƒç”¨å·¥å…·çš„æ­¥éª¤ï¼Œè¯·è¿›å…¥ç”Ÿæˆç»“æœé˜¶æ®µã€‚

## å½“å‰é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š

### å¦‚æœè¿˜æœ‰è®¡åˆ’ä¸­çš„éœ€è¦è°ƒç”¨å·¥å…·çš„æ­¥éª¤æ²¡æœ‰å®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_work",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„æ­¥éª¤"
}}

### å¦‚æœæ‰€æœ‰è®¡åˆ’çš„éœ€è¦è°ƒç”¨å·¥å…·çš„æ­¥éª¤éƒ½å·²å®Œæˆï¼Œè¿›å…¥ç”Ÿæˆç»“æœé˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "answer",
    "unfinished_task": ""
}}
"""    
    else:
        search_prompt = f"""
# å½“å‰çŠ¶æ€ï¼š

## åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

## å½“å‰ç»“æœï¼š
{full_content}

## å½“å‰é˜¶æ®µï¼š
{drs_stage_name}

# æ·±åº¦ç ”ç©¶ä¸€å…±æœ‰ä¸‰ä¸ªé˜¶æ®µï¼š1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: å·¥å…·è°ƒç”¨é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ

## å½“å‰é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š

å¦‚æœåˆå§‹ä»»åŠ¡å·²å®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœåˆå§‹ä»»åŠ¡æœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}
"""    
    return search_prompt

async def generate_stream_response(client,reasoner_client, request: ChatRequest, settings: dict,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search,async_tools_id):
    from mem0 import Memory
    global mcp_client_list,HA_client,ChromeMCP_client,sql_client
    DRS_STAGE = 1 # 1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: å·¥å…·è°ƒç”¨é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ
    if len(request.messages) > 2:
        DRS_STAGE = 2
    images = await images_in_messages(request.messages,fastapi_base_url)
    request.messages = await message_without_images(request.messages)
    from py.load_files import get_files_content,file_tool,image_tool
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        Bing_search_async,
        Google_search_async,
        Brave_search_async,
        Exa_search_async,
        Serper_search_async,
        bochaai_search_async,
        duckduckgo_tool, 
        searxng_tool, 
        tavily_tool, 
        bing_tool,
        google_tool,
        brave_tool,
        exa_tool,
        serper_tool,
        bochaai_tool,
        jina_crawler_tool, 
        Crawl4Ai_tool
    )
    from py.know_base import kb_tool,query_knowledge_base,rerank_knowledge_base
    from py.agent_tool import get_agent_tool
    from py.a2a_tool import get_a2a_tool
    from py.llm_tool import get_llm_tool
    from py.pollinations import pollinations_image_tool,openai_image_tool,openai_chat_image_tool
    from py.code_interpreter import e2b_code_tool,local_run_code_tool
    from py.utility_tools import (
        time_tool, 
        weather_tool,
        location_tool,
        timer_weather_tool,
        wikipedia_summary_tool,
        wikipedia_section_tool,
        arxiv_tool 
    ) 
    from py.autoBehavior import auto_behavior_tool
    from py.cli_tool import claude_code_tool,qwen_code_tool,get_tools_for_mode,get_local_tools_for_mode
    from py.cdp_tool import all_cdp_tools
    from py.random_topic import random_topics_tools
    m0 = None
    memoryId = None
    if settings["memorySettings"]["is_memory"] and settings["memorySettings"]["selectedMemory"] and settings["memorySettings"]["selectedMemory"] != "":
        memoryId = settings["memorySettings"]["selectedMemory"]
        cur_memory = None
        for memory in settings["memories"]:
            if memory["id"] == memoryId:
                cur_memory = memory
                break
        if cur_memory and cur_memory["providerId"]:
            print("é•¿æœŸè®°å¿†å¯ç”¨")
            config={
                "embedder": {
                    "provider": 'openai',
                    "config": {
                        "model": cur_memory['model'],
                        "api_key": cur_memory['api_key'],
                        "openai_base_url":cur_memory["base_url"],
                        "embedding_dims":cur_memory.get("embedding_dims", 1024)
                    },
                },
                "llm": {
                    "provider": 'openai',
                    "config": {
                        "model": settings['model'],
                        "api_key": settings['api_key'],
                        "openai_base_url":settings["base_url"]
                    }
                },
                "vector_store": {
                    "provider": "faiss",
                    "config": {
                        "collection_name": "agent-party",
                        "path": os.path.join(MEMORY_CACHE_DIR,memoryId),
                        "distance_strategy": "euclidean",
                        "embedding_model_dims": cur_memory.get("embedding_dims", 1024)
                    }
                }
            }
            m0 = Memory.from_config(config)
    open_tag = "<think>"
    close_tag = "</think>"
    try:
        tools = request.tools or []
        if mcp_client_list:
            for server_name, mcp_client in mcp_client_list.items():
                if server_name in settings['mcpServers']:
                    if 'disabled' not in settings['mcpServers'][server_name]:
                        settings['mcpServers'][server_name]['disabled'] = False
                    if settings['mcpServers'][server_name]['disabled'] == False and settings['mcpServers'][server_name]['processingStatus'] == 'ready':
                        disable_tools = []
                        for tool in settings['mcpServers'][server_name].get("tools", []): 
                            if tool.get("enabled", True) == False:
                                disable_tools.append(tool["name"])
                        function = await mcp_client.get_openai_functions(disable_tools=disable_tools)
                        if function:
                            tools.extend(function)
        get_llm_tool_fuction = await get_llm_tool(settings)
        if get_llm_tool_fuction:
            tools.append(get_llm_tool_fuction)
        get_agent_tool_fuction = await get_agent_tool(settings)
        if get_agent_tool_fuction:
            tools.append(get_agent_tool_fuction)
        get_a2a_tool_fuction = await get_a2a_tool(settings)
        if get_a2a_tool_fuction:
            tools.append(get_a2a_tool_fuction)
        if settings["HASettings"]["enabled"]:
            ha_tool = await HA_client.get_openai_functions(disable_tools=[])
            if ha_tool:
                tools.extend(ha_tool)
        if settings['chromeMCPSettings']['enabled'] and settings['chromeMCPSettings']['type']=='external':
            chromeMCP_tool = await ChromeMCP_client.get_openai_functions(disable_tools=[])
            if chromeMCP_tool:
                tools.extend(chromeMCP_tool)
        if settings['chromeMCPSettings']['enabled'] and settings['chromeMCPSettings']['type']=='internal':
            tools.extend(all_cdp_tools)
        if settings['sqlSettings']['enabled']:
            sql_tool = await sql_client.get_openai_functions(disable_tools=[])
            if sql_tool:
                tools.extend(sql_tool)
        if settings['CLISettings']['enabled']:
            if settings['CLISettings']['engine'] == 'cc':
                tools.append(claude_code_tool)
            elif settings['CLISettings']['engine'] == 'qc':
                tools.append(qwen_code_tool)
            elif settings['CLISettings']['engine'] == 'ds':
                tools.extend(get_tools_for_mode('yolo'))
            elif settings['CLISettings']['engine'] == 'local':
                tools.extend(get_local_tools_for_mode('yolo'))
        if settings['tools']['time']['enabled'] and settings['tools']['time']['triggerMode'] == 'afterThinking':
            tools.append(time_tool)
        if settings["tools"]["weather"]['enabled']:
            tools.append(weather_tool)
            tools.append(location_tool)
            tools.append(timer_weather_tool)
        if settings["tools"]["wikipedia"]['enabled']:
            tools.append(wikipedia_summary_tool)
            tools.append(wikipedia_section_tool)
        if settings["tools"]["randomTopic"]['enabled']:
            tools.extend(random_topics_tools)
        if settings["tools"]["arxiv"]['enabled']:
            tools.append(arxiv_tool)
        if settings['text2imgSettings']['enabled']:
            if settings['text2imgSettings']['engine'] == 'pollinations':
                tools.append(pollinations_image_tool)
            elif settings['text2imgSettings']['engine'] == 'openai':
                tools.append(openai_image_tool)
            elif settings['text2imgSettings']['engine'] == 'openaiChat':
                tools.append(openai_chat_image_tool)
        if settings['tools']['getFile']['enabled']:
            tools.append(file_tool)
            tools.append(image_tool)
        if settings['tools']['autoBehavior']['enabled'] and request.messages[-1]['role'] == 'user':
            tools.append(auto_behavior_tool)
        if settings["codeSettings"]['enabled']:
            if settings["codeSettings"]["engine"] == "e2b":
                tools.append(e2b_code_tool)
            elif settings["codeSettings"]["engine"] == "sandbox":
                tools.append(local_run_code_tool)
        if settings["custom_http"]:
            for custom_http in settings["custom_http"]:
                if custom_http["enabled"]:
                    if custom_http['body'] == "":
                        custom_http['body'] = "{}"
                    custom_http_tool = {
                        "type": "function",
                        "function": {
                            "name": f"custom_http_{custom_http['name']}",
                            "description": f"{custom_http['description']}",
                            "parameters": json.loads(custom_http['body']),
                        },
                    }
                    tools.append(custom_http_tool)
        if settings["workflows"]:
            for workflow in settings["workflows"]:
                if workflow["enabled"]:
                    comfyui_properties = {}
                    comfyui_required = []
                    if workflow["text_input"] is not None:
                        comfyui_properties["text_input"] = {
                            "description": "ç¬¬ä¸€ä¸ªæ–‡å­—è¾“å…¥ï¼Œéœ€è¦è¾“å…¥çš„æç¤ºè¯ï¼Œç”¨äºç”Ÿæˆå›¾ç‰‡æˆ–è€…è§†é¢‘ï¼Œå¦‚æœæ— ç‰¹åˆ«æç¤ºï¼Œé»˜è®¤ä¸ºè‹±æ–‡",
                            "type": "string"
                        }
                        comfyui_required.append("text_input")
                    if workflow["text_input_2"] is not None:
                        comfyui_properties["text_input_2"] = {
                            "description": "ç¬¬äºŒä¸ªæ–‡å­—è¾“å…¥ï¼Œéœ€è¦è¾“å…¥çš„æç¤ºè¯ï¼Œç”¨äºç”Ÿæˆå›¾ç‰‡æˆ–è€…è§†é¢‘ï¼Œå¦‚æœæ— ç‰¹åˆ«æç¤ºï¼Œé»˜è®¤ä¸ºè‹±æ–‡",
                            "type": "string"
                        }
                        comfyui_required.append("text_input_2")
                    if workflow["image_input"] is not None:
                        comfyui_properties["image_input"] = {
                            "description": "ç¬¬ä¸€ä¸ªå›¾ç‰‡è¾“å…¥ï¼Œéœ€è¦è¾“å…¥çš„å›¾ç‰‡ï¼Œå¿…é¡»æ˜¯å›¾ç‰‡URLï¼Œå¯ä»¥æ˜¯å¤–éƒ¨é“¾æ¥ï¼Œä¹Ÿå¯ä»¥æ˜¯æœåŠ¡å™¨å†…éƒ¨çš„URLï¼Œä¾‹å¦‚ï¼šhttps://www.example.com/xxx.png  æˆ–è€…  http://127.0.0.1:3456/xxx.jpg",
                            "type": "string"
                        }
                        comfyui_required.append("image_input")
                    if workflow["image_input_2"] is not None:
                        comfyui_properties["image_input_2"] = {
                            "description": "ç¬¬äºŒä¸ªå›¾ç‰‡è¾“å…¥ï¼Œéœ€è¦è¾“å…¥çš„å›¾ç‰‡ï¼Œå¿…é¡»æ˜¯å›¾ç‰‡URLï¼Œå¯ä»¥æ˜¯å¤–éƒ¨é“¾æ¥ï¼Œä¹Ÿå¯ä»¥æ˜¯æœåŠ¡å™¨å†…éƒ¨çš„URLï¼Œä¾‹å¦‚ï¼šhttps://www.example.com/xxx.png  æˆ–è€…  http://127.0.0.1:3456/xxx.jpg",
                            "type": "string"
                        }
                        comfyui_required.append("image_input_2")
                    comfyui_parameters = {
                        "type": "object",
                        "properties": comfyui_properties,
                        "required": comfyui_required
                    }
                    comfyui_tool = {
                        "type": "function",
                        "function": {
                            "name": f"comfyui_{workflow['unique_filename']}",
                            "description": f"{workflow['description']}+\nå¦‚æœè¦è¾“å…¥å›¾ç‰‡æç¤ºè¯æˆ–è€…ä¿®æ”¹æç¤ºè¯ï¼Œå°½å¯èƒ½ä½¿ç”¨è‹±è¯­ã€‚\nè¿”å›çš„å›¾ç‰‡ç»“æœï¼Œè¯·å°†å›¾ç‰‡çš„URLæ”¾å…¥![image]()è¿™æ ·çš„markdownè¯­æ³•ä¸­ï¼Œç”¨æˆ·æ‰èƒ½çœ‹åˆ°å›¾ç‰‡ã€‚å¦‚æœæ˜¯è§†é¢‘ï¼Œè¯·å°†è§†é¢‘çš„URLæ”¾å…¥<video controls> <source src=''></video>çš„ä¸­srcä¸­ï¼Œç”¨æˆ·æ‰èƒ½çœ‹åˆ°è§†é¢‘ã€‚å¦‚æœæœ‰å¤šä¸ªç»“æœï¼Œåˆ™è¯·ç”¨æ¢è¡Œç¬¦åˆ†éš”å¼€è¿™å‡ ä¸ªå›¾ç‰‡æˆ–è€…è§†é¢‘ï¼Œç”¨æˆ·æ‰èƒ½çœ‹åˆ°å¤šä¸ªç»“æœã€‚",
                            "parameters": comfyui_parameters,
                        },
                    }
                    tools.append(comfyui_tool)
        print(tools)
        source_prompt = ""
        if request.fileLinks:
            print("fileLinks",request.fileLinks)
            # å¼‚æ­¥è·å–æ–‡ä»¶å†…å®¹
            files_content = await get_files_content(request.fileLinks)
            fileLinks_message = f"\n\nç›¸å…³æ–‡ä»¶å†…å®¹ï¼š{files_content}"
            
            # ä¿®å¤å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
            content_append(request.messages, 'system', fileLinks_message)
            source_prompt += fileLinks_message
        user_prompt = request.messages[-1]['content']
        if settings["memorySettings"]["is_memory"] and settings["memorySettings"]["selectedMemory"] and settings["memorySettings"]["selectedMemory"] != "":
            if settings["memorySettings"]["userName"]:
                print("æ·»åŠ ç”¨æˆ·åï¼š\n\n" + settings["memorySettings"]["userName"] + "\n\nç”¨æˆ·åç»“æŸ\n\n")
                content_append(request.messages, 'system', "ä¸ä½ äº¤æµçš„ç”¨æˆ·åä¸ºï¼š\n\n" + settings["memorySettings"]["userName"] + "\n\n")
            lore_content = ""
            assistant_reply = ""
            # æ‰¾å‡ºrequest.messagesä¸­ä¸Šæ¬¡çš„assistantå›å¤
            for i in range(len(request.messages)-1, -1, -1):
                if request.messages[i]['role'] == 'assistant':
                    assistant_reply = request.messages[i]['content']
                    break
            if cur_memory["characterBook"]:
                for lore in cur_memory["characterBook"]:
                    # lore['keysRaw'] æŒ‰ç…§æ¢è¡Œç¬¦åˆ†å‰²ï¼Œå¹¶å»é™¤ç©ºå­—ç¬¦ä¸²
                    lore_keys = lore["keysRaw"].split("\n")
                    lore_keys = [key for key in lore_keys if key != ""]
                    print(lore_keys)
                    # å¦‚æœlore_keysä¸ä¸ºç©ºï¼Œå¹¶ä¸”lore_keysçš„ä»»æ„ä¸€ä¸ªå…ƒç´ åœ¨user_promptæˆ–è€…assistant_replyä¸­ï¼Œåˆ™æ·»åŠ lore['content']åˆ°lore_contentä¸­
                    if lore_keys != [] and any(key in user_prompt or key in assistant_reply for key in lore_keys):
                        lore_content += lore['content'] + "\n\n"
            if lore_content:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢lore_contentä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    lore_content = lore_content.replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢lore_contentä¸­çš„{{char}}ä¸ºcur_memory["name"]
                lore_content = lore_content.replace("{{char}}", cur_memory["name"])
                print("æ·»åŠ ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n")
                content_append(request.messages, 'system', "ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n")
            if cur_memory["description"]:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢cur_memory["description"]ä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    cur_memory["description"] = cur_memory["description"].replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢cur_memory["description"]ä¸­çš„{{char}}ä¸ºcur_memory["name"]
                cur_memory["description"] = cur_memory["description"].replace("{{char}}", cur_memory["name"])
                print("æ·»åŠ è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["description"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n")
                content_append(request.messages, 'system', "è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["description"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n")
            if cur_memory["personality"]:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢cur_memory["personality"]ä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    cur_memory["personality"] = cur_memory["personality"].replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢cur_memory["personality"]ä¸­çš„{{char}}ä¸ºcur_memory["name"]
                cur_memory["personality"] = cur_memory["personality"].replace("{{char}}", cur_memory["name"])
                print("æ·»åŠ æ€§æ ¼è®¾å®šï¼š\n\n" + cur_memory["personality"] + "\n\næ€§æ ¼è®¾å®šç»“æŸ\n\n")
                content_append(request.messages, 'system', "æ€§æ ¼è®¾å®šï¼š\n\n" + cur_memory["personality"] + "\n\næ€§æ ¼è®¾å®šç»“æŸ\n\n") 
            if cur_memory['mesExample']:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢cur_memory["mesExample"]ä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    cur_memory["mesExample"] = cur_memory["mesExample"].replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢cur_memory["mesExample"]ä¸­çš„{{char}}ä¸ºcur_memory["name"]
                cur_memory["mesExample"] = cur_memory["mesExample"].replace("{{char}}", cur_memory["name"])
                print("æ·»åŠ å¯¹è¯ç¤ºä¾‹ï¼š\n\n" + cur_memory['mesExample'] + "\n\nå¯¹è¯ç¤ºä¾‹ç»“æŸ\n\n")
                content_append(request.messages, 'system', "å¯¹è¯ç¤ºä¾‹ï¼š\n\n" + cur_memory['mesExample'] + "\n\nå¯¹è¯ç¤ºä¾‹ç»“æŸ\n\n")
            if cur_memory["systemPrompt"]:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢cur_memory["systemPrompt"]ä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    cur_memory["systemPrompt"] = cur_memory["systemPrompt"].replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢cur_memory["systemPrompt"]ä¸­çš„{{char}}ä¸ºcur_memory["name"]
                cur_memory["systemPrompt"] = cur_memory["systemPrompt"].replace("{{char}}", cur_memory["name"])
                content_append(request.messages, 'system', "\n\n" + cur_memory["systemPrompt"] + "\n\n")
            if settings["memorySettings"]["genericSystemPrompt"]:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢settings["memorySettings"]["genericSystemPrompt"]ä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    settings["memorySettings"]["genericSystemPrompt"] = settings["memorySettings"]["genericSystemPrompt"].replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢cur_memory["systemPrompt"]ä¸­çš„{{char}}ä¸ºcur_memory["name"]
                settings["memorySettings"]["genericSystemPrompt"] = settings["memorySettings"]["genericSystemPrompt"].replace("{{char}}", cur_memory["name"])
                content_append(request.messages, 'system', "\n\n" + settings["memorySettings"]["genericSystemPrompt"] + "\n\n")
            if m0:
                memoryLimit = settings["memorySettings"]["memoryLimit"]
                try:
                    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šä½¿ç”¨ asyncio.to_thread å°†åŒæ­¥çš„ search æ–¹æ³•æ”¾å…¥çº¿ç¨‹æ± è¿è¡Œ
                    # è¿™æ ·ä¸»çº¿ç¨‹ï¼ˆEvent Loopï¼‰ä¼šè¢«é‡Šæ”¾ï¼Œå¯ä»¥å»å¤„ç† /minilm/embeddings è¯·æ±‚ï¼Œä»è€Œé¿å…æ­»é”
                    relevant_memories = await asyncio.to_thread(
                        m0.search, 
                        query=user_prompt, 
                        user_id=memoryId, 
                        limit=memoryLimit
                    )
                    relevant_memories = json.dumps(relevant_memories, ensure_ascii=False)
                except Exception as e:
                    print("m0.search error:",e)
                    relevant_memories = ""
                print("æ·»åŠ ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n")
                content_append(request.messages, 'system', "ä¹‹å‰çš„ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n")                   
        request = await tools_change_messages(request, settings)
        chat_vendor = 'OpenAI'
        reasoner_vendor = 'OpenAI'
        for modelProvider in settings['modelProviders']: 
            if modelProvider['id'] == settings['selectedProvider']:
                chat_vendor = modelProvider['vendor']
                break
        for modelProvider in settings['modelProviders']: 
            if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                reasoner_vendor = modelProvider['vendor']
                break
        if chat_vendor == 'Dify':
            try:
                if len(request.messages) >= 3:
                    if request.messages[2]['role'] == 'user':
                        if request.messages[1]['role'] == 'assistant':
                            request.messages[2]['content'] = "ä½ ä¸Šä¸€æ¬¡çš„å‘è¨€ï¼š\n" +request.messages[0]['content'] + "\nä½ ä¸Šä¸€æ¬¡çš„å‘è¨€ç»“æŸ\n\nç”¨æˆ·ï¼š" + request.messages[2]['content']
                        if request.messages[0]['role'] == 'system':
                            request.messages[2]['content'] = "ç³»ç»Ÿæç¤ºï¼š\n" +request.messages[0]['content'] + "\nç³»ç»Ÿæç¤ºç»“æŸ\n\n" + request.messages[2]['content']
                elif len(request.messages) >= 2:
                    if request.messages[1]['role'] == 'user':
                        if request.messages[0]['role'] == 'system':
                            request.messages[1]['content'] = "ç³»ç»Ÿæç¤ºï¼š\n" +request.messages[0]['content'] + "\nç³»ç»Ÿæç¤ºç»“æŸ\n\nç”¨æˆ·ï¼š" + request.messages[1]['content']
            except Exception as e:
                print("Dify error:",e)
        model = settings['model']
        extra_params = settings['extra_params']
        # ç§»é™¤extra_paramsè¿™ä¸ªlistä¸­"name"ä¸åŒ…å«éç©ºç™½ç¬¦çš„é”®å€¼å¯¹
        if extra_params:
            for extra_param in extra_params:
                if not extra_param['name'].strip():
                    extra_params.remove(extra_param)
            # åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸
            extra_params = {item['name']: item['value'] for item in extra_params}
        else:
            extra_params = {}
        async def stream_generator(user_prompt,DRS_STAGE):
            # ---------- ç»Ÿä¸€ SSE å°è£… ----------
            def make_sse(tool_data: dict) -> str:
                chunk = {
                    "choices": [{
                        "delta": {
                            "tool_content": tool_data, # è¿™é‡Œç›´æ¥ä¼ å­—å…¸
                        }
                    }]
                }
                return f"data: {json.dumps(chunk)}\n\n"
            try:
                extra = {}
                reasoner_extra = {}
                if chat_vendor == 'OpenAI':
                    extra['max_completion_tokens'] = request.max_tokens or settings['max_tokens']
                else:
                    extra['max_tokens'] = request.max_tokens or settings['max_tokens']
                if settings.get('enableOmniTTS',False):
                    extra['modalities'] = ["text", "audio"]
                    extra['audio'] ={"voice": settings.get('omniVoice',"Cherry"), "format": "wav"}
                if reasoner_vendor == 'OpenAI':
                    reasoner_extra['max_completion_tokens'] = settings['reasoner']['max_tokens']
                else:
                    reasoner_extra['max_tokens'] = settings['reasoner']['max_tokens']
                if request.reasoning_effort or settings['reasoning_effort']:
                    extra['reasoning_effort'] = request.reasoning_effort or settings['reasoning_effort']
                if settings['reasoner']['reasoning_effort'] is not None:
                    reasoner_extra['reasoning_effort'] = settings['reasoner']['reasoning_effort']
                # å¤„ç†ä¼ å…¥çš„å¼‚æ­¥å·¥å…·IDæŸ¥è¯¢
                if async_tools_id:
                    responses_to_send = []
                    responses_to_wait = []
                    async with async_tools_lock:
                        # æ”¶é›†å·²å®Œæˆçš„ç»“æœå¹¶åˆ é™¤æ¡ç›®
                        for tid in list(async_tools.keys()):  # è½¬æˆlisté¿å…å­—å…¸ä¿®æ”¹å¼‚å¸¸
                            if tid in async_tools_id:
                                if async_tools[tid]["status"] in ("completed", "error"):
                                    responses_to_send.append({
                                        "tool_id": tid,
                                        **async_tools.pop(tid)  # ç§»é™¤å·²å¤„ç†çš„æ¡ç›®
                                    })
                                elif async_tools[tid]["status"] == "pending":
                                    responses_to_wait.append({
                                        "tool_id": tid,
                                        "name":async_tools[tid]["name"],
                                        "parameters": async_tools[tid]["parameters"]
                                    })
                    for response in responses_to_send:
                        tid = response["tool_id"]
                        if response["status"] == "completed":
                            tool_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": response["name"], "content": str(response["result"]), "type": "tool_result"},
                                        "async_tool_id": tid,
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(tool_chunk)}\n\n"
                            request.messages.insert(-1, 
                                {
                                    "tool_calls": [
                                        {
                                            "id": "agentParty",
                                            "function": {
                                                "arguments": json.dumps(response["parameters"]),
                                                "name": response["name"],
                                            },
                                            "type": "function",
                                        }
                                    ],
                                    "role": "assistant",
                                    "content": "",
                                }
                            )
                            request.messages.insert(-1, 
                                {
                                    "role": "tool",
                                    "tool_call_id": "agentParty",
                                    "name": response["name"],
                                    "content": f"ä¹‹å‰è°ƒç”¨çš„å¼‚æ­¥å·¥å…·ï¼ˆ{tid}ï¼‰çš„ç»“æœï¼š\n\n{response['result']}\n\n====ç»“æœç»“æŸ====\n\nä½ å¿…é¡»æ ¹æ®å·¥å…·ç»“æœå›å¤æœªå›å¤çš„é—®é¢˜æˆ–éœ€æ±‚ã€‚è¯·ä¸è¦é‡å¤è°ƒç”¨è¯¥å·¥å…·ï¼"
                                }
                            )
                        if response["status"] == "error":
                            tool_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": f"{tid}{await t('tool_result')}", "content": f"Error: {str(response['result'])}"},
                                        "async_tool_id": tid
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(tool_chunk)}\n\n"
                            request.messages.append({
                                "role": "system",
                                "content": f"ä¹‹å‰è°ƒç”¨çš„å¼‚æ­¥å·¥å…·ï¼ˆ{tid}ï¼‰å‘ç”Ÿé”™è¯¯ï¼š\n\n{response['result']}\n\n====é”™è¯¯ç»“æŸ====\n\n"
                            }) 
                    for response in responses_to_wait:
                        # åœ¨request.messageså€’æ•°ç¬¬ä¸€ä¸ªå…ƒç´ ä¹‹å‰çš„ä½ç½®æ’å…¥ä¸€ä¸ªæ–°å…ƒç´ 
                        request.messages.insert(-1, 
                            {
                                "tool_calls": [
                                    {
                                        "id": "agentParty",
                                        "function": {
                                            "arguments": json.dumps(response["parameters"]),
                                            "name": response["name"],
                                        },
                                        "type": "function",
                                    }
                                ],
                                "role": "assistant",
                                "content": "",
                            }
                        )
                        results = f"{response["name"]}å·¥å…·å·²æˆåŠŸå¯åŠ¨ï¼Œè·å–ç»“æœéœ€è¦èŠ±è´¹å¾ˆä¹…çš„æ—¶é—´ã€‚è¯·ä¸è¦å†æ¬¡è°ƒç”¨è¯¥å·¥å…·ï¼Œå› ä¸ºå·¥å…·ç»“æœå°†ç”Ÿæˆåè‡ªåŠ¨å‘é€ï¼Œå†æ¬¡è°ƒç”¨ä¹Ÿä¸èƒ½æ›´å¿«çš„è·å–åˆ°ç»“æœã€‚è¯·ç›´æ¥å‘Šè¯‰ç”¨æˆ·ï¼Œä½ ä¼šåœ¨è·å¾—ç»“æœåå›ç­”ä»–çš„é—®é¢˜ã€‚"
                        request.messages.insert(-1, 
                            {
                                "role": "tool",
                                "tool_call_id": "agentParty",
                                "name": response["name"],
                                "content": str(results),
                            }
                        )
                kb_list = []
                if settings["knowledgeBases"]:
                    for kb in settings["knowledgeBases"]:
                        if kb["enabled"] and kb["processingStatus"] == "completed":
                            kb_list.append({"kb_id":kb["id"],"name": kb["name"],"introduction":kb["introduction"]})
                if settings["KBSettings"]["when"] == "before_thinking" or settings["KBSettings"]["when"] == "both":
                    if kb_list:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "tool_content": {"title": "query_knowledge_base", "content": "", "type": "call"},
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                        all_kb_content = []
                        # ç”¨query_knowledge_baseå‡½æ•°æŸ¥è¯¢kb_listä¸­æ‰€æœ‰çš„çŸ¥è¯†åº“
                        for kb in kb_list:
                            kb_content = await query_knowledge_base(kb["kb_id"],user_prompt)
                            all_kb_content.extend(kb_content)
                            if settings["KBSettings"]["is_rerank"]:
                                all_kb_content = await rerank_knowledge_base(user_prompt,all_kb_content)
                        if all_kb_content:
                            all_kb_content = json.dumps(all_kb_content, ensure_ascii=False, indent=4)
                            kb_message = f"\n\nå¯å‚è€ƒçš„çŸ¥è¯†åº“å†…å®¹ï¼š{all_kb_content}"
                            content_append(request.messages, 'user',  f"{kb_message}\n\nç”¨æˆ·ï¼š{user_prompt}")
                            tool_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": "query_knowledge_base", "content": str(all_kb_content), "type": "tool_result"},
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(tool_chunk)}\n\n"
                if settings["KBSettings"]["when"] == "after_thinking" or settings["KBSettings"]["when"] == "both":
                    if kb_list:
                        kb_list_message = f"\n\nå¯è°ƒç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨ï¼š{json.dumps(kb_list, ensure_ascii=False)}"
                        content_append(request.messages, 'system', kb_list_message)
                else:
                    kb_list = []
                if settings['webSearch']['enabled'] or enable_web_search:
                    if settings['webSearch']['when'] == 'before_thinking' or settings['webSearch']['when'] == 'both':
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "tool_content": {"title": "web_search", "content": "", "type": "call"},
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                        if settings['webSearch']['engine'] == 'duckduckgo':
                            results = await DDGsearch_async(user_prompt)
                        elif settings['webSearch']['engine'] == 'searxng':
                            results = await searxng_async(user_prompt)
                        elif settings['webSearch']['engine'] == 'tavily':
                            results = await Tavily_search_async(user_prompt)
                        elif settings['webSearch']['engine'] == 'bing':
                            results = await Bing_search_async(user_prompt)
                        elif settings['webSearch']['engine'] == 'google':
                            results = await Google_search_async(user_prompt)
                        elif settings['webSearch']['engine'] == 'brave':
                            results = await Brave_search_async(user_prompt)
                        elif settings['webSearch']['engine'] == 'exa':
                            results = await Exa_search_async(user_prompt)
                        elif settings['webSearch']['engine'] == 'serper':
                            results = await Serper_search_async(user_prompt)
                        elif settings['webSearch']['engine'] == 'bochaai':
                            results = await bochaai_search_async(user_prompt)
                        if results:
                            content_append(request.messages, 'user',  f"\n\nè”ç½‘æœç´¢ç»“æœï¼š{results}\n\nè¯·æ ¹æ®è”ç½‘æœç´¢ç»“æœç»„ç»‡ä½ çš„å›ç­”ï¼Œå¹¶ç¡®ä¿ä½ çš„å›ç­”æ˜¯å‡†ç¡®çš„ã€‚")
                            tool_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": "web_search", "content": str(results), "type": "tool_result"},
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(tool_chunk)}\n\n"
                    if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                        if settings['webSearch']['engine'] == 'duckduckgo':
                            tools.append(duckduckgo_tool)
                        elif settings['webSearch']['engine'] == 'searxng':
                            tools.append(searxng_tool)
                        elif settings['webSearch']['engine'] == 'tavily':
                            tools.append(tavily_tool)
                        elif settings['webSearch']['engine'] == 'bing':
                            tools.append(bing_tool)
                        elif settings['webSearch']['engine'] == 'google':
                            tools.append(google_tool)
                        elif settings['webSearch']['engine'] == 'brave':
                            tools.append(brave_tool)
                        elif settings['webSearch']['engine'] == 'exa':
                            tools.append(exa_tool)
                        elif settings['webSearch']['crawler'] == 'serper':
                            tools.append(serper_tool)
                        elif settings['webSearch']['crawler'] == 'bochaai':
                            tools.append(bochaai_tool)

                        if settings['webSearch']['crawler'] == 'jina':
                            tools.append(jina_crawler_tool)
                        elif settings['webSearch']['crawler'] == 'crawl4ai':
                            tools.append(Crawl4Ai_tool)
                if kb_list:
                    tools.append(kb_tool)
                if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                    deepsearch_messages = copy.deepcopy(request.messages)
                    content_append(deepsearch_messages, 'user',  "\n\nå°†ç”¨æˆ·æå‡ºçš„é—®é¢˜æˆ–ç»™å‡ºçš„å½“å‰ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤ï¼Œæ¯ä¸€ä¸ªæ­¥éª¤ç”¨ä¸€å¥ç®€çŸ­çš„è¯æ¦‚æ‹¬å³å¯ï¼Œæ— éœ€å›ç­”æˆ–æ‰§è¡Œè¿™äº›å†…å®¹ï¼Œç›´æ¥è¿”å›æ€»ç»“å³å¯ï¼Œä½†ä¸èƒ½çœç•¥é—®é¢˜æˆ–ä»»åŠ¡çš„ç»†èŠ‚ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„åªæ˜¯é—²èŠæˆ–è€…ä¸åŒ…å«ä»»åŠ¡å’Œé—®é¢˜ï¼Œç›´æ¥æŠŠç”¨æˆ·è¾“å…¥é‡å¤è¾“å‡ºä¸€éå³å¯ã€‚å¦‚æœæ˜¯éå¸¸ç®€å•çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥åªç»™å‡ºä¸€ä¸ªæ­¥éª¤å³å¯ã€‚ä¸€èˆ¬æƒ…å†µä¸‹éƒ½æ˜¯éœ€è¦æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤çš„ã€‚")
                    response = await client.chat.completions.create(
                        model=model,
                        messages=deepsearch_messages,
                        temperature=0.5,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                    user_prompt = response.choices[0].message.content
                    deepsearch_chunk = {
                        "choices": [{
                            "delta": {
                                "tool_content": {"title": "deep_research", "content": user_prompt, "type": "call"},
                            }
                        }]
                    }
                    yield f"data: {json.dumps(deepsearch_chunk)}\n\n"
                    content_append(request.messages, 'user',  f"\n\nå¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºé—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œç›´æ¥é—²èŠå³å¯ï¼Œå¦‚æœç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œä»»åŠ¡æè¿°ä¸æ¸…æ™°æˆ–è€…ä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚")
                # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å‹
                if settings['reasoner']['enabled'] or enable_thinking:
                    reasoner_messages = copy.deepcopy(request.messages)
                    if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                        content_append(reasoner_messages, 'user',  f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n")
                        drs_msg = get_drs_stage(DRS_STAGE)
                        if drs_msg:
                            content_append(reasoner_messages, 'user',  f"\n\n{drs_msg}\n\n")
                    if tools:
                        content_append(reasoner_messages, 'system',  f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}")
                    for modelProvider in settings['modelProviders']: 
                        if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                            vendor = modelProvider['vendor']
                            break
                    msg = await images_add_in_messages(reasoner_messages, images,settings)
                    if vendor == 'Ollama':
                        # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                        reasoner_stream = await reasoner_client.chat.completions.create(
                            model=settings['reasoner']['model'],
                            messages=msg,
                            stream=True,
                            temperature=settings['reasoner']['temperature'],
                            **reasoner_extra
                        )
                        full_reasoning = ""
                        buffer = ""  # è·¨chunkçš„å†…å®¹ç¼“å†²åŒº
                        in_reasoning = False  # æ˜¯å¦åœ¨æ ‡ç­¾å†…
                        
                        async for chunk in reasoner_stream:
                            if not chunk.choices:
                                continue
                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0].get("delta", {})
                            if delta:
                                current_content = delta.get("content", "")
                                buffer += current_content  # ç´¯ç§¯åˆ°ç¼“å†²åŒº
                                
                                # å®æ—¶å¤„ç†ç¼“å†²åŒºå†…å®¹
                                while True:
                                    reasoning_content = delta.get("reasoning_content", "")
                                    if reasoning_content:
                                        full_reasoning += reasoning_content
                                    else:
                                        reasoning_content = delta.get("reasoning", "")
                                        if reasoning_content:
                                            delta['reasoning_content'] = reasoning_content
                                            full_reasoning += reasoning_content
                                    if reasoning_content:
                                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                                        break
                                    if not in_reasoning:
                                        # å¯»æ‰¾å¼€æ”¾æ ‡ç­¾
                                        start_pos = buffer.find(open_tag)
                                        if start_pos != -1:
                                            # å¼€æ”¾æ ‡ç­¾å‰çš„å†…å®¹ï¼ˆéæ€è€ƒå†…å®¹ï¼‰
                                            non_reasoning = buffer[:start_pos]
                                            buffer = buffer[start_pos+len(open_tag):]
                                            in_reasoning = True
                                        else:
                                            break  # æ— å¼€æ”¾æ ‡ç­¾ï¼Œä¿ç•™åç»­å¤„ç†
                                    else:
                                        # å¯»æ‰¾é—­åˆæ ‡ç­¾
                                        end_pos = buffer.find(close_tag)
                                        if end_pos != -1:
                                            # æå–æ€è€ƒå†…å®¹å¹¶æ„é€ å“åº”
                                            reasoning_part = buffer[:end_pos]
                                            chunk_dict["choices"][0]["delta"] = {
                                                "reasoning_content": reasoning_part,
                                                "content": ""  # æ¸…é™¤éæ€è€ƒå†…å®¹
                                            }
                                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                                            full_reasoning += reasoning_part
                                            buffer = buffer[end_pos+len(close_tag):]
                                            in_reasoning = False
                                        else:
                                            # å‘é€æœªé—­åˆçš„ä¸­é—´å†…å®¹
                                            if buffer:
                                                chunk_dict["choices"][0]["delta"] = {
                                                    "reasoning_content": buffer,
                                                    "content": ""
                                                }
                                                yield f"data: {json.dumps(chunk_dict)}\n\n"
                                                full_reasoning += buffer
                                                buffer = ""
                                            break  # ç­‰å¾…æ›´å¤šå†…å®¹
                    else:
                        # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                        reasoner_stream = await reasoner_client.chat.completions.create(
                            model=settings['reasoner']['model'],
                            messages=msg,
                            stream=True,
                            stop=settings['reasoner']['stop_words'],
                            temperature=settings['reasoner']['temperature'],
                            **reasoner_extra
                        )
                        full_reasoning = ""
                        # å¤„ç†æ¨ç†æ¨¡å‹çš„æµå¼å“åº”
                        async for chunk in reasoner_stream:
                            if not chunk.choices:
                                continue

                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0].get("delta", {})
                            if delta:
                                reasoning_content = delta.get("reasoning_content", "")
                                if reasoning_content:
                                    full_reasoning += reasoning_content
                                else:
                                    reasoning_content = delta.get("reasoning", "")
                                    if reasoning_content:
                                        delta['reasoning_content'] = reasoning_content
                                        full_reasoning += reasoning_content
                                # ç§»é™¤contentå­—æ®µï¼Œç¡®ä¿yieldçš„å†…å®¹ä¸­ä¸åŒ…å«content
                                if 'content' in delta:
                                    del delta['content']
                            yield f"data: {json.dumps(chunk_dict)}\n\n"

                    # åœ¨æ¨ç†ç»“æŸåæ·»åŠ å®Œæ•´æ¨ç†å†…å®¹åˆ°æ¶ˆæ¯
                    content_append(request.messages, 'assistant', f"<think>\n{full_reasoning}\n</think>")  # å¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹
                # çŠ¶æ€è·Ÿè¸ªå˜é‡
                in_reasoning = False
                reasoning_buffer = []
                content_buffer = []
                if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                    content_append(request.messages, 'user',  f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n")
                    drs_msg = get_drs_stage(DRS_STAGE)
                    if drs_msg:
                        content_append(request.messages, 'user',  f"\n\n{drs_msg}\n\n")
                msg = await images_add_in_messages(request.messages, images,settings)
                if tools:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                        temperature=request.temperature,
                        tools=tools,
                        stream=True,
                        top_p=request.top_p or settings['top_p'],
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                        **extra
                    )
                else:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                        temperature=request.temperature,
                        stream=True,
                        top_p=request.top_p or settings['top_p'],
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                        **extra
                    )
                tool_calls = []
                full_content = ""
                search_not_done = False
                search_task = ""
                is_tool_call = False
                async for chunk in response:
                    if not chunk.choices:
                        continue
                    choice = chunk.choices[0]
                    if choice.delta.tool_calls:  # function_calling
                        is_tool_call = True
                        for idx, tool_call in enumerate(choice.delta.tool_calls):
                            tool = choice.delta.tool_calls[idx]
                            if len(tool_calls) <= idx:
                                tool_calls.append(tool)
                                continue
                            if tool.function.arguments:
                                # functionå‚æ•°ä¸ºæµå¼å“åº”ï¼Œéœ€è¦æ‹¼æ¥
                                if tool_calls[idx].function.arguments:
                                    tool_calls[idx].function.arguments += tool.function.arguments
                                else:
                                    tool_calls[idx].function.arguments = tool.function.arguments
                            current_tool = tool_calls[idx]
                            if current_tool.function and current_tool.function.name:
                                progress_chunk = {
                                    "choices": [{
                                        "delta": {
                                            "tool_progress": {  # æ–°å¢å­—æ®µï¼ŒåŒºåˆ«äºæœ€ç»ˆçš„ tool_content
                                                "name": current_tool.function.name,
                                                "arguments": current_tool.function.arguments or "",
                                                "index": idx,
                                                "id": current_tool.id or f"call_{idx}"
                                            }
                                        }
                                    }]
                                }
                                yield f"data: {json.dumps(progress_chunk)}\n\n"
                    else:
                        if hasattr(choice.delta, "audio") and choice.delta.audio and is_tool_call == False:
                            # åªæŠŠ Base64 éŸ³é¢‘æ•°æ®ç•™åœ¨ delta é‡Œï¼Œåˆ«åŠ¨å®ƒ
                            yield f"data: {chunk.model_dump_json()}\n\n"
                            continue
                        elif hasattr(choice.delta, "audio") and choice.delta.audio and is_tool_call == True:
                            continue
                        # åˆ›å»ºåŸå§‹chunkçš„æ‹·è´
                        chunk_dict = chunk.model_dump()
                        delta = chunk_dict["choices"][0]["delta"]
                        
                        # åˆå§‹åŒ–å¿…è¦å­—æ®µ
                        delta.setdefault("content", "")
                        delta.setdefault("reasoning_content", "")
                        
                        # ä¼˜å…ˆå¤„ç† reasoning_content
                        if delta["reasoning_content"]:
                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                            continue
                        if delta.get("reasoning", ""):
                            delta["reasoning_content"] = delta["reasoning"]
                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                            continue

                        # å¤„ç†å†…å®¹
                        current_content = delta["content"]
                        buffer = current_content
                        
                        while buffer:
                            if not in_reasoning:
                                # å¯»æ‰¾å¼€å§‹æ ‡ç­¾
                                start_pos = buffer.find(open_tag)
                                if start_pos != -1:
                                    # å¤„ç†å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹
                                    content_buffer.append(buffer[:start_pos])
                                    buffer = buffer[start_pos+len(open_tag):]
                                    in_reasoning = True
                                else:
                                    content_buffer.append(buffer)
                                    buffer = ""
                            else:
                                # å¯»æ‰¾ç»“æŸæ ‡ç­¾
                                end_pos = buffer.find(close_tag)
                                if end_pos != -1:
                                    # å¤„ç†æ€è€ƒå†…å®¹
                                    reasoning_buffer.append(buffer[:end_pos])
                                    buffer = buffer[end_pos+len(close_tag):]
                                    in_reasoning = False
                                else:
                                    reasoning_buffer.append(buffer)
                                    buffer = ""
                        
                        # æ„é€ æ–°çš„deltaå†…å®¹
                        new_content = "".join(content_buffer)
                        new_reasoning = "".join(reasoning_buffer)
                        
                        # æ›´æ–°chunkå†…å®¹
                        delta["content"] = new_content.strip("\x00")  # ä¿ç•™æœªå®Œæˆå†…å®¹
                        delta["reasoning_content"] = new_reasoning.strip("\x00") or None
                        
                        # é‡ç½®ç¼“å†²åŒºä½†ä¿ç•™æœªå®Œæˆéƒ¨åˆ†
                        if in_reasoning:
                            content_buffer = [new_content.split(open_tag)[-1]] 
                        else:
                            content_buffer = []
                        reasoning_buffer = []
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                        full_content += delta.get("content") or "" 
                # æœ€ç»ˆflushæœªå®Œæˆå†…å®¹
                if content_buffer or reasoning_buffer:
                    final_chunk = {
                        "choices": [{
                            "delta": {
                                "content": "".join(content_buffer),
                                "reasoning_content": "".join(reasoning_buffer)
                            }
                        }]
                    }
                    yield f"data: {json.dumps(final_chunk)}\n\n"
                    full_content += final_chunk["choices"][0]["delta"].get("content", "")
                # å°†å“åº”æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
                content_append(request.messages, 'assistant', full_content)
                # å·¥å…·å’Œæ·±åº¦æœç´¢
                if tool_calls:
                    print("tool_calls",tool_calls)
                    pass
                elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                    search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,full_content)
                    response = await client.chat.completions.create(
                        model=model,
                        messages=[
                            {
                            "role": "system",
                            "content": source_prompt,
                            },
                            {
                            "role": "user",
                            "content": search_prompt,
                            }
                        ],
                        temperature=0.5,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                    response_content = response.choices[0].message.content
                    # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                    if "```json" in response_content:
                        try:
                            response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                        except:
                            # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                            response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                    try:
                        response_content = json.loads(response_content)
                    except json.JSONDecodeError:
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "tool_content": {"title": f"âŒ{await t('task_error')}", "content": ""}
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                    if response_content["status"] == "done":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                   "tool_content": {"title": f"âœ…{await t('task_done')}", "content": ""},
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = False
                    elif response_content["status"] == "not_done":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "tool_content": {"title": f"â{await t('task_not_done')}", "content": ""},
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        search_task = response_content["unfinished_task"]
                        task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": task_prompt,
                            }
                        )
                    elif response_content["status"] == "need_more_info":
                        DRS_STAGE = 2
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "tool_content": {"title": f"â“{await t('task_need_more_info')}", "content": ""}
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = False
                    elif response_content["status"] == "need_work":
                        DRS_STAGE = 2
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "tool_content": {"title": f"ğŸ”{await t('enter_search_stage')}", "content": ""}
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        drs_msg = get_drs_stage(DRS_STAGE)
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": drs_msg,
                            }
                        )
                    elif response_content["status"] == "need_more_work":
                        DRS_STAGE = 2
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "tool_content": {"title": f"ğŸ”{await t('need_more_work')}", "content": ""}
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        search_task = response_content["unfinished_task"]
                        task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": task_prompt,
                            }
                        )
                    elif response_content["status"] == "answer":
                        DRS_STAGE = 3
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "tool_content": {"title": f"â­{await t('enter_answer_stage')}", "content": ""}
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        drs_msg = get_drs_stage(DRS_STAGE)
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": drs_msg,
                            }
                        )
                reasoner_messages = copy.deepcopy(request.messages)
                while tool_calls or search_not_done:
                    full_content = ""
                    if tool_calls:
                        response_content = tool_calls[0].function
                        print(response_content)
                        modified_data = '[' + response_content.arguments.replace('}{', '},{') + ']'
                        # ä½¿ç”¨json.loadsæ¥è§£æä¿®æ”¹åçš„å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                        data_list = json.loads(modified_data)
                        modified_tool = f"{await t("sendArg")}{data_list[0]}"
                        if settings['tools']['asyncTools']['enabled']:
                            tool_id = uuid.uuid4()
                            async_tool_id = f"{response_content.name}_{tool_id}"
                            chunk_dict = {
                                "id": "agentParty",
                                "choices": [
                                    {
                                        "finish_reason": None,
                                        "index": 0,
                                        "delta": {
                                            "role":"assistant",
                                            "content": "",
                                            "async_tool_id": async_tool_id
                                        }
                                    }
                                ]
                            }
                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                            # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡å¹¶è®°å½•çŠ¶æ€
                            asyncio.create_task(
                                execute_async_tool(
                                    async_tool_id,
                                    response_content.name,
                                    data_list[0],
                                    settings,
                                    user_prompt
                                )
                            )
                            
                            async with async_tools_lock:
                                async_tools[async_tool_id] = {
                                    "status": "pending",
                                    "result": None,
                                    "name":response_content.name,
                                    "parameters":data_list[0]
                                }
                            results = f"{response_content.name}å·¥å…·å·²æˆåŠŸå¯åŠ¨ï¼Œè·å–ç»“æœéœ€è¦èŠ±è´¹å¾ˆä¹…çš„æ—¶é—´ã€‚è¯·ä¸è¦å†æ¬¡è°ƒç”¨è¯¥å·¥å…·ï¼Œå› ä¸ºå·¥å…·ç»“æœå°†ç”Ÿæˆåè‡ªåŠ¨å‘é€ï¼Œå†æ¬¡è°ƒç”¨ä¹Ÿä¸èƒ½æ›´å¿«çš„è·å–åˆ°ç»“æœã€‚è¯·ç›´æ¥å‘Šè¯‰ç”¨æˆ·ï¼Œä½ ä¼šåœ¨è·å¾—ç»“æœåå›ç­”ä»–çš„é—®é¢˜ã€‚"
                        else:
                            results = await dispatch_tool(response_content.name, data_list[0],settings)
                        
                        if isinstance(results, str) and '"type": "approval_required"' in results:
                            # 1. æ„é€  SSE æ¶ˆæ¯å‘é€ç»™å‰ç«¯
                            yield make_sse({
                                "title": response_content.name, 
                                "content": results, # è¿™æ˜¯ dispatch_tool è¿”å›çš„å®¡æ‰¹ JSON
                                "type": "tool_approval", # æ–°ç±»å‹ï¼šå®¡æ‰¹
                                "tool_call_id": tool_calls[0].id
                            })
                            # 2. ç»ˆæ­¢ç”Ÿæˆå™¨ï¼Œé‡Šæ”¾è¿æ¥
                            # æ­¤æ—¶ AI è¿˜æ²¡æœ‰æ”¶åˆ°ç»“æœï¼Œå®ƒå¤„äºâ€œç­‰å¾…å·¥å…·è¿”å›â€çš„çŠ¶æ€
                            return 
                        if results is None:
                            chunk = {
                                "id": "extra_tools",
                                "choices": [
                                    {
                                        "index": 0,
                                        "delta": {
                                            "role":"assistant",
                                            "content": "",
                                            "tool_calls":modified_data,
                                        }
                                    }
                                ]
                            }
                            yield f"data: {json.dumps(chunk)}\n\n"
                            break
                        if response_content.name in ["query_knowledge_base"] and type(results) == list:
                            if settings["KBSettings"]["is_rerank"]:
                                results = await rerank_knowledge_base(user_prompt,results)
                            results = json.dumps(results, ensure_ascii=False, indent=4)
                        request.messages.append(
                            {
                                "tool_calls": [
                                    {
                                        "id": tool_calls[0].id,
                                        "function": {
                                            "arguments": json.dumps(data_list[0]),
                                            "name": response_content.name,
                                        },
                                        "type": tool_calls[0].type,
                                    }
                                ],
                                "role": "assistant",
                                "content": "",
                            }
                        )
                        if (settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both') and settings['tools']['asyncTools']['enabled'] is False:
                            content_append(request.messages, 'user',  f"\nå¯¹äºè”ç½‘æœç´¢çš„ç»“æœï¼Œå¦‚æœè”ç½‘æœç´¢çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨è”ç½‘æœç´¢æŸ¥è¯¢è¿˜æœªç»™å‡ºçš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœå·²ç»è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚")
                        if settings['tools']['asyncTools']['enabled']:
                            pass
                        else:

                            # å·¥å…·åå›½é™…åŒ–
                            tool_name_text = f"{response_content.name}{await t('tool_result')}"
                            stream_tool_name_text = f"{response_content.name}{await t('stream_tool_result')}"


                            # ---------- åˆ†æƒ…å†µå¤„ç† ----------
                            if not isinstance(results, AsyncIterator):
                                yield make_sse({"title": response_content.name, "content": str(results), "type": "tool_result"})
                            else:  # AsyncIterator[str]
                                buffer = []
                                first = True
                                async for chunk in results:
                                    buffer.append(chunk)
                                    if first:                       # ç¬¬ä¸€æ¬¡ï¼šå¸¦å¤´éƒ¨
                                        yield make_sse({"title": response_content.name, "content": chunk, "type": "tool_result_stream"})
                                        first = False
                                    else:                           # åç»­ï¼šä¸å¸¦å¤´éƒ¨
                                        yield make_sse({"title": "tool_result_stream", "content": chunk, "type": "tool_result_stream"})

                                results = "".join(buffer)
                        request.messages.append(
                            {
                                "role": "tool",
                                "tool_call_id": tool_calls[0].id,
                                "name": response_content.name,
                                "content": str("".join(results)),
                            }
                        )
                        reasoner_messages.append(
                            {
                                "role": "assistant",
                                "content": str(response_content),
                            }
                        )
                        reasoner_messages.append(
                            {
                                "role": "user",
                                "content": f"{response_content.name}å·¥å…·ç»“æœï¼š"+str(results),
                            }
                        )
                    # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å‹
                    if settings['reasoner']['enabled'] or enable_thinking:
                        if tools:
                            content_append(reasoner_messages, 'system',  f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}")
                        for modelProvider in settings['modelProviders']: 
                            if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                                vendor = modelProvider['vendor']
                                break
                        msg = await images_add_in_messages(reasoner_messages, images,settings)
                        if vendor == 'Ollama':
                            # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                            reasoner_stream = await reasoner_client.chat.completions.create(
                                model=settings['reasoner']['model'],
                                messages=msg,
                                stream=True,
                                temperature=settings['reasoner']['temperature']
                            )
                            full_reasoning = ""
                            buffer = ""  # è·¨chunkçš„å†…å®¹ç¼“å†²åŒº
                            in_reasoning = False  # æ˜¯å¦åœ¨æ ‡ç­¾å†…
                            
                            async for chunk in reasoner_stream:
                                if not chunk.choices:
                                    continue
                                chunk_dict = chunk.model_dump()
                                delta = chunk_dict["choices"][0].get("delta", {})
                                if delta:
                                    current_content = delta.get("content", "")
                                    buffer += current_content  # ç´¯ç§¯åˆ°ç¼“å†²åŒº
                                    
                                    # å®æ—¶å¤„ç†ç¼“å†²åŒºå†…å®¹
                                    while True:
                                        reasoning_content = delta.get("reasoning_content", "")
                                        if reasoning_content:
                                            full_reasoning += reasoning_content
                                        else:
                                            reasoning_content = delta.get("reasoning", "")
                                            if reasoning_content:
                                                delta['reasoning_content'] = reasoning_content
                                                full_reasoning += reasoning_content
                                        if reasoning_content:
                                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                                            break
                                        if not in_reasoning:
                                            # å¯»æ‰¾å¼€æ”¾æ ‡ç­¾
                                            start_pos = buffer.find(open_tag)
                                            if start_pos != -1:
                                                # å¼€æ”¾æ ‡ç­¾å‰çš„å†…å®¹ï¼ˆéæ€è€ƒå†…å®¹ï¼‰
                                                non_reasoning = buffer[:start_pos]
                                                buffer = buffer[start_pos+len(open_tag):]
                                                in_reasoning = True
                                            else:
                                                break  # æ— å¼€æ”¾æ ‡ç­¾ï¼Œä¿ç•™åç»­å¤„ç†
                                        else:
                                            # å¯»æ‰¾é—­åˆæ ‡ç­¾
                                            end_pos = buffer.find(close_tag)
                                            if end_pos != -1:
                                                # æå–æ€è€ƒå†…å®¹å¹¶æ„é€ å“åº”
                                                reasoning_part = buffer[:end_pos]
                                                chunk_dict["choices"][0]["delta"] = {
                                                    "reasoning_content": reasoning_part,
                                                    "content": ""  # æ¸…é™¤éæ€è€ƒå†…å®¹
                                                }
                                                yield f"data: {json.dumps(chunk_dict)}\n\n"
                                                full_reasoning += reasoning_part
                                                buffer = buffer[end_pos+len(close_tag):]
                                                in_reasoning = False
                                            else:
                                                # å‘é€æœªé—­åˆçš„ä¸­é—´å†…å®¹
                                                if buffer:
                                                    chunk_dict["choices"][0]["delta"] = {
                                                        "reasoning_content": buffer,
                                                        "content": ""
                                                    }
                                                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                                                    full_reasoning += buffer
                                                    buffer = ""
                                                break  # ç­‰å¾…æ›´å¤šå†…å®¹
                        else:
                            # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                            reasoner_stream = await reasoner_client.chat.completions.create(
                                model=settings['reasoner']['model'],
                                messages=msg,
                                stream=True,
                                stop=settings['reasoner']['stop_words'],
                                temperature=settings['reasoner']['temperature']
                            )
                            full_reasoning = ""
                            # å¤„ç†æ¨ç†æ¨¡å‹çš„æµå¼å“åº”
                            async for chunk in reasoner_stream:
                                if not chunk.choices:
                                    continue

                                chunk_dict = chunk.model_dump()
                                delta = chunk_dict["choices"][0].get("delta", {})
                                if delta:
                                    reasoning_content = delta.get("reasoning_content", "")
                                    if reasoning_content:
                                        full_reasoning += reasoning_content
                                    else:
                                        reasoning_content = delta.get("reasoning", "")
                                        if reasoning_content:
                                            delta['reasoning_content'] = reasoning_content
                                            full_reasoning += reasoning_content
                                    # ç§»é™¤contentå­—æ®µï¼Œç¡®ä¿yieldçš„å†…å®¹ä¸­ä¸åŒ…å«content
                                    if 'content' in delta:
                                        del delta['content']
                                yield f"data: {json.dumps(chunk_dict)}\n\n"

                        # åœ¨æ¨ç†ç»“æŸåæ·»åŠ å®Œæ•´æ¨ç†å†…å®¹åˆ°æ¶ˆæ¯
                        content_append(request.messages, 'assistant', f"<think>\n{full_reasoning}\n</think>") # å¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹
                    msg = await images_add_in_messages(request.messages, images,settings)
                    if tools:
                        response = await client.chat.completions.create(
                            model=model,
                            messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                            temperature=request.temperature,
                            tools=tools,
                            stream=True,
                            top_p=request.top_p or settings['top_p'],
                            extra_body = extra_params, # å…¶ä»–å‚æ•°
                            **extra
                        )
                    else:
                        response = await client.chat.completions.create(
                            model=model,
                            messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                            temperature=request.temperature,
                            stream=True,
                            top_p=request.top_p or settings['top_p'],
                            extra_body = extra_params, # å…¶ä»–å‚æ•°
                            **extra
                        )
                    tool_calls = []
                    async for chunk in response:
                        if not chunk.choices:
                            continue
                        if chunk.choices:
                            choice = chunk.choices[0]
                            if hasattr(choice.delta, "audio") and choice.delta.audio:
                                # åªæŠŠ Base64 éŸ³é¢‘æ•°æ®ç•™åœ¨ delta é‡Œï¼Œåˆ«åŠ¨å®ƒ
                                yield f"data: {chunk.model_dump_json()}\n\n"
                                continue
                            if choice.delta.tool_calls:  # function_calling
                                for idx, tool_call in enumerate(choice.delta.tool_calls):
                                    tool = choice.delta.tool_calls[idx]
                                    if len(tool_calls) <= idx:
                                        tool_calls.append(tool)
                                        continue
                                    if tool.function.arguments:
                                        # functionå‚æ•°ä¸ºæµå¼å“åº”ï¼Œéœ€è¦æ‹¼æ¥
                                        if tool_calls[idx].function.arguments:
                                            tool_calls[idx].function.arguments += tool.function.arguments
                                        else:
                                            tool_calls[idx].function.arguments = tool.function.arguments
                                current_tool = tool_calls[idx]
                                if current_tool.function and current_tool.function.name:
                                    progress_chunk = {
                                        "choices": [{
                                            "delta": {
                                                "tool_progress": {  # æ–°å¢å­—æ®µï¼ŒåŒºåˆ«äºæœ€ç»ˆçš„ tool_content
                                                    "name": current_tool.function.name,
                                                    "arguments": current_tool.function.arguments or "",
                                                    "index": idx,
                                                    "id": current_tool.id or f"call_{idx}"
                                                }
                                            }
                                        }]
                                    }
                                    yield f"data: {json.dumps(progress_chunk)}\n\n"
                            else:
                                # åˆ›å»ºåŸå§‹chunkçš„æ‹·è´
                                chunk_dict = chunk.model_dump()
                                delta = chunk_dict["choices"][0]["delta"]
                                
                                # åˆå§‹åŒ–å¿…è¦å­—æ®µ
                                delta.setdefault("content", "")
                                delta.setdefault("reasoning_content", "")

                                # ä¼˜å…ˆå¤„ç† reasoning_content
                                if delta["reasoning_content"]:
                                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                                    continue
                                if delta.get("reasoning", ""):
                                    delta["reasoning_content"] = delta["reasoning"]
                                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                                    continue
                                # å¤„ç†å†…å®¹
                                current_content = delta["content"]
                                buffer = current_content
                                
                                while buffer:
                                    if not in_reasoning:
                                        # å¯»æ‰¾å¼€å§‹æ ‡ç­¾
                                        start_pos = buffer.find(open_tag)
                                        if start_pos != -1:
                                            # å¤„ç†å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹
                                            content_buffer.append(buffer[:start_pos])
                                            buffer = buffer[start_pos+len(open_tag):]
                                            in_reasoning = True
                                        else:
                                            content_buffer.append(buffer)
                                            buffer = ""
                                    else:
                                        # å¯»æ‰¾ç»“æŸæ ‡ç­¾
                                        end_pos = buffer.find(close_tag)
                                        if end_pos != -1:
                                            # å¤„ç†æ€è€ƒå†…å®¹
                                            reasoning_buffer.append(buffer[:end_pos])
                                            buffer = buffer[end_pos+len(close_tag):]
                                            in_reasoning = False
                                        else:
                                            reasoning_buffer.append(buffer)
                                            buffer = ""
                                
                                # æ„é€ æ–°çš„deltaå†…å®¹
                                new_content = "".join(content_buffer)
                                new_reasoning = "".join(reasoning_buffer)
                                
                                # æ›´æ–°chunkå†…å®¹
                                delta["content"] = new_content.strip("\x00")  # ä¿ç•™æœªå®Œæˆå†…å®¹
                                delta["reasoning_content"] = new_reasoning.strip("\x00") or None
                                
                                # é‡ç½®ç¼“å†²åŒºä½†ä¿ç•™æœªå®Œæˆéƒ¨åˆ†
                                if in_reasoning:
                                    content_buffer = [new_content.split(open_tag)[-1]] 
                                else:
                                    content_buffer = []
                                reasoning_buffer = []
                                
                                yield f"data: {json.dumps(chunk_dict)}\n\n"
                                full_content += delta.get("content") or "" 
                    # æœ€ç»ˆflushæœªå®Œæˆå†…å®¹
                    if content_buffer or reasoning_buffer:
                        final_chunk = {
                            "choices": [{
                                "delta": {
                                    "content": "".join(content_buffer),
                                    "reasoning_content": "".join(reasoning_buffer)
                                }
                            }]
                        }
                        yield f"data: {json.dumps(final_chunk)}\n\n"
                        full_content += final_chunk["choices"][0]["delta"].get("content", "")
                    # å°†å“åº”æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
                    content_append(request.messages, 'assistant', full_content)
                    # å·¥å…·å’Œæ·±åº¦æœç´¢
                    if tool_calls:
                        pass
                    elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                        search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,full_content)
                        response = await client.chat.completions.create(
                            model=model,
                            messages=[                        
                                {
                                "role": "system",
                                "content": source_prompt,
                                },
                                {
                                "role": "user",
                                "content": search_prompt,
                                }
                            ],
                            temperature=0.5,
                            extra_body = extra_params, # å…¶ä»–å‚æ•°
                        )
                        response_content = response.choices[0].message.content
                        # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                        if "```json" in response_content:
                            try:
                                response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                            except:
                                # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                                response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                        try:
                            response_content = json.loads(response_content)
                        except json.JSONDecodeError:
                            search_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": f"âŒ{await t('task_error')}", "content": ""}
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(search_chunk)}\n\n"
                        if response_content["status"] == "done":
                            search_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": f"âœ…{await t('task_done')}", "content": ""}
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(search_chunk)}\n\n"
                            search_not_done = False
                        elif response_content["status"] == "not_done":
                            search_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": f"â{await t('task_not_done')}", "content": ""}
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(search_chunk)}\n\n"
                            search_not_done = True
                            search_task = response_content["unfinished_task"]
                            task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                            request.messages.append(
                                {
                                    "role": "assistant",
                                    "content": full_content,
                                }
                            )
                            request.messages.append(
                                {
                                    "role": "user",
                                    "content": task_prompt,
                                }
                            )
                        elif response_content["status"] == "need_more_info":
                            DRS_STAGE = 2
                            search_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": f"â“{await t('task_need_more_info')}", "content": ""}
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(search_chunk)}\n\n"
                            search_not_done = False
                        elif response_content["status"] == "need_work":
                            DRS_STAGE = 2
                            search_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": f"ğŸ”{await t('enter_search_stage')}", "content": ""}
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(search_chunk)}\n\n"
                            search_not_done = True
                            drs_msg = get_drs_stage(DRS_STAGE)
                            request.messages.append(
                                {
                                    "role": "assistant",
                                    "content": full_content,
                                }
                            )
                            request.messages.append(
                                {
                                    "role": "user",
                                    "content": drs_msg,
                                }
                            )
                        elif response_content["status"] == "need_more_work":
                            DRS_STAGE = 2
                            search_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": f"ğŸ”{await t('need_more_work')}", "content": ""}
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(search_chunk)}\n\n"
                            search_not_done = True
                            search_task = response_content["unfinished_task"]
                            task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                            request.messages.append(
                                {
                                    "role": "assistant",
                                    "content": full_content,
                                }
                            )
                            request.messages.append(
                                {
                                    "role": "user",
                                    "content": task_prompt,
                                }
                            )
                        elif response_content["status"] == "answer":
                            DRS_STAGE = 3
                            search_chunk = {
                                "choices": [{
                                    "delta": {
                                        "tool_content": {"title": f"â­{await t('enter_answer_stage')}", "content": ""}
                                    }
                                }]
                            }
                            yield f"data: {json.dumps(search_chunk)}\n\n"
                            search_not_done = True
                            drs_msg = get_drs_stage(DRS_STAGE)
                            request.messages.append(
                                {
                                    "role": "assistant",
                                    "content": full_content,
                                }
                            )
                            request.messages.append(
                                {
                                    "role": "user",
                                    "content": drs_msg,
                                }
                            )
                yield "data: [DONE]\n\n"
                if m0:
                    messages=f"ç”¨æˆ·è¯´ï¼š{user_prompt}\n\n---\n\nä½ è¯´ï¼š{full_content}"
                    executor = ThreadPoolExecutor()
                    async def add_async():
                        loop = asyncio.get_event_loop()
                        # ç»‘å®š user_id å…³é”®å­—å‚æ•°
                        metadata = {
                            "timetamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        }
                        func = partial(m0.add, user_id=memoryId,metadata=metadata,infer=False)
                        # ä¼ é€’ messages ä½œä¸ºä½ç½®å‚æ•°
                        await loop.run_in_executor(executor, func, messages)
                        print("çŸ¥è¯†åº“æ›´æ–°å®Œæˆ")

                    asyncio.create_task(add_async())
                    print("çŸ¥è¯†åº“æ›´æ–°ä»»åŠ¡å·²æäº¤")
                return
            except Exception as e:
                        logger.error(f"Error occurred: {e}")
                        # æ•è·å¼‚å¸¸å¹¶è¿”å›ç»“æ„åŒ–é”™è¯¯ä¿¡æ¯
                        error_chunk = {
                            "choices": [{
                                "delta": {
                                    "tool_content": {
                                        "title": "â Error", # ç»Ÿä¸€æ ‡é¢˜
                                        "content": str(e),   # é”™è¯¯è¯¦æƒ…
                                        "type": "error"      # æ ‡è®°ç±»å‹ï¼Œæ–¹ä¾¿å‰ç«¯åˆ‡æ¢æ ·å¼
                                    }
                                }
                            }]
                        }
                        yield f"data: {json.dumps(error_chunk)}\n\n"
                        yield "data: [DONE]\n\n"  # ç¡®ä¿æœ€ç»ˆç»“æŸ
                        return
        
        return StreamingResponse(
            stream_generator(user_prompt, DRS_STAGE),
            media_type="text/event-stream",
            headers={
                "Content-Type": "text/event-stream",
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
    except Exception as e:
        logger.error(f"Error occurred: {e}")
        # å¦‚æœe.status_codeå­˜åœ¨ï¼Œåˆ™ä½¿ç”¨å®ƒä½œä¸ºHTTPçŠ¶æ€ç ï¼Œå¦åˆ™ä½¿ç”¨500
        return JSONResponse(
            status_code=getattr(e, "status_code", 500),
            content={"error": str(e)},
        )

async def generate_complete_response(client,reasoner_client, request: ChatRequest, settings: dict,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search):
    from mem0 import Memory
    global mcp_client_list,HA_client,ChromeMCP_client,sql_client
    DRS_STAGE = 1 # 1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: å·¥å…·è°ƒç”¨é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ
    if len(request.messages) > 2:
        DRS_STAGE = 2
    from py.load_files import get_files_content,file_tool,image_tool
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        Bing_search_async,
        Google_search_async,
        Brave_search_async,
        Exa_search_async,
        Serper_search_async,
        bochaai_search_async,
        duckduckgo_tool, 
        searxng_tool, 
        tavily_tool, 
        bing_tool,
        google_tool,
        brave_tool,
        exa_tool,
        serper_tool,
        bochaai_tool,
        jina_crawler_tool, 
        Crawl4Ai_tool
    )
    from py.know_base import kb_tool,query_knowledge_base,rerank_knowledge_base
    from py.agent_tool import get_agent_tool
    from py.a2a_tool import get_a2a_tool
    from py.llm_tool import get_llm_tool
    from py.pollinations import pollinations_image_tool,openai_image_tool,openai_chat_image_tool
    from py.code_interpreter import e2b_code_tool,local_run_code_tool
    from py.utility_tools import time_tool
    from py.utility_tools import (
        time_tool, 
        weather_tool,
        location_tool,
        timer_weather_tool,
        wikipedia_summary_tool,
        wikipedia_section_tool,
        arxiv_tool
    ) 
    from py.autoBehavior import auto_behavior_tool
    from py.cli_tool import claude_code_tool,qwen_code_tool,get_tools_for_mode,get_local_tools_for_mode
    from py.cdp_tool import all_cdp_tools
    m0 = None
    if settings["memorySettings"]["is_memory"] and settings["memorySettings"]["selectedMemory"] and settings["memorySettings"]["selectedMemory"] != "":
        memoryId = settings["memorySettings"]["selectedMemory"]
        cur_memory = None
        for memory in settings["memories"]:
            if memory["id"] == memoryId:
                cur_memory = memory
                break
        if cur_memory and cur_memory["providerId"]:
            print("é•¿æœŸè®°å¿†å¯ç”¨")
            config={
                "embedder": {
                    "provider": 'openai',
                    "config": {
                        "model": cur_memory['model'],
                        "api_key": cur_memory['api_key'],
                        "openai_base_url":cur_memory["base_url"],
                        "embedding_dims":cur_memory.get("embedding_dims", 1024)
                    },
                },
                "llm": {
                    "provider": 'openai',
                    "config": {
                        "model": settings['model'],
                        "api_key": settings['api_key'],
                        "openai_base_url":settings["base_url"]
                    }
                },
                "vector_store": {
                    "provider": "faiss",
                    "config": {
                        "collection_name": "agent-party",
                        "path": os.path.join(MEMORY_CACHE_DIR,memoryId),
                        "distance_strategy": "euclidean",
                        "embedding_model_dims": cur_memory.get("embedding_dims", 1024)
                    }
                }
            }
            m0 = Memory.from_config(config)
    images = await images_in_messages(request.messages,fastapi_base_url)
    request.messages = await message_without_images(request.messages)
    open_tag = "<think>"
    close_tag = "</think>"
    tools = request.tools or []
    tools = request.tools or []
    extra = {}
    reasoner_extra = {}
    if mcp_client_list:
        for server_name, mcp_client in mcp_client_list.items():
            if server_name in settings['mcpServers']:
                if 'disabled' not in settings['mcpServers'][server_name]:
                    settings['mcpServers'][server_name]['disabled'] = False
                if settings['mcpServers'][server_name]['disabled'] == False and settings['mcpServers'][server_name]['processingStatus'] == 'ready':
                    disable_tools = []
                    for tool in settings['mcpServers'][server_name]["tools"]: 
                        if tool.get("enabled", True) == False:
                            disable_tools.append(tool["name"])
                    function = await mcp_client.get_openai_functions(disable_tools=disable_tools)
                    if function:
                        tools.extend(function)
    get_llm_tool_fuction = await get_llm_tool(settings)
    if get_llm_tool_fuction:
        tools.append(get_llm_tool_fuction)
    get_agent_tool_fuction = await get_agent_tool(settings)
    if get_agent_tool_fuction:
        tools.append(get_agent_tool_fuction)
    get_a2a_tool_fuction = await get_a2a_tool(settings)
    if get_a2a_tool_fuction:
        tools.append(get_a2a_tool_fuction)
    if settings["HASettings"]["enabled"]:
        ha_tool = await HA_client.get_openai_functions(disable_tools=[])
        if ha_tool:
            tools.extend(ha_tool)
    if settings['chromeMCPSettings']['enabled'] and settings['chromeMCPSettings']['type']=='external':
        chromeMCP_tool = await ChromeMCP_client.get_openai_functions(disable_tools=[])
        if chromeMCP_tool:
            tools.extend(chromeMCP_tool)
    if settings['chromeMCPSettings']['enabled'] and settings['chromeMCPSettings']['type']=='internal':
        tools.extend(all_cdp_tools)
    if settings['sqlSettings']['enabled']:
        sql_tool = await sql_client.get_openai_functions(disable_tools=[])
        if sql_tool:
            tools.extend(sql_tool)
    if settings['CLISettings']['enabled']:
        if settings['CLISettings']['engine'] == 'cc':
            tools.append(claude_code_tool)
        elif settings['CLISettings']['engine'] == 'qc':
            tools.append(qwen_code_tool)
        elif settings['CLISettings']['engine'] == 'ds':
            tools.extend(get_tools_for_mode('yolo'))
        elif settings['CLISettings']['engine'] == 'local':
            tools.extend(get_local_tools_for_mode('yolo'))
    if settings['tools']['time']['enabled'] and settings['tools']['time']['triggerMode'] == 'afterThinking':
        tools.append(time_tool)
    if settings["tools"]["weather"]['enabled']:
        tools.append(weather_tool)
        tools.append(location_tool)
        tools.append(timer_weather_tool)
    if settings["tools"]["wikipedia"]['enabled']:
        tools.append(wikipedia_summary_tool)
        tools.append(wikipedia_section_tool)
    if settings["tools"]["arxiv"]['enabled']:
        tools.append(arxiv_tool)
    if settings['text2imgSettings']['enabled']:
        if settings['text2imgSettings']['engine'] == 'pollinations':
            tools.append(pollinations_image_tool)
        elif settings['text2imgSettings']['engine'] == 'openai':
            tools.append(openai_image_tool)
        elif settings['text2imgSettings']['engine'] == 'openaiChat':
            tools.append(openai_chat_image_tool)
    if settings['tools']['getFile']['enabled']:
        tools.append(file_tool)
        tools.append(image_tool)
    if settings['tools']['autoBehavior']['enabled'] and request.messages[-1]['role'] == 'user':
        tools.append(auto_behavior_tool)
    if settings["codeSettings"]['enabled']:
        if settings["codeSettings"]["engine"] == "e2b":
            tools.append(e2b_code_tool)
        elif settings["codeSettings"]["engine"] == "sandbox":
            tools.append(local_run_code_tool)
    if settings["custom_http"]:
        for custom_http in settings["custom_http"]:
            if custom_http["enabled"]:
                if custom_http['body'] == "":
                    custom_http['body'] = "{}"
                custom_http_tool = {
                    "type": "function",
                    "function": {
                        "name": f"custom_http_{custom_http['name']}",
                        "description": f"{custom_http['description']}",
                        "parameters": json.loads(custom_http['body']),
                    },
                }
                tools.append(custom_http_tool)
    if settings["workflows"]:
        for workflow in settings["workflows"]:
            if workflow["enabled"]:
                comfyui_properties = {}
                comfyui_required = []
                if workflow["text_input"] is not None:
                    comfyui_properties["text_input"] = {
                        "description": "ç¬¬ä¸€ä¸ªæ–‡å­—è¾“å…¥ï¼Œéœ€è¦è¾“å…¥çš„æç¤ºè¯ï¼Œç”¨äºç”Ÿæˆå›¾ç‰‡æˆ–è€…è§†é¢‘ï¼Œå¦‚æœæ— ç‰¹åˆ«æç¤ºï¼Œé»˜è®¤ä¸ºè‹±æ–‡",
                        "type": "string"
                    }
                    comfyui_required.append("text_input")
                if workflow["text_input_2"] is not None:
                    comfyui_properties["text_input_2"] = {
                        "description": "ç¬¬äºŒä¸ªæ–‡å­—è¾“å…¥ï¼Œéœ€è¦è¾“å…¥çš„æç¤ºè¯ï¼Œç”¨äºç”Ÿæˆå›¾ç‰‡æˆ–è€…è§†é¢‘ï¼Œå¦‚æœæ— ç‰¹åˆ«æç¤ºï¼Œé»˜è®¤ä¸ºè‹±æ–‡",
                        "type": "string"
                    }
                    comfyui_required.append("text_input_2")
                if workflow["image_input"] is not None:
                    comfyui_properties["image_input"] = {
                        "description": "ç¬¬ä¸€ä¸ªå›¾ç‰‡è¾“å…¥ï¼Œéœ€è¦è¾“å…¥çš„å›¾ç‰‡ï¼Œå¿…é¡»æ˜¯å›¾ç‰‡URLï¼Œå¯ä»¥æ˜¯å¤–éƒ¨é“¾æ¥ï¼Œä¹Ÿå¯ä»¥æ˜¯æœåŠ¡å™¨å†…éƒ¨çš„URLï¼Œä¾‹å¦‚ï¼šhttps://www.example.com/xxx.png  æˆ–è€…  http://127.0.0.1:3456/xxx.jpg",
                        "type": "string"
                    }
                    comfyui_required.append("image_input")
                if workflow["image_input_2"] is not None:
                    comfyui_properties["image_input_2"] = {
                        "description": "ç¬¬äºŒä¸ªå›¾ç‰‡è¾“å…¥ï¼Œéœ€è¦è¾“å…¥çš„å›¾ç‰‡ï¼Œå¿…é¡»æ˜¯å›¾ç‰‡URLï¼Œå¯ä»¥æ˜¯å¤–éƒ¨é“¾æ¥ï¼Œä¹Ÿå¯ä»¥æ˜¯æœåŠ¡å™¨å†…éƒ¨çš„URLï¼Œä¾‹å¦‚ï¼šhttps://www.example.com/xxx.png  æˆ–è€…  http://127.0.0.1:3456/xxx.jpg",
                        "type": "string"
                    }
                    comfyui_required.append("image_input_2")
                comfyui_parameters = {
                    "type": "object",
                    "properties": comfyui_properties,
                    "required": comfyui_required
                }
                comfyui_tool = {
                    "type": "function",
                    "function": {
                        "name": f"comfyui_{workflow['unique_filename']}",
                        "description": f"{workflow['description']}+\nå¦‚æœè¦è¾“å…¥å›¾ç‰‡æç¤ºè¯æˆ–è€…ä¿®æ”¹æç¤ºè¯ï¼Œå°½å¯èƒ½ä½¿ç”¨è‹±è¯­ã€‚\nè¿”å›çš„å›¾ç‰‡ç»“æœï¼Œè¯·å°†å›¾ç‰‡çš„URLæ”¾å…¥![image]()è¿™æ ·çš„markdownè¯­æ³•ä¸­ï¼Œç”¨æˆ·æ‰èƒ½çœ‹åˆ°å›¾ç‰‡ã€‚å¦‚æœæ˜¯è§†é¢‘ï¼Œè¯·å°†è§†é¢‘çš„URLæ”¾å…¥<video controls> <source src=''></video>çš„ä¸­srcä¸­ï¼Œç”¨æˆ·æ‰èƒ½çœ‹åˆ°è§†é¢‘ã€‚å¦‚æœæœ‰å¤šä¸ªç»“æœï¼Œåˆ™è¯·ç”¨æ¢è¡Œç¬¦åˆ†éš”å¼€è¿™å‡ ä¸ªå›¾ç‰‡æˆ–è€…è§†é¢‘ï¼Œç”¨æˆ·æ‰èƒ½çœ‹åˆ°å¤šä¸ªç»“æœã€‚",
                        "parameters": comfyui_parameters,
                    },
                }
                tools.append(comfyui_tool)
    search_not_done = False
    search_task = ""
    try:
        model = settings['model']
        extra_params = settings['extra_params']
        # ç§»é™¤extra_paramsè¿™ä¸ªlistä¸­"name"ä¸åŒ…å«éç©ºç™½ç¬¦çš„é”®å€¼å¯¹
        if extra_params:
            for extra_param in extra_params:
                if not extra_param['name'].strip():
                    extra_params.remove(extra_param)
            # åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸
            extra_params = {item['name']: item['value'] for item in extra_params}
        else:
            extra_params = {}
        if request.fileLinks:
            # å¼‚æ­¥è·å–æ–‡ä»¶å†…å®¹
            files_content = await get_files_content(request.fileLinks)
            system_message = f"\n\nç›¸å…³æ–‡ä»¶å†…å®¹ï¼š{files_content}"
            
            # ä¿®å¤å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
            content_append(request.messages, 'system', system_message)
        kb_list = []
        user_prompt = request.messages[-1]['content']
        if settings["memorySettings"]["is_memory"] and settings["memorySettings"]["selectedMemory"] and settings["memorySettings"]["selectedMemory"] != "":
            if settings["memorySettings"]["userName"] and settings["memorySettings"]["userName"] != "user":
                print("æ·»åŠ ç”¨æˆ·åï¼š\n\n" + settings["memorySettings"]["userName"] + "\n\nç”¨æˆ·åç»“æŸ\n\n")
                content_append(request.messages, 'system', "å½“å‰ä¸ä½ äº¤æµçš„äººçš„åå­—ä¸ºï¼š\n\n" + settings["memorySettings"]["userName"] + "\n\n")
            lore_content = ""
            assistant_reply = ""
            # æ‰¾å‡ºrequest.messagesä¸­ä¸Šæ¬¡çš„assistantå›å¤
            for i in range(len(request.messages)-1, -1, -1):
                if request.messages[i]['role'] == 'assistant':
                    assistant_reply = request.messages[i]['content']
                    break
            if cur_memory["characterBook"]:
                for lore in cur_memory["characterBook"]:
                    # lore['keysRaw'] æŒ‰ç…§æ¢è¡Œç¬¦åˆ†å‰²ï¼Œå¹¶å»é™¤ç©ºå­—ç¬¦ä¸²
                    lore_keys = lore["keysRaw"].split("\n")
                    lore_keys = [key for key in lore_keys if key != ""]
                    print(lore_keys)
                    # å¦‚æœlore_keysä¸ä¸ºç©ºï¼Œå¹¶ä¸”lore_keysçš„ä»»æ„ä¸€ä¸ªå…ƒç´ åœ¨user_promptæˆ–è€…assistant_replyä¸­ï¼Œåˆ™æ·»åŠ lore['content']åˆ°lore_contentä¸­
                    if lore_keys != [] and any(key in user_prompt or key in assistant_reply for key in lore_keys):
                        lore_content += lore['content'] + "\n\n"
            if lore_content:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢lore_contentä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    lore_content = lore_content.replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢lore_contentä¸­çš„{{char}}ä¸ºcur_memory["name"]
                lore_content = lore_content.replace("{{char}}", cur_memory["name"])
                print("æ·»åŠ ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n")
                content_append(request.messages, 'system', "ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n")
            if cur_memory["description"]:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢cur_memory["description"]ä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    cur_memory["description"] = cur_memory["description"].replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢cur_memory["description"]ä¸­çš„{{char}}ä¸ºcur_memory["name"]
                cur_memory["description"] = cur_memory["description"].replace("{{char}}", cur_memory["name"])
                print("æ·»åŠ è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["description"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n")
                content_append(request.messages, 'system', "è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["description"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n")
            if cur_memory["personality"]:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢cur_memory["personality"]ä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    cur_memory["personality"] = cur_memory["personality"].replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢cur_memory["personality"]ä¸­çš„{{char}}ä¸ºcur_memory["name"]
                cur_memory["personality"] = cur_memory["personality"].replace("{{char}}", cur_memory["name"])
                print("æ·»åŠ æ€§æ ¼è®¾å®šï¼š\n\n" + cur_memory["personality"] + "\n\næ€§æ ¼è®¾å®šç»“æŸ\n\n")
                content_append(request.messages, 'system', "æ€§æ ¼è®¾å®šï¼š\n\n" + cur_memory["personality"] + "\n\næ€§æ ¼è®¾å®šç»“æŸ\n\n") 
            if cur_memory['mesExample']:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢cur_memory["mesExample"]ä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    cur_memory["mesExample"] = cur_memory["mesExample"].replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢cur_memory["mesExample"]ä¸­çš„{{char}}ä¸ºcur_memory["name"]
                cur_memory["mesExample"] = cur_memory["mesExample"].replace("{{char}}", cur_memory["name"])
                print("æ·»åŠ å¯¹è¯ç¤ºä¾‹ï¼š\n\n" + cur_memory['mesExample'] + "\n\nå¯¹è¯ç¤ºä¾‹ç»“æŸ\n\n")
                content_append(request.messages, 'system', "å¯¹è¯ç¤ºä¾‹ï¼š\n\n" + cur_memory['mesExample'] + "\n\nå¯¹è¯ç¤ºä¾‹ç»“æŸ\n\n")
            if cur_memory["systemPrompt"]:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢cur_memory["systemPrompt"]ä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    cur_memory["systemPrompt"] = cur_memory["systemPrompt"].replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢cur_memory["systemPrompt"]ä¸­çš„{{char}}ä¸ºcur_memory["name"]
                cur_memory["systemPrompt"] = cur_memory["systemPrompt"].replace("{{char}}", cur_memory["name"])
                print("æ·»åŠ ç³»ç»Ÿæç¤ºï¼š\n\n" + cur_memory["systemPrompt"] + "\n\nç³»ç»Ÿæç¤ºç»“æŸ\n\n")
                content_append(request.messages, 'system', "ç³»ç»Ÿæç¤ºï¼š\n\n" + cur_memory["systemPrompt"] + "\n\nç³»ç»Ÿæç¤ºç»“æŸ\n\n")
            if settings["memorySettings"]["genericSystemPrompt"]:
                if settings["memorySettings"]["userName"]:
                    # æ›¿æ¢settings["memorySettings"]["genericSystemPrompt"]ä¸­çš„{{user}}ä¸ºsettings["memorySettings"]["userName"]
                    settings["memorySettings"]["genericSystemPrompt"] = settings["memorySettings"]["genericSystemPrompt"].replace("{{user}}", settings["memorySettings"]["userName"])
                # æ›¿æ¢cur_memory["systemPrompt"]ä¸­çš„{{char}}ä¸ºcur_memory["name"]
                settings["memorySettings"]["genericSystemPrompt"] = settings["memorySettings"]["genericSystemPrompt"].replace("{{char}}", cur_memory["name"])
                print("æ·»åŠ ç³»ç»Ÿæç¤ºï¼š\n\n" + settings["memorySettings"]["genericSystemPrompt"] + "\n\nç³»ç»Ÿæç¤ºç»“æŸ\n\n")
                content_append(request.messages, 'system', "ç³»ç»Ÿæç¤ºï¼š\n\n" + settings["memorySettings"]["genericSystemPrompt"] + "\n\nç³»ç»Ÿæç¤ºç»“æŸ\n\n")
                    
            if m0:
                memoryLimit = settings["memorySettings"]["memoryLimit"]
                try:
                    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šä½¿ç”¨ asyncio.to_thread å°†åŒæ­¥çš„ search æ–¹æ³•æ”¾å…¥çº¿ç¨‹æ± è¿è¡Œ
                    # è¿™æ ·ä¸»çº¿ç¨‹ï¼ˆEvent Loopï¼‰ä¼šè¢«é‡Šæ”¾ï¼Œå¯ä»¥å»å¤„ç† /minilm/embeddings è¯·æ±‚ï¼Œä»è€Œé¿å…æ­»é”
                    relevant_memories = await asyncio.to_thread(
                        m0.search, 
                        query=user_prompt, 
                        user_id=memoryId, 
                        limit=memoryLimit
                    )
                    relevant_memories = json.dumps(relevant_memories, ensure_ascii=False)
                except Exception as e:
                    print("m0.search error:",e)
                    relevant_memories = ""
                print("æ·»åŠ ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n")
                content_append(request.messages, 'system', "ä¹‹å‰çš„ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n") 
        if settings["knowledgeBases"]:
            for kb in settings["knowledgeBases"]:
                if kb["enabled"] and kb["processingStatus"] == "completed":
                    kb_list.append({"kb_id":kb["id"],"name": kb["name"],"introduction":kb["introduction"]})
        if settings["KBSettings"]["when"] == "before_thinking" or settings["KBSettings"]["when"] == "both":
            if kb_list:
                all_kb_content = []
                # ç”¨query_knowledge_baseå‡½æ•°æŸ¥è¯¢kb_listä¸­æ‰€æœ‰çš„çŸ¥è¯†åº“
                for kb in kb_list:
                    kb_content = await query_knowledge_base(kb["kb_id"],user_prompt)
                    all_kb_content.extend(kb_content)
                    if settings["KBSettings"]["is_rerank"]:
                        all_kb_content = await rerank_knowledge_base(user_prompt,all_kb_content)
                if all_kb_content:
                    kb_message = f"\n\nå¯å‚è€ƒçš„çŸ¥è¯†åº“å†…å®¹ï¼š{all_kb_content}"
                    content_append(request.messages, 'user',  f"{kb_message}\n\nç”¨æˆ·ï¼š{user_prompt}")
        if settings["KBSettings"]["when"] == "after_thinking" or settings["KBSettings"]["when"] == "both":
            if kb_list:
                kb_list_message = f"\n\nå¯è°ƒç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨ï¼š{json.dumps(kb_list, ensure_ascii=False)}"
                content_append(request.messages, 'system', kb_list_message)
        else:
            kb_list = []
        request = await tools_change_messages(request, settings)
        chat_vendor = 'OpenAI'
        reasoner_vendor = 'OpenAI'
        for modelProvider in settings['modelProviders']: 
            if modelProvider['id'] == settings['selectedProvider']:
                chat_vendor = modelProvider['vendor']
                break
        for modelProvider in settings['modelProviders']: 
            if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                reasoner_vendor = modelProvider['vendor']
                break
        if chat_vendor == 'Dify':
            try:
                if len(request.messages) >= 3:
                    if request.messages[2]['role'] == 'user':
                        if request.messages[1]['role'] == 'assistant':
                            request.messages[2]['content'] = "ä½ ä¸Šä¸€æ¬¡çš„å‘è¨€ï¼š\n" +request.messages[0]['content'] + "\nä½ ä¸Šä¸€æ¬¡çš„å‘è¨€ç»“æŸ\n\nç”¨æˆ·ï¼š" + request.messages[2]['content']
                        if request.messages[0]['role'] == 'system':
                            request.messages[2]['content'] = "ç³»ç»Ÿæç¤ºï¼š\n" +request.messages[0]['content'] + "\nç³»ç»Ÿæç¤ºç»“æŸ\n\n" + request.messages[2]['content']
                elif len(request.messages) >= 2:
                    if request.messages[1]['role'] == 'user':
                        if request.messages[0]['role'] == 'system':
                            request.messages[1]['content'] = "ç³»ç»Ÿæç¤ºï¼š\n" +request.messages[0]['content'] + "\nç³»ç»Ÿæç¤ºç»“æŸ\n\nç”¨æˆ·ï¼š" + request.messages[1]['content']
            except Exception as e:
                print("Dify error:",e)
        if settings['webSearch']['enabled'] or enable_web_search:
            if settings['webSearch']['when'] == 'before_thinking' or settings['webSearch']['when'] == 'both':
                if settings['webSearch']['engine'] == 'duckduckgo':
                    results = await DDGsearch_async(user_prompt)
                elif settings['webSearch']['engine'] == 'searxng':
                    results = await searxng_async(user_prompt)
                elif settings['webSearch']['engine'] == 'tavily':
                    results = await Tavily_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'bing':
                    results = await Bing_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'google':
                    results = await Google_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'brave':
                    results = await Brave_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'exa':
                    results = await Exa_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'serper':
                    results = await Serper_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'bochaai':
                    results = await bochaai_search_async(user_prompt)
                if results:
                    content_append(request.messages, 'user',  f"\n\nè”ç½‘æœç´¢ç»“æœï¼š{results}")
            if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                if settings['webSearch']['engine'] == 'duckduckgo':
                    tools.append(duckduckgo_tool)
                elif settings['webSearch']['engine'] == 'searxng':
                    tools.append(searxng_tool)
                elif settings['webSearch']['engine'] == 'tavily':
                    tools.append(tavily_tool)
                elif settings['webSearch']['engine'] == 'bing':
                    tools.append(bing_tool)
                elif settings['webSearch']['engine'] == 'google':
                    tools.append(google_tool)
                elif settings['webSearch']['engine'] == 'brave':
                    tools.append(brave_tool)
                elif settings['webSearch']['engine'] == 'exa':
                    tools.append(exa_tool)
                elif settings['webSearch']['crawler'] == 'serper':
                    tools.append(serper_tool)
                elif settings['webSearch']['crawler'] == 'bochaai':
                    tools.append(bochaai_tool)

                if settings['webSearch']['crawler'] == 'jina':
                    tools.append(jina_crawler_tool)
                elif settings['webSearch']['crawler'] == 'crawl4ai':
                    tools.append(Crawl4Ai_tool)
        if kb_list:
            tools.append(kb_tool)
        if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
            deepsearch_messages = copy.deepcopy(request.messages)
            content_append(deepsearch_messages, 'user',  "\n\nå°†ç”¨æˆ·æå‡ºçš„é—®é¢˜æˆ–ç»™å‡ºçš„å½“å‰ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤ï¼Œæ¯ä¸€ä¸ªæ­¥éª¤ç”¨ä¸€å¥ç®€çŸ­çš„è¯æ¦‚æ‹¬å³å¯ï¼Œæ— éœ€å›ç­”æˆ–æ‰§è¡Œè¿™äº›å†…å®¹ï¼Œç›´æ¥è¿”å›æ€»ç»“å³å¯ï¼Œä½†ä¸èƒ½çœç•¥é—®é¢˜æˆ–ä»»åŠ¡çš„ç»†èŠ‚ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„åªæ˜¯é—²èŠæˆ–è€…ä¸åŒ…å«ä»»åŠ¡å’Œé—®é¢˜ï¼Œç›´æ¥æŠŠç”¨æˆ·è¾“å…¥é‡å¤è¾“å‡ºä¸€éå³å¯ã€‚å¦‚æœæ˜¯éå¸¸ç®€å•çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥åªç»™å‡ºä¸€ä¸ªæ­¥éª¤å³å¯ã€‚ä¸€èˆ¬æƒ…å†µä¸‹éƒ½æ˜¯éœ€è¦æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤çš„ã€‚")
            response = await client.chat.completions.create(
                model=model,
                messages=deepsearch_messages,
                temperature=0.5, 
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
            user_prompt = response.choices[0].message.content
            content_append(request.messages, 'user',  f"\n\nå¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºé—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œç›´æ¥é—²èŠå³å¯ï¼Œå¦‚æœç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œä»»åŠ¡æè¿°ä¸æ¸…æ™°æˆ–è€…ä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚")
        if settings['reasoner']['enabled'] or enable_thinking:
            reasoner_messages = copy.deepcopy(request.messages)
            if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                drs_msg = get_drs_stage(DRS_STAGE)
                if drs_msg:
                    content_append(reasoner_messages, 'user',  f"\n\n{drs_msg}\n\n")
                content_append(reasoner_messages, 'user',  f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n")
            if tools:
                content_append(reasoner_messages, 'system',  f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}")
            for modelProvider in settings['modelProviders']: 
                if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                    vendor = modelProvider['vendor']
                    break
            msg = await images_add_in_messages(reasoner_messages, images,settings)   
            if chat_vendor == 'OpenAI':
                extra['max_completion_tokens'] = request.max_tokens or settings['max_tokens']
            else:
                extra['max_tokens'] = request.max_tokens or settings['max_tokens']
            if reasoner_vendor == 'OpenAI':
                reasoner_extra['max_completion_tokens'] = settings['reasoner']['max_tokens']
            else:
                reasoner_extra['max_tokens'] = settings['reasoner']['max_tokens']
            if request.reasoning_effort or settings['reasoning_effort']:
                extra['reasoning_effort'] = request.reasoning_effort or settings['reasoning_effort']
            if settings['reasoner']['reasoning_effort'] is not None:
                reasoner_extra['reasoning_effort'] = settings['reasoner']['reasoning_effort'] 
            if vendor == 'Ollama':
                reasoner_response = await reasoner_client.chat.completions.create(
                    model=settings['reasoner']['model'],
                    messages=msg,
                    stream=False,
                    temperature=settings['reasoner']['temperature'],
                    **reasoner_extra
                )
                reasoning_buffer = reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
                if reasoning_buffer:
                    content_prepend(request.messages, 'assistant', reasoning_buffer) # å¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹
                else:
                    reasoning_buffer = reasoner_response.model_dump()['choices'][0]['message']['reasoning']
                    if reasoning_buffer:
                        content_prepend(request.messages, 'assistant', reasoning_buffer) # å¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹
                    else:
                        # å°†æ¨ç†ç»“æœä¸­çš„æ€è€ƒå†…å®¹æå–å‡ºæ¥
                        reasoning_content = reasoner_response.model_dump()['choices'][0]['message']['content']
                        # open_tagå’Œclose_tagä¹‹é—´çš„å†…å®¹
                        start_index = reasoning_content.find(open_tag) + len(open_tag)
                        end_index = reasoning_content.find(close_tag)
                        if start_index != -1 and end_index != -1:
                            reasoning_content = reasoning_content[start_index:end_index]
                        else:
                            reasoning_content = ""
                        content_prepend(request.messages, 'assistant', reasoning_content) # å¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹
            else:
                reasoner_response = await reasoner_client.chat.completions.create(
                    model=settings['reasoner']['model'],
                    messages=msg,
                    stream=False,
                    stop=settings['reasoner']['stop_words'],
                    temperature=settings['reasoner']['temperature'],
                    **reasoner_extra
                )
                reasoning_buffer = reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
                if reasoning_buffer:
                    content_prepend(request.messages, 'assistant', reasoning_buffer) # å¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹
                else:
                    reasoning_buffer = reasoner_response.model_dump()['choices'][0]['message']['reasoning']
                    if reasoning_buffer:
                        content_prepend(request.messages, 'assistant', reasoning_buffer) # å¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹
                    else:
                        reasoning_buffer = ""
                        content_prepend(request.messages, 'assistant', reasoning_buffer) # å¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹
        if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
            content_append(request.messages, 'user',  f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n")
            drs_msg = get_drs_stage(DRS_STAGE)
            if drs_msg:
                content_append(request.messages, 'user',  f"\n\n{drs_msg}\n\n")
        msg = await images_add_in_messages(request.messages, images,settings)
        if tools:
            response = await client.chat.completions.create(
                model=model,
                messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                temperature=request.temperature,
                tools=tools,
                stream=False,
                top_p=request.top_p or settings['top_p'],
                extra_body = extra_params, # å…¶ä»–å‚æ•°
                **extra
            )
        else:
            response = await client.chat.completions.create(
                model=model,
                messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                temperature=request.temperature,
                stream=False,
                top_p=request.top_p or settings['top_p'],
                extra_body = extra_params, # å…¶ä»–å‚æ•°
                **extra
            )
        if response.choices[0].message.tool_calls:
            pass
        elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
            search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,response.choices[0].message.content)
            research_response = await client.chat.completions.create(
                model=model,
                messages=[
                    {
                    "role": "user",
                    "content": search_prompt,
                    }
                ],
                temperature=0.5,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
            response_content = research_response.choices[0].message.content
            # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
            if "```json" in response_content:
                try:
                    response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                except:
                    # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                    response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
            response_content = json.loads(response_content)
            if response_content["status"] == "done":
                search_not_done = False
            elif response_content["status"] == "not_done":
                search_not_done = True
                search_task = response_content["unfinished_task"]
                task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": task_prompt,
                    }
                )
            elif response_content["status"] == "need_more_info":
                DRS_STAGE = 2
                search_not_done = False
            elif response_content["status"] == "need_work":
                DRS_STAGE = 2
                search_not_done = True
                drs_msg = get_drs_stage(DRS_STAGE)
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": drs_msg,
                    }
                )
            elif response_content["status"] == "need_more_work":
                DRS_STAGE = 2
                search_not_done = True
                search_task = response_content["unfinished_task"]
                task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": task_prompt,
                    }
                )
            elif response_content["status"] == "answer":
                DRS_STAGE = 3
                search_not_done = True
                drs_msg = get_drs_stage(DRS_STAGE)
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": drs_msg,
                    }
                )
        reasoner_messages = copy.deepcopy(request.messages)
        while response.choices[0].message.tool_calls or search_not_done:
            if response.choices[0].message.tool_calls:
                assistant_message = response.choices[0].message
                response_content = assistant_message.tool_calls[0].function
                print(response_content.name)
                modified_data = '[' + response_content.arguments.replace('}{', '},{') + ']'
                # ä½¿ç”¨json.loadsæ¥è§£æä¿®æ”¹åçš„å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                data_list = json.loads(modified_data)
                # å­˜å‚¨å¤„ç†ç»“æœ
                results = []
                for data in data_list:
                    result = await dispatch_tool(response_content.name, data,settings) # å°†ç»“æœæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
                    if isinstance(results, AsyncIterator):
                        buffer = []
                        async for chunk in results:
                            buffer.append(chunk)
                        results = "".join(buffer)
                    if result is not None:
                        # å°†ç»“æœæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
                        results.append(json.dumps(result))
                # å°†æ‰€æœ‰ç»“æœæ‹¼æ¥æˆä¸€ä¸ªè¿ç»­çš„å­—ç¬¦ä¸²
                combined_results = ''.join(results)
                if combined_results:
                    results = combined_results
                else:
                    results = None
                if results is None:
                    break
                if response_content.name in ["query_knowledge_base"]:
                    if settings["KBSettings"]["is_rerank"]:
                        results = await rerank_knowledge_base(user_prompt,results)
                    results = json.dumps(results, ensure_ascii=False, indent=4)
                request.messages.append(
                    {
                        "tool_calls": [
                            {
                                "id": assistant_message.tool_calls[0].id,
                                "function": {
                                    "arguments": response_content.arguments,
                                    "name": response_content.name,
                                },
                                "type": assistant_message.tool_calls[0].type,
                            }
                        ],
                        "role": "assistant",
                        "content": "",
                    }
                )
                request.messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": assistant_message.tool_calls[0].id,
                        "name": response_content.name,
                        "content": str(results),
                    }
                )
            if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                content_append(request.messages, 'user',  f"\nå¯¹äºè”ç½‘æœç´¢çš„ç»“æœï¼Œå¦‚æœè”ç½‘æœç´¢çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨è”ç½‘æœç´¢æŸ¥è¯¢è¿˜æœªç»™å‡ºçš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœå·²ç»è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚")
            reasoner_messages.append(
                {
                    "role": "assistant",
                    "content": str(response_content),
                }
            )
            reasoner_messages.append(
                {
                    "role": "user",
                    "content": f"{response_content.name}å·¥å…·ç»“æœï¼š"+str(results),
                }
            )
            if settings['reasoner']['enabled'] or enable_thinking:
                if tools:
                    content_append(reasoner_messages, 'system',  f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}")
                for modelProvider in settings['modelProviders']: 
                    if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                        vendor = modelProvider['vendor']
                        break
                msg = await images_add_in_messages(reasoner_messages, images,settings)
                if vendor == 'Ollama':
                    reasoner_response = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=False,
                        temperature=settings['reasoner']['temperature'],
                        **reasoner_extra
                    )
                    # å°†æ¨ç†ç»“æœä¸­çš„æ€è€ƒå†…å®¹æå–å‡ºæ¥
                    reasoning_content = reasoner_response.model_dump()['choices'][0]['message']['content']
                    # open_tagå’Œclose_tagä¹‹é—´çš„å†…å®¹
                    start_index = reasoning_content.find(open_tag) + len(open_tag)
                    end_index = reasoning_content.find(close_tag)
                    if start_index != -1 and end_index != -1:
                        reasoning_content = reasoning_content[start_index:end_index]
                    else:
                        reasoning_content = ""
                    content_prepend(request.messages, 'assistant', reasoning_content) # å¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹
                else:
                    reasoner_response = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=False,
                        stop=settings['reasoner']['stop_words'],
                        temperature=settings['reasoner']['temperature'],
                        **reasoner_extra
                    )
                    content_prepend(request.messages, 'assistant', reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']) # å¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹
            msg = await images_add_in_messages(request.messages, images,settings)
            if tools:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    tools=tools,
                    stream=False,
                    top_p=request.top_p or settings['top_p'],
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                    **extra
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    stream=False,
                    top_p=request.top_p or settings['top_p'],
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                    **extra
                )
            if response.choices[0].message.tool_calls:
                pass
            elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,response.choices[0].message.content)
                research_response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                        "role": "user",
                        "content": search_prompt,
                        }
                    ],
                    temperature=0.5,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
                response_content = research_response.choices[0].message.content
                # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                if "```json" in response_content:
                    try:
                        response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                    except:
                        # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                        response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                response_content = json.loads(response_content)
                if response_content["status"] == "done":
                    search_not_done = False
                elif response_content["status"] == "not_done":
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "need_more_info":
                    DRS_STAGE = 2
                    search_not_done = False
                elif response_content["status"] == "need_work":
                    DRS_STAGE = 2
                    search_not_done = True
                    drs_msg = get_drs_stage(DRS_STAGE)
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": drs_msg,
                        }
                    )
                elif response_content["status"] == "need_more_work":
                    DRS_STAGE = 2
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "answer":
                    DRS_STAGE = 3
                    search_not_done = True
                    drs_msg = get_drs_stage(DRS_STAGE)
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": drs_msg,
                        }
                    )
       # å¤„ç†å“åº”å†…å®¹
        response_dict = response.model_dump()
        content = response_dict["choices"][0]['message']['content']
        if response_dict["choices"][0]['message'].get('reasoning_content',""):
            pass
        else:
            response_dict["choices"][0]['message']['reasoning_content'] = response_dict["choices"][0]['message'].get('reasoning',"")
        if open_tag in content and close_tag in content:
            reasoning_content = re.search(fr'{open_tag}(.*?)\{close_tag}', content, re.DOTALL)
            if reasoning_content:
                # å­˜å‚¨åˆ° reasoning_content å­—æ®µ
                response_dict["choices"][0]['message']['reasoning_content'] = reasoning_content.group(1).strip()
                # ç§»é™¤åŸå†…å®¹ä¸­çš„æ ‡ç­¾éƒ¨åˆ†
                response_dict["choices"][0]['message']['content'] = re.sub(fr'{open_tag}(.*?)\{close_tag}', '', content, flags=re.DOTALL).strip()
        if m0:
            messages=f"ç”¨æˆ·è¯´ï¼š{user_prompt}\n\n---\n\nä½ è¯´ï¼š{response_dict["choices"][0]['message']['content']}"
            executor = ThreadPoolExecutor()
            async def add_async():
                loop = asyncio.get_event_loop()
                # ç»‘å®š user_id å…³é”®å­—å‚æ•°
                metadata = {
                    "timetamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                }
                func = partial(m0.add, user_id=memoryId,metadata=metadata,infer=False)
                # ä¼ é€’ messages ä½œä¸ºä½ç½®å‚æ•°
                await loop.run_in_executor(executor, func, messages)
                print("çŸ¥è¯†åº“æ›´æ–°å®Œæˆ")

            asyncio.create_task(add_async())
        return JSONResponse(content=response_dict)
    except Exception as e:
        return JSONResponse(
            content={"error": {"message": str(e), "type": "api_error"}}
        )

@app.post("/execute_tool_manually")
async def execute_tool_manually(request: Request):
    """
    å‰ç«¯ç‚¹å‡»å®¡æ‰¹æŒ‰é’®åè°ƒç”¨çš„æ¥å£
    """
    data = await request.json()
    tool_name = data.get("tool_name")
    tool_params = data.get("tool_params")
    approval_type = data.get("approval_type") # 'once' æˆ– 'always'
    
    # è·å–å½“å‰é…ç½®
    settings = await load_settings()
    cwd = settings.get("CLISettings", {}).get("cc_path")
    
    # ==================== æ ¸å¿ƒé€»è¾‘ï¼šå¤„ç† "Always" ====================
    if approval_type == "always":
        # å¦‚æœç”¨æˆ·é€‰æ‹©â€œä¸å†è¯¢é—®â€ï¼Œåˆ™å°†è¯¥å·¥å…·å†™å…¥å½“å‰é¡¹ç›®çš„ .party/config.json
        if cwd:
            try:
                add_tool_to_project_config(cwd, tool_name)
                print(f"[Permission] Added {tool_name} to whitelist for project {cwd}")
            except Exception as e:
                return {"result": f"[System Error] Failed to save permission: {str(e)}"}
        else:
             return {"result": "[System Error] No working directory found to save config."}

    # ==================== 1. å¯¼å…¥æ‰€æœ‰å·¥å…·å‡½æ•° ====================
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        Bing_search_async,
        Google_search_async,
        Brave_search_async,
        Exa_search_async,
        Serper_search_async,
        bochaai_search_async,
        jina_crawler_async,
        Crawl4Ai_search_async, 
    )
    from py.know_base import query_knowledge_base
    from py.agent_tool import agent_tool_call
    from py.a2a_tool import a2a_tool_call
    from py.llm_tool import custom_llm_tool
    from py.pollinations import pollinations_image,openai_image,openai_chat_image
    from py.load_files import get_file_content
    from py.code_interpreter import e2b_code_async,local_run_code_async
    from py.custom_http import fetch_custom_http
    from py.comfyui_tool import comfyui_tool_call
    from py.utility_tools import (
        time_async,
        get_weather_async,
        get_location_coordinates_async,
        get_weather_by_city_async,
        get_wikipedia_summary_and_sections,
        get_wikipedia_section_content,
        search_arxiv_papers
    )
    from py.autoBehavior import auto_behavior

    # Docker CLI å·¥å…·ï¼ˆåŸæœ‰ï¼‰
    from py.cli_tool import (
        claude_code_async,
        qwen_code_async,
        docker_sandbox_async,
        list_files_tool,
        read_file_tool,
        search_files_tool,
        edit_file_tool,
        edit_file_patch_tool, 
        glob_files_tool,       
        todo_write_tool, 
        manage_processes_tool,
        docker_manage_ports_tool,
    )

    # æ–°å¢ï¼šæœ¬åœ°ç¯å¢ƒ CLI å·¥å…·ï¼ˆå‡è®¾ä¿å­˜åœ¨ py/local_cli_tool.pyï¼‰
    from py.cli_tool import (
        bash_tool_local,           # æœ¬åœ° bash æ‰§è¡Œï¼ˆå¯¹åº” docker_sandbox_asyncï¼‰
        list_files_tool_local,     # æœ¬åœ°æ–‡ä»¶åˆ—è¡¨
        read_file_tool_local,      # æœ¬åœ°æ–‡ä»¶è¯»å–
        search_files_tool_local,   # æœ¬åœ°æ–‡ä»¶æœç´¢
        edit_file_tool_local,      # æœ¬åœ°æ–‡ä»¶å†™å…¥
        edit_file_patch_tool_local,# æœ¬åœ°ç²¾ç¡®æ›¿æ¢
        glob_files_tool_local,     # æœ¬åœ° glob æŸ¥æ‰¾
        todo_write_tool_local,     # æœ¬åœ°ä»»åŠ¡ç®¡ç†
        local_net_tool,            # æœ¬åœ°ç½‘ç»œå·¥å…·
    )

    from py.cdp_tool import (
        list_pages,
        navigate_page,
        new_page,
        close_page,
        select_page,
        take_snapshot,
        wait_for,
        click,
        fill,
        hover,
        press_key,
        evaluate_script,
        take_screenshot,
        fill_form,
        drag,
        handle_dialog
    )
    from py.random_topic import get_random_topics,get_categories

    # ==================== 2. å®šä¹‰å·¥å…·æ˜ å°„è¡¨ ====================
    _TOOL_HOOKS = {
        "DDGsearch_async": DDGsearch_async,
        "searxng_async": searxng_async,
        "Tavily_search_async": Tavily_search_async,
        "query_knowledge_base": query_knowledge_base,
        "jina_crawler_async": jina_crawler_async,
        "Crawl4Ai_search_async": Crawl4Ai_search_async,
        "agent_tool_call": agent_tool_call,
        "a2a_tool_call": a2a_tool_call,
        "custom_llm_tool": custom_llm_tool,
        "pollinations_image":pollinations_image,
        "get_file_content":get_file_content,
        "get_image_content": get_image_content,
        "e2b_code_async": e2b_code_async,
        "local_run_code_async": local_run_code_async,
        "openai_image": openai_image,
        "openai_chat_image":openai_chat_image,
        "Bing_search_async": Bing_search_async,
        "Google_search_async": Google_search_async,
        "Brave_search_async": Brave_search_async,
        "Exa_search_async": Exa_search_async,
        "Serper_search_async": Serper_search_async,
        "bochaai_search_async": bochaai_search_async,
        "comfyui_tool_call": comfyui_tool_call,
        "time_async": time_async,
        "get_weather_async": get_weather_async,
        "get_location_coordinates_async": get_location_coordinates_async,
        "get_weather_by_city_async":get_weather_by_city_async,
        "get_wikipedia_summary_and_sections": get_wikipedia_summary_and_sections,
        "get_wikipedia_section_content": get_wikipedia_section_content,
        "search_arxiv_papers": search_arxiv_papers,
        "auto_behavior": auto_behavior,
        "claude_code_async": claude_code_async,
        "qwen_code_async": qwen_code_async,
        "list_pages": list_pages,
        "new_page": new_page,
        "close_page": close_page,
        "select_page": select_page,
        "navigate_page": navigate_page,
        "take_snapshot": take_snapshot,
        "click": click,
        "fill": fill,
        "evaluate_script": evaluate_script,
        "take_screenshot": take_screenshot,
        "hover": hover,
        "press_key": press_key,
        "wait_for": wait_for,
        "fill_form":fill_form,
        "drag": drag,
        "handle_dialog": handle_dialog,
        "get_random_topics":get_random_topics,
        "get_categories":get_categories,
        
        # Docker Sandbox ç›¸å…³å·¥å…·ï¼ˆåŸæœ‰ï¼‰
        "docker_sandbox_async": docker_sandbox_async,
        "list_files_tool": list_files_tool,
        "read_file_tool": read_file_tool,
        "search_files_tool": search_files_tool,
        "edit_file_tool": edit_file_tool,
        "edit_file_patch_tool": edit_file_patch_tool,
        "glob_files_tool": glob_files_tool,
        "todo_write_tool": todo_write_tool,
        "manage_processes_tool": manage_processes_tool,
        "docker_manage_ports_tool": docker_manage_ports_tool,
        
        # æœ¬åœ°ç¯å¢ƒå·¥å…·ï¼ˆæ–°å¢ï¼‰- ä¸ Docker ç‰ˆæœ¬åŠŸèƒ½ç›¸åŒä½†æ“ä½œæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
        "bash_tool_local": bash_tool_local,                     # æœ¬åœ° bash æ‰§è¡Œ
        "list_files_tool_local": list_files_tool_local,         # æœ¬åœ°æ–‡ä»¶åˆ—è¡¨
        "read_file_tool_local": read_file_tool_local,           # æœ¬åœ°æ–‡ä»¶è¯»å–
        "search_files_tool_local": search_files_tool_local,     # æœ¬åœ°æ–‡ä»¶æœç´¢
        "edit_file_tool_local": edit_file_tool_local,           # æœ¬åœ°æ–‡ä»¶å†™å…¥
        "edit_file_patch_tool_local": edit_file_patch_tool_local,  # æœ¬åœ°ç²¾ç¡®æ›¿æ¢
        "glob_files_tool_local": glob_files_tool_local,         # æœ¬åœ° glob æŸ¥æ‰¾
        "todo_write_tool_local": todo_write_tool_local,         # æœ¬åœ°ä»»åŠ¡ç®¡ç†
        "local_net_tool": local_net_tool,                       # æœ¬åœ°ç½‘ç»œå·¥å…·
    }
    
    if tool_name not in _TOOL_HOOKS:
        return {"result": f"Tool {tool_name} not found in backend registry."}
    
    tool_func = _TOOL_HOOKS[tool_name]
    
    try:
        # 2. æ‰§è¡Œå·¥å…·
        result = await tool_func(**tool_params)
        
        # 3. å¤„ç†æµå¼è¾“å‡º (AsyncIterator)
        # å¦‚æœæ˜¯æµï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶æ¶ˆè€—å®Œåˆå¹¶æˆå­—ç¬¦ä¸²è¿”å›ç»™å‰ç«¯ä¸€æ¬¡æ€§æ˜¾ç¤º
        # å› ä¸ºæ‰‹åŠ¨æ‰§è¡Œé€šå¸¸ä¸å†æ”¯æŒæµå¼æ‰“å­—æœºæ•ˆæœï¼ˆæˆ–è€…å‰ç«¯å¤„ç†ä¼šæ¯”è¾ƒå¤æ‚ï¼‰
        if hasattr(result, "__aiter__"):
            output_buffer = []
            async for chunk in result:
                output_buffer.append(chunk)
            return {"result": "".join(output_buffer)}
        
        return {"result": str(result)}
        
    except Exception as e:
        return {"result": f"Error executing {tool_name}: {str(e)}"}

# åœ¨ç°æœ‰è·¯ç”±åæ·»åŠ ä»¥ä¸‹ä»£ç 
@app.get("/v1/models")
async def get_models():
    """
    è·å–æ¨¡å‹åˆ—è¡¨
    """
    from openai.types import Model
    from openai.pagination import SyncPage
    try:
        # é‡æ–°åŠ è½½æœ€æ–°è®¾ç½®
        current_settings = await load_settings()
        agents = current_settings['agents']
        # æ„é€ ç¬¦åˆ OpenAI æ ¼å¼çš„ Model å¯¹è±¡
        model_data = [
            Model(
                id=agent["name"],  
                created=0,  
                object="model",
                owned_by="super-agent-party"  # éç©ºå­—ç¬¦ä¸²
            )
            for agent in agents.values()  
        ]
        # æ·»åŠ é»˜è®¤çš„ 'super-model'
        model_data.append(
            Model(
                id='super-model',
                created=0,
                object="model",
                owned_by="super-agent-party"  # éç©ºå­—ç¬¦ä¸²
            )
        )

        # æ„é€ å®Œæ•´ SyncPage å“åº”
        response = SyncPage[Model](
            object="list",
            data=model_data,
            has_more=False  # æ·»åŠ åˆ†é¡µæ ‡è®°
        )
        # ç›´æ¥è¿”å›æ¨¡å‹å­—å…¸ï¼Œç”± FastAPI è‡ªåŠ¨åºåˆ—åŒ–ä¸º JSON
        return response.model_dump()  
        
    except Exception as e:
        return JSONResponse(
            status_code=e.status_code,
            content={
                "error": {
                    "message": str(e),
                    "type": "api_error",
                }
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "error": {
                    "message": str(e),
                    "type": "server_error",
                    "code": 500
                }
            }
        )

# åœ¨ç°æœ‰è·¯ç”±åæ·»åŠ ä»¥ä¸‹ä»£ç 
@app.get("/v1/agents",operation_id="get_agents")
async def get_agents():
    """
    è·å–æ¨¡å‹åˆ—è¡¨
    """
    from openai.types import Model
    from openai.pagination import SyncPage
    try:
        # é‡æ–°åŠ è½½æœ€æ–°è®¾ç½®
        current_settings = await load_settings()
        agents = current_settings['agents']
        # æ„é€ ç¬¦åˆ OpenAI æ ¼å¼çš„ Model å¯¹è±¡
        model_data = [
            {
                "name": agent["name"],
                "description": agent["system_prompt"],
            }
            for agent in agents.values()  
        ]
        # æ·»åŠ é»˜è®¤çš„ 'super-model'
        model_data.append(
            {
                "name": 'super-model',
                "description": "Super-Agent-Party default agent",
            }
        )
        return model_data
        
    except Exception as e:
        return JSONResponse(
            status_code=e.status_code,
            content={
                "error": {
                    "message": str(e),
                    "type": "api_error",
                }
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "error": {
                    "message": str(e),
                    "type": "server_error",
                    "code": 500
                }
            }
        )

class ProviderModelRequest(BaseModel):
    url: str
    api_key: str

@app.post("/v1/providers/models")
async def fetch_provider_models(request: ProviderModelRequest):
    try:
        # ä½¿ç”¨ä¼ å…¥çš„provideré…ç½®åˆ›å»ºAsyncOpenAIå®¢æˆ·ç«¯
        client = AsyncOpenAI(api_key=request.api_key, base_url=request.url)
        # è·å–æ¨¡å‹åˆ—è¡¨
        model_list = await client.models.list()
        # æå–æ¨¡å‹IDå¹¶è¿”å›
        return JSONResponse(content={"data": [model.id for model in model_list.data]})
    except Exception as e:
        # å¤„ç†å¼‚å¸¸ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/chat/completions", operation_id="chat_with_agent_party")
async def chat_endpoint(request: ChatRequest,fastapi_request: Request):
    """
    ç”¨æ¥ä¸agent partyä¸­çš„æ¨¡å‹èŠå¤©
    messages: å¿…å¡«é¡¹ï¼ŒèŠå¤©è®°å½•ï¼ŒåŒ…æ‹¬roleå’Œcontent
    model: å¯é€‰é¡¹ï¼Œé»˜è®¤ä½¿ç”¨ 'super-model'ï¼Œå¯ä»¥ç”¨get_models()è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
    stream: å¯é€‰é¡¹ï¼Œé»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨æµå¼å“åº”
    enable_thinking: é»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
    enable_deep_research: é»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨æ·±åº¦ç ”ç©¶æ¨¡å¼
    enable_web_search: é»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
    """
    fastapi_base_url = str(fastapi_request.base_url)
    global client, settings,reasoner_client,mcp_client_list
    raw_model = request.model or 'super-model'
    override_memory_id = None
    
    if raw_model.startswith("memory/"):
        parts = raw_model.split('/', 2) # åˆ†è§£ä¸º ['memory', 'id', 'rest']
        if len(parts) >= 2:
            override_memory_id = parts[1]
            # å¦‚æœæœ‰ç¬¬ä¸‰éƒ¨åˆ†ï¼Œåˆ™æ˜¯å®é™…çš„æ¨¡å‹/Agentåï¼›å¦åˆ™é»˜è®¤ä¸º super-model
            request.model = parts[2] if len(parts) > 2 else 'super-model'
            print(f"æ£€æµ‹åˆ°åŠ¨æ€ Memory ID: {override_memory_id}, ç›®æ ‡æ¨¡å‹æ›´æ–°ä¸º: {request.model}")
    
    model = request.model or 'super-model'
    enable_thinking = request.enable_thinking or False
    enable_deep_research = request.enable_deep_research or False
    enable_web_search = request.enable_web_search or False
    async_tools_id = request.asyncToolsID or None
    if model == 'super-model':
        current_settings = await load_settings()
        if override_memory_id:
            current_settings["memorySettings"]["is_memory"] = True
            current_settings["memorySettings"]["selectedMemory"] = override_memory_id
        if len(current_settings['modelProviders']) <= 0:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": await t("NoModelProvidersConfigured"), "type": "server_error", "code": 500}}
            )
        vendor = 'OpenAI'
        for modelProvider in current_settings['modelProviders']: 
            if modelProvider['id'] == current_settings['selectedProvider']:
                vendor = modelProvider['vendor']
                break
        client_class = AsyncOpenAI
        if vendor == 'Dify':
            client_class = DifyOpenAIAsync
        reasoner_vendor = 'OpenAI'
        for modelProvider in current_settings['modelProviders']: 
            if modelProvider['id'] == current_settings['reasoner']['selectedProvider']:
                reasoner_vendor = modelProvider['vendor']
                break
        reasoner_client_class = AsyncOpenAI
        if reasoner_vendor == 'Dify':
            reasoner_client_class = DifyOpenAIAsync
        # åŠ¨æ€æ›´æ–°å®¢æˆ·ç«¯é…ç½®
        if (current_settings['api_key'] != settings['api_key'] 
            or current_settings['base_url'] != settings['base_url']):
            client = client_class(
                api_key=current_settings['api_key'],
                base_url=current_settings['base_url'] or "https://api.openai.com/v1",
            )
        if (current_settings['reasoner']['api_key'] != settings['reasoner']['api_key'] 
            or current_settings['reasoner']['base_url'] != settings['reasoner']['base_url']):
            reasoner_client = reasoner_client_class(
                api_key=current_settings['reasoner']['api_key'],
                base_url=current_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
            )
        # å°†"system_prompt"æ’å…¥åˆ°request.messages[0].contentä¸­
        if current_settings['system_prompt']:
            content_prepend(request.messages, 'system', current_settings['system_prompt'] + "\n\n")
        if current_settings != settings:
            settings = current_settings
        try:
            if request.stream:
                return await generate_stream_response(client,reasoner_client, request, settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search,async_tools_id)
            return await generate_complete_response(client,reasoner_client, request, settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search)
        except asyncio.CancelledError:
            # å¤„ç†å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥çš„æƒ…å†µ
            print("Client disconnected")
            raise
        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": str(e), "type": "server_error", "code": 500}}
            )
    else:
        current_settings = await load_settings()
        agentSettings = current_settings['agents'].get(model, {})
        if not agentSettings:
            for agentId , agentConfig in current_settings['agents'].items():
                if current_settings['agents'][agentId]['name'] == model:
                    agentSettings = current_settings['agents'][agentId]
                    break
        if not agentSettings:
            return JSONResponse(
                status_code=404,
                content={"error": {"message": f"Agent {model} not found", "type": "not_found", "code": 404}}
            )
        if agentSettings['config_path']:
            with open(agentSettings['config_path'], 'r' , encoding='utf-8') as f:
                agent_settings = json.load(f)
            # å°†"system_prompt"æ’å…¥åˆ°request.messages[0].contentä¸­
            if agentSettings['system_prompt']:
                content_prepend(request.messages, 'user', agentSettings['system_prompt'] + "\n\n")
        vendor = 'OpenAI'
        for modelProvider in agent_settings['modelProviders']: 
            if modelProvider['id'] == agent_settings['selectedProvider']:
                vendor = modelProvider['vendor']
                break
        client_class = AsyncOpenAI
        if vendor == 'Dify':
            client_class = DifyOpenAIAsync
        reasoner_vendor = 'OpenAI'
        for modelProvider in agent_settings['modelProviders']: 
            if modelProvider['id'] == agent_settings['reasoner']['selectedProvider']:
                reasoner_vendor = modelProvider['vendor']
                break
        reasoner_client_class = AsyncOpenAI
        if reasoner_vendor == 'Dify':
            reasoner_client_class = DifyOpenAIAsync
        agent_client = client_class(
            api_key=agent_settings['api_key'],
            base_url=agent_settings['base_url'] or "https://api.openai.com/v1",
        )
        agent_reasoner_client = reasoner_client_class(
            api_key=agent_settings['reasoner']['api_key'],
            base_url=agent_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
        )
        try:
            if request.stream:
                return await generate_stream_response(agent_client,agent_reasoner_client, request, agent_settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search,async_tools_id)
            return await generate_complete_response(agent_client,agent_reasoner_client, request, agent_settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search)
        except asyncio.CancelledError:
            # å¤„ç†å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥çš„æƒ…å†µ
            print("Client disconnected")
            raise
        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": str(e), "type": "server_error", "code": 500}}
            )

@app.post("/simple_chat")
async def simple_chat_endpoint(request: ChatRequest):
    """
    åŒæ—¶æ”¯æŒæµå¼(stream=true)ä¸éæµå¼(stream=false)
    """
    global client, settings

    current_settings = await load_settings()
    if len(current_settings['modelProviders']) <= 0:
        return JSONResponse(
            status_code=500,
            content={"error": {"message": await t("NoModelProvidersConfigured"),
                               "type": "server_error", "code": 500}}
        )

    # --------------- é€‰ vendor & åˆå§‹åŒ– client ---------------
    vendor = 'OpenAI'
    for mp in current_settings['modelProviders']:
        if mp['id'] == current_settings['selectedProvider']:
            vendor = mp['vendor']
            break
    client_class = DifyOpenAIAsync if vendor == 'Dify' else AsyncOpenAI
    if (current_settings['api_key'] != settings['api_key'] or
            current_settings['base_url'] != settings['base_url']):
        client = client_class(
            api_key=current_settings['api_key'],
            base_url=current_settings['base_url'] or "https://api.openai.com/v1",
        )

    # --------------- è°ƒç”¨å¤§æ¨¡å‹ ---------------
    response = await client.chat.completions.create(
        model=current_settings['model'],
        messages=request.messages,
        stream=request.stream,
        temperature=request.temperature,
    )

    # --------------- éæµå¼ï¼šä¸€æ¬¡æ€§è¿”å› JSON ---------------
    if not request.stream:
        # æ³¨æ„ï¼šopenai è¿”å›çš„æ˜¯ ChatCompletion å¯¹è±¡
        return JSONResponse(content=response.model_dump())

    # --------------- æµå¼ï¼šä¿æŒåŸæ¥çš„ StreamingResponse ---------------
    async def openai_raw_stream():
        async for chunk in response:
            yield chunk.model_dump_json() + '\n'
        # ä¸å‘é€ [DONE]

    return StreamingResponse(
        openai_raw_stream(),
        media_type="text/plain",      # ä¹Ÿå¯ä»¥ä¿æŒ "text/event-stream"
        headers={"Cache-Control": "no-cache"}
    )

def sanitize_proxy_url(input_url: str) -> str:
    """
    é’ˆå¯¹ä»£ç†åœºæ™¯ä¼˜åŒ–çš„ URL å®‰å…¨è¿‡æ»¤
    """
    if not input_url:
        raise HTTPException(status_code=400, detail="URL ä¸èƒ½ä¸ºç©º")
    
    # 1. è§£æ URL
    parsed = urlparse(input_url)
    
    # 2. éªŒè¯åè®® (ç¦æ­¢ file://, gopher:// ç­‰åè®®)
    if parsed.scheme not in ["http", "https"]:
        raise HTTPException(status_code=400, detail="ä»…æ”¯æŒ http æˆ– https åè®®")
    
    if not parsed.netloc:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„åŸŸåæˆ– IP")

    # 3. é‡æ–°æ„é€  URL (æ¶ˆé™¤ SSRF æ±¡ç‚¹)
    # æ’é™¤ userinfo, åªä¿ç•™å¿…è¦éƒ¨åˆ†
    safe_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
    if parsed.query:
        safe_url += f"?{parsed.query}"
    if parsed.fragment:
        safe_url += f"#{parsed.fragment}"

    # 4. å†…ç½‘å®¡è®¡
    if is_private_ip(parsed.hostname):
        logger.warning(f"Internal access detected: {safe_url}")

    return safe_url

@app.api_route("/extension_proxy", methods=["GET", "POST"])
async def extension_proxy(request: Request, url: str):
    """
    æ–¹ä¾¿SAPæ’ä»¶è°ƒç”¨çš„é€šç”¨ä»£ç†æ¥å£ï¼Œè®©æ’ä»¶èƒ½å¤Ÿç»•è¿‡ CORS é™åˆ¶è®¿é—®ä»»æ„ URLã€‚
    """
    BROWSER_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    # --- é˜¶æ®µ A: å®‰å…¨æ ¡éªŒ (ä¿ç•™ï¼Œé˜²æ­¢ SSRF æ”»å‡»å†…ç½‘) ---
    try:
        target_url = sanitize_proxy_url(url)
    except HTTPException as e:
        return Response(content=e.detail, status_code=e.status_code)
    
    # --- é˜¶æ®µ B: æ‰§è¡Œä»£ç†è¯·æ±‚ ---
    method = request.method
    body = await request.body()
    
    # æ„é€  Headerï¼šåªä¿ç•™å¿…è¦çš„ï¼Œå»é™¤æ‚è´¨ï¼Œæ·»åŠ èº«ä»½æ ‡è¯†
    # æ’é™¤å¯èƒ½å¯¼è‡´æŒ‡çº¹æ³„éœ²æˆ–è¢«æ‹’ç»çš„ Header
    excluded_headers = {
        'host', 'content-length', 'connection', 'keep-alive', 
        'upgrade-insecure-requests', 'accept-encoding', 'cookie', 'user-agent'
    }
    
    headers = {
        k: v for k, v in request.headers.items() 
        if k.lower() not in excluded_headers
    }
    
    # ã€å…³é”®ç‚¹ 1ã€‘ï¼šä½¿ç”¨æ ‡å‡†æµè§ˆå™¨ UAï¼Œå£°æ˜è¿™æ˜¯ç”¨æˆ·é˜…è¯»è¡Œä¸º
    headers["User-Agent"] = BROWSER_USER_AGENT
    
    # ã€å…³é”®ç‚¹ 2ã€‘ï¼šæ˜ç¡®å‘Šè¯‰æœåŠ¡å™¨æˆ‘ä»¬æ¥å— XML/RSS æ ¼å¼ï¼Œè¿™æ˜¾å¾—æ›´åƒä¸€ä¸ªè‰¯æ€§é˜…è¯»å™¨
    if "accept" not in headers or "*/*" in headers["accept"]:
        headers["Accept"] = "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"

    # ã€å…³é”®ç‚¹ 3ã€‘ï¼šå¤„ç† Refererã€‚æœ‰äº›é˜²ç›—é“¾æœºåˆ¶éœ€è¦ Refererï¼Œæœ‰äº›ï¼ˆå¦‚ Redditï¼‰çœ‹åˆ°å¥‡æ€ªçš„ Referer ä¼šæ‹¦æˆª
    # æœ€å®‰å…¨çš„åšæ³•æ˜¯ä¸å‘é€ Refererï¼Œæˆ–è€…è®¾ä¸ºç›®æ ‡åŸŸåçš„æ ¹ç›®å½•
    headers.pop("Referer", None) 
    
    print(f"--- [Extension Proxy] ---")
    print(f"Target: {target_url} | Method: {method} | Mode: Browser Emulation")
    
    # trust_env=False: é˜²æ­¢ä½ çš„ Python ä»£ç æ„å¤–ä½¿ç”¨äº†ç³»ç»Ÿå±‚çš„ HTTP ä»£ç†
    async with httpx.AsyncClient(verify=False, follow_redirects=True, timeout=30.0, trust_env=False) as client:
        try:
            resp = await client.request(
                method=method,
                url=target_url,
                headers=headers,
                content=body
            )
            
            # æ¸…æ´—å“åº”å¤´ï¼šé˜²æ­¢å°†å‹ç¼©ç¼–ç æˆ–åˆ†å—ä¼ è¾“é€ä¼ ç»™å‰ç«¯å¯¼è‡´è§£æé”™è¯¯
            resp_headers = {
                k: v for k, v in resp.headers.items()
                if k.lower() not in {
                    'content-encoding', 'content-length', 'transfer-encoding', 
                    'server', 'set-cookie' # ä¹Ÿä¸è¦é€ä¼  Set-Cookieï¼Œä¿æŠ¤ç”¨æˆ·éšç§
                }
            }
            
            # å¦‚æœ Reddit ä¾ç„¶è¿”å› 403ï¼Œé€šå¸¸å†…å®¹é‡Œä¼šæœ‰é”™è¯¯æç¤ºï¼Œç…§æ ·è¿”å›ç»™å‰ç«¯ä¾¿äºè°ƒè¯•
            if resp.status_code == 403:
                print(f"[Proxy Warning] Target returned 403. Body sample: {resp.content[:100]}")

            return Response(
                content=resp.content,
                status_code=resp.status_code,
                headers=resp_headers,
                media_type=resp.headers.get("content-type", "application/octet-stream")
            )

        except httpx.ConnectError as e:
            err_msg = f"Proxy Connect Error: {e}"
            # è¿”å› JSON æ ¼å¼é”™è¯¯ä»¥ä¾¿å‰ç«¯ä¼˜é›…å¤„ç†
            return Response(content=f'{{"error": "{err_msg}"}}', status_code=502, media_type="application/json")
            
        except Exception as e:
            print(f"[Proxy Error] System: {repr(e)}")
            return Response(content='{"error": "Internal Proxy Error"}', status_code=500, media_type="application/json")

        
# å­˜å‚¨æ´»è·ƒçš„ASR WebSocketè¿æ¥
asr_connections = []

# å­˜å‚¨æ¯ä¸ªè¿æ¥çš„éŸ³é¢‘å¸§æ•°æ®
audio_buffer: Dict[str, Dict[str, Any]] = {}

def convert_audio_to_pcm16(audio_bytes: bytes, target_sample_rate: int = 16000) -> bytes:
    """
    å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºPCM16æ ¼å¼ï¼Œé‡‡æ ·ç‡16kHz
    """
    import numpy as np
    from scipy.io import wavfile
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
            temp_file.write(audio_bytes)
            temp_file_path = temp_file.name
        
        try:
            # è¯»å–éŸ³é¢‘æ–‡ä»¶
            sample_rate, audio_data = wavfile.read(temp_file_path)
            
            # è½¬æ¢ä¸ºå•å£°é“
            if len(audio_data.shape) > 1:
                audio_data = np.mean(audio_data, axis=1)
            
            # è½¬æ¢ä¸ºfloat32è¿›è¡Œé‡é‡‡æ ·
            if audio_data.dtype != np.float32:
                if audio_data.dtype == np.int16:
                    audio_data = audio_data.astype(np.float32) / 32768.0
                elif audio_data.dtype == np.int32:
                    audio_data = audio_data.astype(np.float32) / 2147483648.0
                else:
                    audio_data = audio_data.astype(np.float32)
            
            # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
            if sample_rate != target_sample_rate:
                from scipy.signal import resample
                num_samples = int(len(audio_data) * target_sample_rate / sample_rate)
                audio_data = resample(audio_data, num_samples)
            
            # è½¬æ¢ä¸ºint16 PCMæ ¼å¼
            audio_data = (audio_data * 32767).astype(np.int16)
            
            return audio_data.tobytes()
            
        finally:
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file_path)
            
    except Exception as e:
        print(f"Audio conversion error: {e}")
        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥è¿”å›åŸå§‹æ•°æ®
        return audio_bytes

async def funasr_recognize(audio_data: bytes, funasr_settings: dict,ws: WebSocket,frame_id) -> str:
    """
    ä½¿ç”¨FunASRè¿›è¡Œè¯­éŸ³è¯†åˆ«
    """
    try:
        # è·å–FunASRæœåŠ¡å™¨åœ°å€
        funasr_url = funasr_settings.get('funasr_ws_url', 'ws://localhost:10095')
        hotwords = funasr_settings.get('hotwords', '')
        if not funasr_url.startswith('ws://') and not funasr_url.startswith('wss://'):
            funasr_url = f"ws://{funasr_url}"
        
        # è¿æ¥åˆ°FunASRæœåŠ¡å™¨
        async with websockets.connect(funasr_url) as websocket:
            print(f"Connected to FunASR server: {funasr_url}")
            
            # 1. å‘é€åˆå§‹åŒ–é…ç½®
            init_config = {
                "chunk_size": [5, 10, 5],
                "wav_name": "python_client",
                "is_speaking": True,
                "chunk_interval": 10,
                "mode": "offline",  # ä½¿ç”¨ç¦»çº¿æ¨¡å¼
                "hotwords": hotwords_to_json(hotwords),
                "use_itn": True
            }
            
            await websocket.send(json.dumps(init_config))
            print("Sent init config")
            
            # 2. è½¬æ¢éŸ³é¢‘æ•°æ®ä¸ºPCM16æ ¼å¼
            pcm_data = convert_audio_to_pcm16(audio_data)
            print(f"PCM data length: {len(pcm_data)} bytes")
            
            # 3. åˆ†å—å‘é€éŸ³é¢‘æ•°æ®
            chunk_size = 960  # 30msçš„éŸ³é¢‘æ•°æ® (16000 * 0.03 * 2 = 960å­—èŠ‚)
            total_sent = 0
            
            while total_sent < len(pcm_data):
                chunk_end = min(total_sent + chunk_size, len(pcm_data))
                chunk = pcm_data[total_sent:chunk_end]
                
                # å‘é€äºŒè¿›åˆ¶PCMæ•°æ®
                await websocket.send(chunk)
                total_sent = chunk_end
            
            print(f"Sent all audio data: {total_sent} bytes")
            
            # 4. å‘é€ç»“æŸä¿¡å·
            end_config = {
                "is_speaking": False,
            }
            
            await websocket.send(json.dumps(end_config))
            print("Sent end signal")
            
            # 5. ç­‰å¾…è¯†åˆ«ç»“æœ
            result_text = ""
            timeout_count = 0
            max_timeout = 200  # æœ€å¤§ç­‰å¾…20ç§’
            
            while timeout_count < max_timeout:
                try:
                    # ç­‰å¾…å“åº”æ¶ˆæ¯
                    response = await asyncio.wait_for(websocket.recv(), timeout=0.1)
                    
                    try:
                        # å°è¯•è§£æJSONå“åº”
                        json_response = json.loads(response)
                        print(f"Received response: {json_response}")
                        
                        if 'text' in json_response:
                            text = json_response['text']
                            if text and text.strip():
                                result_text += text
                                print(f"Got text: {text}")
                                # å‘é€ç»“æœ
                                await ws.send_json({
                                    "type": "transcription",
                                    "id": frame_id,
                                    "text": result_text,
                                    "is_final": True
                                })
                            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ç»ˆç»“æœ
                            if json_response.get('is_final', False):
                                print("Got final result")
                                break
                                
                    except json.JSONDecodeError:
                        # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå¯èƒ½æ˜¯äºŒè¿›åˆ¶æ•°æ®ï¼Œå¿½ç•¥
                        print(f"Non-JSON response: {response}")
                        pass
                        
                except asyncio.TimeoutError:
                    timeout_count += 1
                    continue
                except websockets.exceptions.ConnectionClosed:
                    print("WebSocket connection closed")
                    break
            
            if not result_text:
                print("No recognition result received")
                return ""
            
            return result_text.strip()
            
    except Exception as e:
        print(f"FunASR recognition error: {e}")
        return f"FunASRè¯†åˆ«é”™è¯¯: {str(e)}"

def hotwords_to_json(input_str):
    # åˆå§‹åŒ–ç»“æœå­—å…¸
    result = {}
    
    # æŒ‰è¡Œåˆ†å‰²è¾“å…¥å­—ç¬¦ä¸²
    lines = input_str.split('\n')
    
    for line in lines:
        # æ¸…ç†è¡Œé¦–å°¾çš„ç©ºç™½å­—ç¬¦
        cleaned_line = line.strip()
        
        # è·³è¿‡ç©ºè¡Œ
        if not cleaned_line:
            continue
            
        # åˆ†å‰²è¯è¯­å’Œæƒé‡
        parts = cleaned_line.rsplit(' ', 1)  # ä»å³è¾¹åˆ†å‰²ä¸€æ¬¡
        
        if len(parts) != 2:
            continue  # è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„è¡Œ
            
        word = parts[0].strip()
        try:
            weight = int(parts[1])
        except ValueError:
            continue  # è·³è¿‡æƒé‡ä¸æ˜¯æ•°å­—çš„è¡Œ
            
        # æ·»åŠ åˆ°ç»“æœå­—å…¸
        result[word] = weight
    
    # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
    return json.dumps(result, ensure_ascii=False)

# ASR WebSocketå¤„ç†
@app.websocket("/ws/asr")
async def asr_websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    
    # ç”Ÿæˆå”¯ä¸€çš„è¿æ¥ID
    connection_id = str(uuid.uuid4())
    asr_connections.append(websocket)
    funasr_websocket = None
    # æ–°å¢ï¼šè¿æ¥çŠ¶æ€è·Ÿè¸ªå˜é‡
    asr_engine = None
    funasr_mode = None
    
    try:
        # å¤„ç†æ¶ˆæ¯
        async for message in websocket.iter_json():
            msg_type = message.get("type")
            
            if msg_type == "init":
                # åŠ è½½è®¾ç½®
                settings = await load_settings()
                asr_settings = settings.get('asrSettings', {})
                asr_engine = asr_settings.get('engine', 'openai')  # å­˜å‚¨å¼•æ“ç±»å‹
                if asr_engine == "funasr":
                    funasr_mode = asr_settings.get('funasr_mode', 'openai')  # å­˜å‚¨æ¨¡å¼
                    if funasr_mode == "2pass" or funasr_mode == "online":
                        # è·å–FunASRæœåŠ¡å™¨åœ°å€
                        funasr_url = asr_settings.get('funasr_ws_url', 'ws://localhost:10095')
                        if not funasr_url.startswith('ws://') and not funasr_url.startswith('wss://'):
                            funasr_url = f"ws://{funasr_url}"
                        try:
                            funasr_websocket = await websockets.connect(funasr_url)
                        except Exception as e:
                            funasr_websocket = None
                            print(f"è¿æ¥FunASRå¤±è´¥: {e}")
                await websocket.send_json({
                    "type": "init_response",
                    "status": "ready"
                })
                print("ASR WebSocket connected:",asr_engine)
            elif msg_type == "audio_start":
                frame_id = message.get("id")
                # åŠ è½½è®¾ç½®
                settings = await load_settings()
                asr_settings = settings.get('asrSettings', {})
                asr_engine = asr_settings.get('engine', 'openai')  # å­˜å‚¨å¼•æ“ç±»å‹
                if asr_engine == "funasr":
                    funasr_mode = asr_settings.get('funasr_mode', '2pass')  # å­˜å‚¨æ¨¡å¼
                    hotwords = asr_settings.get('hotwords', '')
                    if funasr_mode == "2pass":
                        # è·å–FunASRæœåŠ¡å™¨åœ°å€
                        funasr_url = asr_settings.get('funasr_ws_url', 'ws://localhost:10095')
                        if not funasr_url.startswith('ws://') and not funasr_url.startswith('wss://'):
                            funasr_url = f"ws://{funasr_url}"
                        try:
                            if not funasr_websocket:
                                # è¿æ¥åˆ°FunASRæœåŠ¡å™¨ 
                                funasr_websocket = await websockets.connect(funasr_url)
                            # 1. å‘é€åˆå§‹åŒ–é…ç½®
                            init_config = {
                                "chunk_size": [5, 10, 5],
                                "wav_name": "python_client",
                                "is_speaking": True,
                                "chunk_interval": 10,
                                "mode": funasr_mode,  
                                "hotwords": hotwords_to_json(hotwords),
                                "use_itn": True
                            }
                            await funasr_websocket.send(json.dumps(init_config))
                            print("Sent init config")
                            # 2. å¼€å¯ä¸€ä¸ªå¼‚æ­¥ä»»åŠ¡å¤„ç†FunASRçš„å“åº”
                            asyncio.create_task(handle_funasr_response(funasr_websocket, websocket))
                        except Exception as e:
                            print(f"è¿æ¥FunASRå¤±è´¥: {e}")
                            await websocket.send_json({
                                "type": "error",
                                "message": f"æ— æ³•è¿æ¥FunASRæœåŠ¡å™¨: {str(e)}"
                            })
                            # æ ‡è®°è¿æ¥å¤±è´¥ï¼Œé¿å…åç»­æ“ä½œ
                            funasr_websocket = None
                    else:
                        # å…³é—­å¼‚æ­¥ä»»åŠ¡å¤„ç†FunASRçš„å“åº”
                        funasr_websocket = None
                else:
                    # å…³é—­å¼‚æ­¥ä»»åŠ¡å¤„ç†FunASRçš„å“åº”
                    funasr_websocket = None
            # ä¿®æ”¹ç‚¹ï¼šå¢åŠ æµå¼éŸ³é¢‘å¤„ç†å‰çš„æ£€æŸ¥
            elif msg_type == "audio_stream":
                frame_id = message.get("id")
                audio_base64 = message.get("audio")

                # å…³é”®æ£€æŸ¥ï¼šç¡®ä¿funasr_websocketå·²åˆå§‹åŒ–
                if not funasr_websocket:
                    continue  # è·³è¿‡å½“å‰æ¶ˆæ¯å¤„ç†

                if audio_base64:
                    # 1. Base64 è§£ç  â†’ å¾—åˆ°äºŒè¿›åˆ¶ PCM (Int16)
                    pcm_data = base64.b64decode(audio_base64)

                    # 2. ç›´æ¥è½¬å‘äºŒè¿›åˆ¶ç»™ FunASR
                    try:
                        await funasr_websocket.send(pcm_data)
                    except websockets.exceptions.ConnectionClosed:
                        funasr_websocket = None
                        # åŠ è½½è®¾ç½®
                        settings = await load_settings()
                        asr_settings = settings.get('asrSettings', {})
                        asr_engine = asr_settings.get('engine', 'openai')  # å­˜å‚¨å¼•æ“ç±»å‹
                        if asr_engine == "funasr":
                            funasr_mode = asr_settings.get('funasr_mode', '2pass')  # å­˜å‚¨æ¨¡å¼
                            if funasr_mode == "2pass":
                                # è·å–FunASRæœåŠ¡å™¨åœ°å€
                                funasr_url = asr_settings.get('funasr_ws_url', 'ws://localhost:10095')
                                if not funasr_url.startswith('ws://') and not funasr_url.startswith('wss://'):
                                    funasr_url = f"ws://{funasr_url}"
                                try:
                                    funasr_websocket = await websockets.connect(funasr_url)
                                except Exception as e:
                                    funasr_websocket = None
                                    print(f"è¿æ¥FunASRå¤±è´¥: {e}")
            elif msg_type == "audio_complete":
                # å¤„ç†å®Œæ•´çš„éŸ³é¢‘æ•°æ®ï¼ˆéæµå¼æ¨¡å¼ï¼‰
                frame_id = message.get("id")
                audio_b64 = message.get("audio")
                audio_format = message.get("format", "wav")
                
                if audio_b64:
                    # è§£ç base64æ•°æ®
                    audio_bytes = base64.b64decode(audio_b64)
                    print(f"Received audio data: {len(audio_bytes)} bytes, format: {audio_format}")
                    
                    try:
                        # åŠ è½½è®¾ç½®
                        settings = await load_settings()
                        asr_settings = settings.get('asrSettings', {})
                        asr_engine = asr_settings.get('engine', 'openai')
                        
                        result = ""
                        
                        if asr_engine == "openai":
                            # OpenAI ASR
                            audio_file = BytesIO(audio_bytes)
                            audio_file.name = f"audio.{audio_format}"
                            
                            client = AsyncOpenAI(
                                api_key=asr_settings.get('api_key', ''),
                                base_url=asr_settings.get('base_url', '') or "https://api.openai.com/v1"
                            )
                            response = await client.audio.transcriptions.create(
                                file=audio_file,
                                model=asr_settings.get('model', 'whisper-1'),
                            )
                            result = response.text
                            # å‘é€ç»“æœ
                            await websocket.send_json({
                                "type": "transcription",
                                "id": frame_id,
                                "text": result,
                                "is_final": True
                            })
                        elif asr_engine == "funasr":
                            # FunASR
                            print("Using FunASR engine")
                            funasr_mode = asr_settings.get('funasr_mode', 'offline')
                            if funasr_mode == "offline":
                                result = await funasr_recognize(audio_bytes, asr_settings,websocket,frame_id)
                            else:
                                # å…³é”®æ£€æŸ¥ï¼šç¡®ä¿è¿æ¥æœ‰æ•ˆ
                                if not funasr_websocket:
                                    continue
                                
                                # 4. å‘é€ç»“æŸä¿¡å·
                                end_config = {
                                    "is_speaking": False  # åªéœ€å‘é€å¿…è¦çš„ç»“æŸæ ‡è®°
                                }
                                try:
                                    await funasr_websocket.send(json.dumps(end_config))
                                    print("Sent end signal")
                                except websockets.exceptions.ConnectionClosed:
                                    print("FunASRè¿æ¥å·²å…³é—­ï¼Œæ— æ³•å‘é€ç»“æŸä¿¡å·")
                            funasr_websocket = None

                        elif asr_engine == "sherpa":
                            from py.sherpa_asr import sherpa_recognize
                            # æ–°å¢Sherpaå¤„ç†
                            result = await sherpa_recognize(audio_bytes)
                            print(f"Sherpa result: {result}")
                            await websocket.send_json({
                                "type": "transcription",
                                "id": frame_id,
                                "text": result,
                                "is_final": True
                            })
                    except WebSocketDisconnect:
                        print(f"ASR WebSocket disconnected: {connection_id}")
                    except Exception as e:
                        print(f"ASR WebSocket error: {e}")
    finally:
        # æ¸…ç†èµ„æº
        if connection_id in audio_buffer:
            del audio_buffer[connection_id]
        if websocket in asr_connections:
            asr_connections.remove(websocket)
        # æ–°å¢ï¼šç¡®ä¿å…³é—­FunASRè¿æ¥
        if funasr_websocket:
            await funasr_websocket.close()

@app.post("/asr")
async def asr_transcription(
    audio: UploadFile = File(...),
    format: str = Form(default="auto")
):
    """
    HTTPç‰ˆæœ¬çš„ASRæ¥å£
    æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼ï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©ASRå¼•æ“
    """
    try:
        # è¯»å–ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
        audio_bytes = await audio.read()
        print(f"Received audio file: {audio.filename}, size: {len(audio_bytes)} bytes")
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼ˆå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šï¼‰
        if format == "auto":
            if audio.filename:
                file_ext = audio.filename.split('.')[-1].lower()
                format = file_ext if file_ext in ['wav', 'mp3', 'flac', 'ogg', 'm4a'] else 'wav'
            else:
                format = 'wav'
        
        # åŠ è½½è®¾ç½®
        settings = await load_settings()
        asr_settings = settings.get('asrSettings', {})
        asr_engine = asr_settings.get('engine', 'openai')
        
        result = ""
        
        if asr_engine == "openai":
            # OpenAI ASR
            print("Using OpenAI ASR engine")
            audio_file = BytesIO(audio_bytes)
            audio_file.name = f"audio.{format}"
            
            client = AsyncOpenAI(
                api_key=asr_settings.get('api_key', ''),
                base_url=asr_settings.get('base_url', '') or "https://api.openai.com/v1"
            )
            
            response = await client.audio.transcriptions.create(
                file=audio_file,
                model=asr_settings.get('model', 'whisper-1'),
            )
            result = response.text
            
        elif asr_engine == "funasr":
            # FunASRï¼ˆå¼ºåˆ¶ä½¿ç”¨ç¦»çº¿æ¨¡å¼ï¼‰
            print("Using FunASR engine (offline mode)")
            result = await funasr_recognize_offline(audio_bytes, asr_settings)
            
        elif asr_engine == "sherpa":
            from py.sherpa_asr import sherpa_recognize
            # Sherpa ASR
            print("Using Sherpa ASR engine")
            result = await sherpa_recognize(audio_bytes)
        
        else:
            return JSONResponse(
                status_code=400,
                content={
                    "success": False,
                    "error": f"ä¸æ”¯æŒçš„ASRå¼•æ“: {asr_engine}",
                    "text": ""
                }
            )
        
        # è¿”å›è¯†åˆ«ç»“æœ
        return JSONResponse(
            content={
                "success": True,
                "text": result.strip(),
                "engine": asr_engine,
                "format": format
            }
        )
        
    except Exception as e:
        print(f"ASR HTTP interface error: {e}")
        
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": str(e),
                "text": ""
            }
        )

async def funasr_recognize_offline(audio_data: bytes, funasr_settings: dict) -> str:
    """
    FunASRç¦»çº¿è¯†åˆ«ï¼ˆä¸“ä¸ºHTTPæ¥å£ä¼˜åŒ–ï¼‰
    """
    try:
        # è·å–FunASRæœåŠ¡å™¨åœ°å€
        funasr_url = funasr_settings.get('funasr_ws_url', 'ws://localhost:10095')
        hotwords = funasr_settings.get('hotwords', '')
        if not funasr_url.startswith('ws://') and not funasr_url.startswith('wss://'):
            funasr_url = f"ws://{funasr_url}"
        
        # è¿æ¥åˆ°FunASRæœåŠ¡å™¨
        async with websockets.connect(funasr_url) as websocket:
            print(f"Connected to FunASR server: {funasr_url}")
            
            # 1. å‘é€åˆå§‹åŒ–é…ç½®ï¼ˆå¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼‰
            init_config = {
                "chunk_size": [5, 10, 5],
                "wav_name": "http_client",
                "is_speaking": True,
                "chunk_interval": 10,
                "mode": "offline",  # å¼ºåˆ¶ä½¿ç”¨ç¦»çº¿æ¨¡å¼
                "hotwords": hotwords_to_json(hotwords),
                "use_itn": True
            }
            
            await websocket.send(json.dumps(init_config))
            print("Sent init config for offline mode")
            
            # 2. è½¬æ¢éŸ³é¢‘æ•°æ®ä¸ºPCM16æ ¼å¼
            pcm_data = convert_audio_to_pcm16(audio_data)
            print(f"PCM data length: {len(pcm_data)} bytes")
            
            # 3. åˆ†å—å‘é€éŸ³é¢‘æ•°æ®
            chunk_size = 960  # 30msçš„éŸ³é¢‘æ•°æ®
            total_sent = 0
            
            while total_sent < len(pcm_data):
                chunk_end = min(total_sent + chunk_size, len(pcm_data))
                chunk = pcm_data[total_sent:chunk_end]
                await websocket.send(chunk)
                total_sent = chunk_end
            
            print(f"Sent all audio data: {total_sent} bytes")
            
            # 4. å‘é€ç»“æŸä¿¡å·
            end_config = {
                "is_speaking": False,
            }
            await websocket.send(json.dumps(end_config))
            print("Sent end signal")
            
            # 5. ç­‰å¾…è¯†åˆ«ç»“æœ
            result_text = ""
            timeout_count = 0
            max_timeout = 300  # æœ€å¤§ç­‰å¾…30ç§’ï¼ˆHTTPæ¥å£å¯ä»¥ç­‰å¾…æ›´ä¹…ï¼‰
            
            while timeout_count < max_timeout:
                try:
                    response = await asyncio.wait_for(websocket.recv(), timeout=0.1)
                    
                    try:
                        json_response = json.loads(response)
                        print(f"Received response: {json_response}")
                        
                        if 'text' in json_response:
                            text = json_response['text']
                            if text and text.strip():
                                result_text += text
                                print(f"Got text: {text}")
                            
                            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ç»ˆç»“æœ
                            if json_response.get('is_final', False):
                                print("Got final result")
                                break
                                
                    except json.JSONDecodeError:
                        # å¿½ç•¥éJSONæ ¼å¼çš„å“åº”
                        pass
                        
                except asyncio.TimeoutError:
                    timeout_count += 1
                    continue
                except websockets.exceptions.ConnectionClosed:
                    print("WebSocket connection closed")
                    break
            
            if not result_text:
                print("No recognition result received")
                return ""
            
            return result_text.strip()
            
    except Exception as e:
        print(f"FunASR offline recognition error: {e}")
        return f"FunASRè¯†åˆ«é”™è¯¯: {str(e)}"


async def handle_funasr_response(funasr_websocket, 
                               client_websocket: WebSocket):
    """
    å¤„ç† FunASR æœåŠ¡å™¨çš„å“åº”ï¼Œå¹¶å°†ç»“æœè½¬å‘ç»™å®¢æˆ·ç«¯
    """
    try:
        async for message in funasr_websocket:
            try:
                if funasr_websocket:
                    # FunASR è¿”å›çš„æ•°æ®å¯èƒ½æ˜¯ JSON æˆ–äºŒè¿›åˆ¶
                    if isinstance(message, bytes):
                        message = message.decode('utf-8')
                    
                    data = json.loads(message)
                    print(f"FunASR response: {data}")
                    # è§£æ FunASR å“åº”
                    if "text" in data:  # æ™®é€šè¯†åˆ«ç»“æœ
                        if data.get('mode', '') == "2pass-online":
                            await client_websocket.send_json({
                                "type": "transcription",
                                "text": data["text"],
                                "is_final": False
                            })
                        else:
                            await client_websocket.send_json({
                                "type": "transcription",
                                "text": data["text"],
                                "is_final": True
                            })
                    elif "mode" in data:  # åˆå§‹åŒ–å“åº”
                        print(f"FunASR initialized: {data}")
                    else:
                        print(f"Unknown FunASR response: {data}")
                else:
                    # å¦‚æœ FunASR è¿æ¥å…³é—­ï¼Œå‘é€é”™è¯¯æ¶ˆæ¯ï¼Œé€€å‡ºå¾ªç¯ï¼Œç»“æŸä»»åŠ¡
            
                    break
            except json.JSONDecodeError:
                print(f"FunASR sent non-JSON data: {message[:100]}...")
            except Exception as e:
                print(f"Error processing FunASR response: {e}")
                break

    except websockets.exceptions.ConnectionClosed:
        print("FunASR connection closed")
    except Exception as e:
        print(f"FunASR handler error: {e}")
    finally:
        await funasr_websocket.close()

class TTSConnectionManager:
    def __init__(self):
        self.main_connections: List[WebSocket] = []
        self.vrm_connections: List[WebSocket] = []
        self.audio_cache: Dict[str, bytes] = {}  # ç¼“å­˜éŸ³é¢‘æ•°æ®
        
    async def connect_main(self, websocket: WebSocket):
        await websocket.accept()
        self.main_connections.append(websocket)
        logging.info(f"Main interface connected. Total: {len(self.main_connections)}")
        
    async def connect_vrm(self, websocket: WebSocket):
        await websocket.accept()
        self.vrm_connections.append(websocket)
        logging.info(f"VRM interface connected. Total: {len(self.vrm_connections)}")
        
    def disconnect_main(self, websocket: WebSocket):
        if websocket in self.main_connections:
            self.main_connections.remove(websocket)
            logging.info(f"Main interface disconnected. Total: {len(self.main_connections)}")
            
    def disconnect_vrm(self, websocket: WebSocket):
        if websocket in self.vrm_connections:
            self.vrm_connections.remove(websocket)
            logging.info(f"VRM interface disconnected. Total: {len(self.vrm_connections)}")
    
    async def broadcast_to_vrm(self, message: dict):
        """å¹¿æ’­æ¶ˆæ¯åˆ°æ‰€æœ‰VRMè¿æ¥"""
        if self.vrm_connections:
            message_str = json.dumps(message)
            disconnected = []
            
            for connection in self.vrm_connections:
                try:
                    await connection.send_text(message_str)
                except:
                    disconnected.append(connection)
            
            # æ¸…ç†æ–­å¼€çš„è¿æ¥
            for conn in disconnected:
                self.disconnect_vrm(conn)
    
    async def send_to_main(self, message: dict):
        """å‘é€æ¶ˆæ¯åˆ°ä¸»ç•Œé¢"""
        if self.main_connections:
            message_str = json.dumps(message)
            disconnected = []
            
            for connection in self.main_connections:
                try:
                    await connection.send_text(message_str)
                except:
                    disconnected.append(connection)
            
            # æ¸…ç†æ–­å¼€çš„è¿æ¥
            for conn in disconnected:
                self.disconnect_main(conn)
    
    def cache_audio(self, audio_id: str, audio_data: bytes):
        """ç¼“å­˜éŸ³é¢‘æ•°æ®"""
        self.audio_cache[audio_id] = audio_data
        
    def get_cached_audio(self, audio_id: str) -> bytes:
        """è·å–ç¼“å­˜çš„éŸ³é¢‘æ•°æ®"""
        return self.audio_cache.get(audio_id)

# åˆ›å»ºè¿æ¥ç®¡ç†å™¨å®ä¾‹
tts_manager = TTSConnectionManager()

@app.websocket("/ws/tts")
async def tts_websocket_endpoint(websocket: WebSocket):
    """ä¸»ç•Œé¢çš„WebSocketè¿æ¥"""
    await tts_manager.connect_main(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            logging.info(f"Received from main: {message['type']}")
            
            # å¦‚æœæ¶ˆæ¯åŒ…å«éŸ³é¢‘URLï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            if message['type'] == 'startSpeaking' and 'audioUrl' in message['data']:
                # è·å–éŸ³é¢‘æ•°æ®å¹¶è½¬æ¢ä¸ºbase64
                audio_url = message['data']['audioUrl']
                chunk_index = message['data']['chunkIndex']
                expressions = message['data']['expressions']
                # ç”ŸæˆéŸ³é¢‘ID
                audio_id = f"chunk_{chunk_index}_{message['data'].get('timestamp', '')}"
                
                # ä¿®æ”¹æ¶ˆæ¯ï¼Œä½¿ç”¨éŸ³é¢‘IDè€Œä¸æ˜¯URL
                message['data']['audioId'] = audio_id
                message['data']['useBase64'] = True
                
                # å¦‚æœæœ‰ç¼“å­˜çš„éŸ³é¢‘æ•°æ®ï¼Œç›´æ¥å‘é€
                cached_audio = tts_manager.get_cached_audio(audio_id)
                if cached_audio:
                    message['data']['audioData'] = base64.b64encode(cached_audio).decode('utf-8')
            
            # è½¬å‘åˆ°æ‰€æœ‰VRMè¿æ¥
            await tts_manager.broadcast_to_vrm({
                'type': message['type'],
                'data': message['data'],
                'timestamp': message.get('timestamp', None)
            })
            
    except WebSocketDisconnect:
        tts_manager.disconnect_main(websocket)
    except Exception as e:
        logging.error(f"WebSocket error in main connection: {e}")
        tts_manager.disconnect_main(websocket)

@app.websocket("/ws/vrm")
async def vrm_websocket_endpoint(websocket: WebSocket):
    """VRMç•Œé¢çš„WebSocketè¿æ¥"""
    await tts_manager.connect_vrm(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            logging.info(f"Received from VRM: {message['type']}")
            
            # å¤„ç†VRMè¯·æ±‚éŸ³é¢‘æ•°æ®
            if message['type'] == 'requestAudioData':
                audio_id = message['data']['audioId']
                expressions = message['data']['expressions']
                text = message['data']['text']
                cached_audio = tts_manager.get_cached_audio(audio_id)
                
                if cached_audio:
                    await websocket.send_text(json.dumps({
                        'type': 'audioData',
                        'data': {
                            'audioId': audio_id,
                            'audioData': base64.b64encode(cached_audio).decode('utf-8'),
                            'expressions':expressions,
                            'text':text
                        }
                    }))
            
            # å¯ä»¥å¤„ç†VRMå‘é€çš„çŠ¶æ€ä¿¡æ¯
            elif message['type'] == 'animationComplete':
                await tts_manager.send_to_main({
                    'type': 'vrmAnimationComplete',
                    'data': message['data']
                })
            
    except WebSocketDisconnect:
        tts_manager.disconnect_vrm(websocket)
    except Exception as e:
        logging.error(f"WebSocket error in VRM connection: {e}")
        tts_manager.disconnect_vrm(websocket)


@app.get("/tts/status")
async def get_tts_status():
    """è·å–å½“å‰TTSè¿æ¥çŠ¶æ€"""
    return {
        "main_connections": len(tts_manager.main_connections),
        "vrm_connections": len(tts_manager.vrm_connections),
        "total_connections": len(tts_manager.main_connections) + len(tts_manager.vrm_connections)
    }


@app.post("/tts")
async def text_to_speech(request: Request):
    import edge_tts
    try:
        data = await request.json()
        text = data['text']
        if text == "":
            return JSONResponse(status_code=400, content={"error": "Text is empty"})
        
        # ç§»åŠ¨ç«¯ä¸“ç”¨ï¼šå¼ºåˆ¶ä½¿ç”¨opusæ ¼å¼
        mobile_optimized = data.get('mobile_optimized', False)
        target_format = "opus" if mobile_optimized else data.get('format', 'mp3')
        
        new_voice = data.get('voice','default')
        tts_settings = data['ttsSettings']
        if new_voice in tts_settings['newtts'] and new_voice!='default':
            # è·å–æ–°å£°éŸ³çš„é…ç½®
            voice_settings = tts_settings['newtts'][new_voice]
            parent_settings = tts_settings
            
            # ä»çˆ¶é…ç½®ç»§æ‰¿å…³é”®å­—æ®µï¼ˆåªç»§æ‰¿éç©ºå€¼ï¼‰
            inherited_fields = ['api_key', 'base_url', 'model', 'selectedProvider', 'vendor']
            for field in inherited_fields:
                # åªåœ¨å­é…ç½®ä¸­ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œä¸”çˆ¶é…ç½®ä¸­æœ‰éç©ºå€¼æ—¶ç»§æ‰¿
                child_value = voice_settings.get(field, '')
                parent_value = parent_settings.get(field, '')
                if not child_value and parent_value:
                    voice_settings[field] = parent_value
            
            # å¦‚æœæœ‰selectedProviderä½†ä»ç¼ºå°‘api_keyï¼Œä»modelProvidersä¸­æŸ¥æ‰¾
            selected_provider_id = voice_settings.get('selectedProvider')
            if selected_provider_id and not voice_settings.get('api_key'):
                model_providers = parent_settings.get('modelProviders', [])
                for provider in model_providers:
                    if provider.get('id') == selected_provider_id:
                        voice_settings['api_key'] = provider.get('apiKey', '')
                        voice_settings['base_url'] = provider.get('url', '')
                        voice_settings['model'] = provider.get('modelId', '')
                        voice_settings['vendor'] = provider.get('vendor', '')
                        break
            
            tts_settings = voice_settings
        index = data['index']
        tts_engine = tts_settings.get('engine', 'edgetts')
                
        print(f"TTSè¯·æ±‚ - å¼•æ“: {tts_engine}, æ ¼å¼: {target_format}, ç§»åŠ¨ç«¯ä¼˜åŒ–: {mobile_optimized}")
                
        if tts_engine == 'edgetts':
            edgettsLanguage = tts_settings.get('edgettsLanguage', 'zh-CN')
            edgettsVoice = tts_settings.get('edgettsVoice', 'XiaoyiNeural')
            rate = tts_settings.get('edgettsRate', 1.0)
            full_voice_name = f"{edgettsLanguage}-{edgettsVoice}"
            
            # é£ä¹¦ä¼˜åŒ–ï¼šç¨å¾®é™ä½è¯­é€Ÿ
            if mobile_optimized:
                rate = min(rate * 0.95, 1.1)
            
            rate_text = "+0%"
            if rate >= 1.0:
                rate_pent = (rate - 1.0) * 100
                rate_text = f"+{int(rate_pent)}%"
            elif rate < 1.0:
                rate_pent = (1.0 - rate) * 100
                rate_text = f"-{int(rate_pent)}%"
            
            async def generate_audio():
                communicate = edge_tts.Communicate(text, full_voice_name, rate=rate_text)
                
                if target_format == "opus":
                    # éœ€è¦è½¬æ¢ä¸ºopusï¼Œæ”¶é›†å®Œæ•´æ•°æ®
                    audio_chunks = []
                    async for chunk in communicate.stream():
                        if chunk["type"] == "audio":
                            audio_chunks.append(chunk["data"])
                    
                    full_audio = b''.join(audio_chunks)
                    
                    # ã€ä¿®å¤ç‚¹ 2ã€‘æ”¾å…¥çº¿ç¨‹æ±  + è§£åŒ…å…ƒç»„
                    convert_result = await asyncio.to_thread(convert_to_opus_simple, full_audio)
                    if isinstance(convert_result, tuple):
                        opus_audio = convert_result[0]
                    else:
                        opus_audio = convert_result
                    
                    # åˆ†å—è¿”å›opusæ•°æ®
                    chunk_size = 4096
                    for i in range(0, len(opus_audio), chunk_size):
                        yield opus_audio[i:i + chunk_size]
                else:
                    # çœŸæµå¼
                    async for chunk in communicate.stream():
                        if chunk["type"] == "audio":
                            yield chunk["data"]

            # è®¾ç½®æ­£ç¡®çš„åª’ä½“ç±»å‹å’Œæ–‡ä»¶å
            if target_format == "opus":
                media_type = "audio/ogg"  # opusé€šå¸¸åŒ…è£…åœ¨oggå®¹å™¨ä¸­
                filename = f"tts_{index}.opus"
            else:
                media_type = "audio/mpeg"  # EdgeTTSé»˜è®¤è¿”å›mp3
                filename = f"tts_{index}.mp3"
            
            return StreamingResponse(
                generate_audio(),
                media_type=media_type,
                headers={
                    "Content-Disposition": f"inline; filename={filename}",
                    "X-Audio-Index": str(index),
                    "X-Audio-Format": target_format
                }
            )

        elif tts_engine == 'customTTS':
            # ä» tts_settings ä¸­è·å–ç”¨æˆ·é…ç½®çš„é”®åï¼Œå¦‚æœæœªé…ç½®ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
            key_text = tts_settings.get('customTTSKeyText', 'text')
            key_speaker = tts_settings.get('customTTSKeySpeaker', 'speaker')
            key_speed = tts_settings.get('customTTSKeySpeed', 'speed')

            # è·å–ç”¨æˆ·é…ç½®çš„ speaker å’Œ speed çš„å€¼
            speaker_value = tts_settings.get('customTTSspeaker', '')
            speed_value = tts_settings.get('customTTSspeed', 1.0)
            
            # ç§»åŠ¨ç«¯ä¼˜åŒ–
            if mobile_optimized:
                speed_value = min(speed_value * 0.95, 1.2)

            # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„é”®åæ„å»º params
            params = {
                key_text: text,
                key_speaker: speaker_value,
                key_speed: speed_value,
            }
            
            custom_tts_servers_list = tts_settings.get('customTTSserver', 'http://127.0.0.1:9880').split('\n')
            custom_tts_servers_list = [server for server in custom_tts_servers_list if server.strip()]
            custom_tt_server = custom_tts_servers_list[index % len(custom_tts_servers_list)]
            
            # è·å–æµå¼é…ç½®
            custom_streaming = tts_settings.get('customStream', False)
            
            async def generate_audio():
                safe_tts_url = sanitize_url(
                    input_url=custom_tt_server,
                    default_base="http://127.0.0.1:9880", # è¿™é‡Œå¡«ä½ ä»£ç é‡ŒåŸæœ¬çš„é»˜è®¤ TTS åœ°å€
                    endpoint=""  # å› ä¸º TTS URL é€šå¸¸å·²ç»åŒ…å«äº†è·¯å¾„
                )
                timeout_config = httpx.Timeout(None, connect=10.0) 
                async with httpx.AsyncClient(timeout=timeout_config) as client:
                    try:
                        async with client.stream("GET", safe_tts_url, params=params) as response:
                            response.raise_for_status()
                            
                            if custom_streaming:
                                # æµå¼æ¨¡å¼ï¼šç›´æ¥è¿”å›æ•°æ®ï¼Œå‡è®¾æœåŠ¡ç«¯èƒ½è¿”å›æ­£ç¡®æ ¼å¼
                                async for chunk in response.aiter_bytes():
                                    if chunk:
                                        yield chunk
                            else:
                                # éæµå¼æ¨¡å¼ï¼šæ”¶é›†å®Œæ•´æ•°æ®ï¼Œè¿›è¡Œæ ¼å¼è½¬æ¢
                                audio_chunks = []
                                async for chunk in response.aiter_bytes():
                                    if chunk:
                                        audio_chunks.append(chunk)
                                
                                full_audio = b''.join(audio_chunks)
                                
                                # è½¬æ¢ä¸ºopus
                                if target_format == "opus":
                                    # ã€ä¿®å¤ç‚¹ 3ã€‘æ”¾å…¥çº¿ç¨‹æ±  + è§£åŒ…å…ƒç»„
                                    convert_result = await asyncio.to_thread(convert_to_opus_simple, full_audio)
                                    if isinstance(convert_result, tuple):
                                        opus_audio = convert_result[0]
                                    else:
                                        opus_audio = convert_result
                                        
                                    chunk_size = 4096
                                    for i in range(0, len(opus_audio), chunk_size):
                                        yield opus_audio[i:i + chunk_size]
                                else:
                                    chunk_size = 4096
                                    for i in range(0, len(full_audio), chunk_size):
                                        yield full_audio[i:i + chunk_size]
                                        
                    except httpx.RequestError as e:
                        raise HTTPException(status_code=502, detail=f"Custom TTS è¿æ¥å¤±è´¥: {str(e)}")

            # æ ¹æ®æµå¼æ¨¡å¼å’Œç›®æ ‡æ ¼å¼è®¾ç½®åª’ä½“ç±»å‹å’Œæ–‡ä»¶å
            if custom_streaming:
                # æµå¼æ¨¡å¼ï¼šå‡è®¾è¿”å›çš„æ ¼å¼ä¸ç›®æ ‡æ ¼å¼ä¸€è‡´
                if target_format == "opus":
                    media_type = "audio/ogg"
                    filename = f"tts_{index}.opus"
                else:
                    # é»˜è®¤å‡è®¾æ˜¯wavæ ¼å¼
                    media_type = "audio/wav"
                    filename = f"tts_{index}.wav"
            else:
                # éæµå¼æ¨¡å¼ï¼šä¿æŒåŸæœ‰é€»è¾‘
                if target_format == "opus":
                    media_type = "audio/ogg"
                    filename = f"tts_{index}.opus"
                else:
                    media_type = "audio/wav"
                    filename = f"tts_{index}.wav"

            return StreamingResponse(
                generate_audio(),
                media_type=media_type,
                headers={
                    "Content-Disposition": f"inline; filename={filename}",
                    "X-Audio-Index": str(index),
                    "X-Audio-Format": target_format
                }
            )

        elif tts_engine == 'GSV':
            # GSVç”Ÿæˆoggæ ¼å¼ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥ç›´æ¥ä½œä¸ºopusä½¿ç”¨
            audio_path = os.path.join(UPLOAD_FILES_DIR, tts_settings.get('gsvRefAudioPath', ''))
            if not os.path.exists(audio_path):
                audio_path = tts_settings.get('gsvRefAudioPath', '')

            gsv_params = {
                "text": text,
                "text_lang": tts_settings.get('gsvTextLang', 'zh'),
                "ref_audio_path": audio_path,
                "prompt_lang": tts_settings.get('gsvPromptLang', 'zh'),
                "prompt_text": tts_settings.get('gsvPromptText', ''),
                "speed_factor": tts_settings.get('gsvRate', 1.0),
                "sample_steps": tts_settings.get('gsvSample_steps', 4),
                "streaming_mode": True,
                "text_split_method": "cut0",
                "media_type": "ogg",
                "batch_size": 1,
                "seed": 42,
            }
            
            if mobile_optimized:
                gsv_params["speed_factor"] = min(gsv_params["speed_factor"] * 0.95, 1.1)
            
            gsvServer_list = tts_settings.get('gsvServer', 'http://127.0.0.1:9880').split('\n')
            gsvServer_list = [server for server in gsvServer_list if server.strip()]
            gsvServer = gsvServer_list[index % len(gsvServer_list)]
                
            async def generate_audio():
                safe_tts_url = sanitize_url(
                    input_url=gsvServer,
                    default_base="http://127.0.0.1:9880", # è¿™é‡Œå¡«ä½ ä»£ç é‡ŒåŸæœ¬çš„é»˜è®¤ TTS åœ°å€
                    endpoint="/tts"  # å› ä¸º TTS URL é€šå¸¸å·²ç»åŒ…å«äº†è·¯å¾„
                )
                timeout_config = httpx.Timeout(None, connect=10.0) 
                async with httpx.AsyncClient(timeout=timeout_config) as client:
                    try:
                        async with client.stream("POST", safe_tts_url, json=gsv_params) as response:
                            response.raise_for_status()
                            # ç›´æ¥æµå¼è¿”å›ï¼Œä¸ç®¡ç›®æ ‡æ ¼å¼ï¼ˆå‡è®¾GSVçš„oggå†…éƒ¨æ˜¯opusç¼–ç ï¼‰
                            async for chunk in response.aiter_bytes():
                                if chunk:
                                    yield chunk
                                
                    except httpx.HTTPStatusError as e:
                        error_detail = f"GSVæœåŠ¡é”™è¯¯: {e.response.status_code}"
                        raise HTTPException(status_code=502, detail=error_detail)
            
            # ç»Ÿä¸€ä½¿ç”¨oggåª’ä½“ç±»å‹ï¼Œä½†æ–‡ä»¶åæ ¹æ®ç›®æ ‡æ ¼å¼è°ƒæ•´
            media_type = "audio/ogg"
            filename = f"tts_{index}.opus" if target_format == "opus" else f"tts_{index}.ogg"
            
            return StreamingResponse(
                generate_audio(),
                media_type=media_type,
                headers={
                    "Content-Disposition": f"inline; filename={filename}",
                    "X-Audio-Index": str(index),
                    "X-Audio-Format": target_format
                }
            )
            
        elif tts_engine == 'openai':
            # OpenAI TTSå¤„ç†
            openai_config = {
                'api_key': tts_settings.get('api_key', ''),
                'model': tts_settings.get('model', 'tts-1'),
                'voice': tts_settings.get('openaiVoice', 'alloy'),
                'speed': tts_settings.get('openaiSpeed', 1.0),
                'base_url': tts_settings.get('base_url', 'https://api.openai.com/v1'),
                'prompt_text': tts_settings.get('gsvPromptText', ''),
                'ref_audio': tts_settings.get('gsvRefAudioPath', ''),
                'streaming': tts_settings.get('openaiStream', False)
            }
            
            if not openai_config['api_key']:
                raise HTTPException(status_code=400, detail="OpenAI APIå¯†é’¥æœªé…ç½®")
            
            speed = float(openai_config['speed'])
            if mobile_optimized:
                speed = min(speed * 0.95, 1.2)
            
            speed = max(0.25, min(4.0, speed))

            async def generate_audio():
                try:
                    client = AsyncOpenAI(
                        api_key=openai_config['api_key'],
                        base_url=openai_config['base_url']
                    )
                    
                    # æ ¹æ®ç›®æ ‡æ ¼å¼è®¾ç½®response_format
                    response_format = target_format if target_format in ['mp3', 'opus', 'aac', 'flac', 'wav', 'pcm'] else 'mp3'
                    
                    # å‡†å¤‡è¯·æ±‚å‚æ•°
                    request_params = {
                        'model': openai_config['model'],
                        'input': text,
                        'speed': speed,
                        'response_format': response_format
                    }
                    
                    # å¤„ç†å‚è€ƒéŸ³é¢‘
                    if openai_config['ref_audio']:
                        audio_file_path = os.path.join(UPLOAD_FILES_DIR, openai_config['ref_audio'])
                        with open(audio_file_path, "rb") as audio_file:
                            audio_data = audio_file.read()
                        audio_type = Path(audio_file_path).suffix[1:]
                        audio_base64 = base64.b64encode(audio_data).decode('utf-8')
                        audio_uri = f"data:audio/{audio_type};base64,{audio_base64}"
                        
                        request_params['voice'] = None
                        request_params['extra_body'] = {
                            "references": [{"text": openai_config['prompt_text'], "audio": audio_uri}]
                        }
                    else:
                        request_params['voice'] = openai_config['voice']
                    
                    # æ ¹æ®æµå¼è®¾ç½®é€‰æ‹©è°ƒç”¨æ–¹å¼
                    if openai_config['streaming']:
                        # æµå¼æ¨¡å¼ - çœŸæ­£çš„æµå¼ï¼Œæ— éœ€æ ¼å¼è½¬æ¢
                        async with client.audio.speech.with_streaming_response.create(**request_params) as response:
                            async for chunk in response.iter_bytes(chunk_size=4096):
                                yield chunk
                                await asyncio.sleep(0)
                    else:
                        # éæµå¼æ¨¡å¼
                        response = await client.audio.speech.create(**request_params)
                        content = await response.aread()
                        
                        # åˆ†å—è¿”å›
                        chunk_size = 4096
                        for i in range(0, len(content), chunk_size):
                            yield content[i:i + chunk_size]
                            await asyncio.sleep(0)
                                
                except Exception as e:
                    raise HTTPException(status_code=500, detail=f"OpenAI TTSé”™è¯¯: {str(e)}")
            
            # æ ¹æ®ç›®æ ‡æ ¼å¼è®¾ç½®åª’ä½“ç±»å‹å’Œæ–‡ä»¶å
            if target_format == "opus":
                media_type = "audio/ogg"  # opusé€šå¸¸ç”¨oggå®¹å™¨
                filename = f"tts_{index}.opus"
            elif target_format == "wav":
                media_type = "audio/wav"
                filename = f"tts_{index}.wav"
            elif target_format == "aac":
                media_type = "audio/aac"
                filename = f"tts_{index}.aac"
            elif target_format == "flac":
                media_type = "audio/flac"
                filename = f"tts_{index}.flac"
            else:  # mp3 æˆ–å…¶ä»–
                media_type = "audio/mpeg"
                filename = f"tts_{index}.mp3"
            
            return StreamingResponse(
                generate_audio(),
                media_type=media_type,
                headers={
                    "Content-Disposition": f"inline; filename={filename}",
                    "X-Audio-Index": str(index),
                    "X-Audio-Format": target_format
                }
            )
        elif tts_engine == 'systemtts':
            import subprocess
            import uuid
            # æ³¨æ„ï¼špyttsx3 ä¸è¦åœ¨å…¨å±€å¯¼å…¥ï¼Œé˜²æ­¢åœ¨ Mac ä¸Šå¹²æ‰°ä¸»çº¿ç¨‹

            # ==========================================
            # System TTS (Cross-Platform) å¼•æ“
            # ==========================================
            
            # 1. è·å–é…ç½®å‚æ•°
            system_voice_name = tts_settings.get('systemVoiceName', None)
            system_rate = tts_settings.get('systemRate', 200)
            
            # ç§»åŠ¨ç«¯ä¼˜åŒ–ï¼šé€‚å½“é™ä½è¯­é€Ÿ
            if mobile_optimized:
                system_rate = int(system_rate * 0.95)
            
            # 2. å®šä¹‰åŒæ­¥åˆæˆå‡½æ•° (å°†åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ)
            def sync_generate_wav(input_text: str, voice_name: str, rate: int, req_index: int) -> bytes:
                """
                è·¨å¹³å°åŒæ­¥åˆæˆï¼š
                - Windows/Linux: ä½¿ç”¨ pyttsx3
                - macOS: ä½¿ç”¨ç³»ç»ŸåŸç”Ÿ 'say' å‘½ä»¤ (é¿å¼€ Cocoa çº¿ç¨‹é™åˆ¶)
                """
                unique_suffix = uuid.uuid4().hex[:8]
                temp_file = f"temp_tts_{req_index}_{unique_suffix}.wav"
                # å‡è®¾ TOOL_TEMP_DIR æ˜¯ä½ å…¨å±€å®šä¹‰çš„ä¸´æ—¶ç›®å½•ï¼Œå¦‚æœæ²¡æœ‰è¯·æ”¹ä¸º "." æˆ– os.getcwd()
                temp_filename = os.path.join(TOOL_TEMP_DIR, temp_file)
                
                wav_data = b""
                current_os = platform.system()

                try:
                    # -------------------------------------------------
                    # åˆ†æ”¯ A: macOS ç³»ç»Ÿ (ä½¿ç”¨ subprocess è°ƒç”¨ say)
                    # -------------------------------------------------
                    if current_os == 'Darwin':
                        # --data-format=LEI16@22050 å¼ºåˆ¶è¾“å‡ºæ ‡å‡† WAV (16bit Little Endian, 22.05kHz)
                        cmd = ['say', '-o', temp_filename, '--data-format=LEI16@22050', input_text]
                        
                        if voice_name:
                            cmd.extend(['-v', voice_name])
                        
                        if rate:
                            # ç®€å•ä¼ é€’è¯­é€Ÿï¼Œè™½ç„¶ pyttsx3 å’Œ say çš„æ•°å€¼æ ‡å‡†ä¸åŒï¼Œä½†åœ¨åˆç†èŒƒå›´å†…éƒ½å¯ç”¨
                            cmd.extend(['-r', str(rate)])

                        # æ‰§è¡Œå‘½ä»¤
                        subprocess.run(cmd, check=True, stderr=subprocess.PIPE)

                    # -------------------------------------------------
                    # åˆ†æ”¯ B: Windows / Linux (ä½¿ç”¨ pyttsx3)
                    # -------------------------------------------------
                    else:
                        import pyttsx3
                        # åœ¨å­çº¿ç¨‹å†…åˆå§‹åŒ–ï¼Œéš”ç¦»ç¯å¢ƒ
                        engine = pyttsx3.init()
                        engine.setProperty('rate', rate)
                        
                        if voice_name:
                            voices = engine.getProperty('voices')
                            for voice in voices:
                                # æ¨¡ç³ŠåŒ¹é…åç§°æˆ–ç²¾ç¡®åŒ¹é…ID
                                if voice_name.lower() in voice.name.lower() or voice_name == voice.id:
                                    engine.setProperty('voice', voice.id)
                                    break
                        
                        # save_to_file æ˜¯é˜»å¡æ“ä½œï¼Œåœ¨ Windows ä¸Šå®‰å…¨
                        engine.save_to_file(input_text, temp_filename)
                        engine.runAndWait()

                    # -------------------------------------------------
                    # è¯»å–ç”Ÿæˆçš„éŸ³é¢‘
                    # -------------------------------------------------
                    if os.path.exists(temp_filename):
                        with open(temp_filename, 'rb') as f:
                            wav_data = f.read()
                    else:
                        raise Exception("TTSå¼•æ“æœªèƒ½ç”ŸæˆéŸ³é¢‘æ–‡ä»¶")

                except subprocess.CalledProcessError as e:
                    print(f"[SystemTTS-Mac] å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e.stderr.decode() if e.stderr else str(e)}")
                    raise Exception("macOS TTS ç”Ÿæˆå¤±è´¥")
                except Exception as e:
                    print(f"[SystemTTS] åˆæˆå‡ºé”™ ({current_os}): {str(e)}")
                    raise e
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(temp_filename):
                        try:
                            os.remove(temp_filename)
                        except:
                            pass
                
                return wav_data

            # 3. å¼‚æ­¥ç”Ÿæˆæµç¨‹
            async def generate_audio():
                try:
                    # å°†åŒæ­¥é˜»å¡æ“ä½œæ”¾å…¥çº¿ç¨‹æ± 
                    wav_content = await asyncio.to_thread(
                        sync_generate_wav, 
                        text, 
                        system_voice_name, 
                        system_rate, 
                        index
                    )
                    
                    if not wav_content:
                        raise HTTPException(status_code=500, detail="SystemTTS ç”Ÿæˆå†…å®¹ä¸ºç©º")

                    # æ ¼å¼è½¬æ¢é€»è¾‘ (WAV -> OPUS)
                    final_audio = wav_content
                    if target_format == "opus":
                        # ã€ä¿®å¤ç‚¹ 1ã€‘æ”¾å…¥çº¿ç¨‹æ±  + è§£åŒ…å…ƒç»„
                        convert_result = await asyncio.to_thread(convert_to_opus_simple, wav_content)
                        if isinstance(convert_result, tuple):
                            final_audio = convert_result[0] # å–å‡ºæ•°æ®éƒ¨åˆ†
                        else:
                            final_audio = convert_result
                    
                    # åˆ†å—è¿”å› (æ¨¡æ‹Ÿæµå¼)
                    chunk_size = 4096
                    for i in range(0, len(final_audio), chunk_size):
                        yield final_audio[i:i + chunk_size]
                        await asyncio.sleep(0) # è®©å‡ºæ§åˆ¶æƒ
                        
                except Exception as e:
                    raise HTTPException(status_code=500, detail=f"SystemTTS å¤„ç†å¤±è´¥: {str(e)}")
            # 4. è®¾ç½®å“åº”å¤´
            if target_format == "opus":
                media_type = "audio/ogg"
                filename = f"tts_{index}.opus"
            else:
                media_type = "audio/wav"
                filename = f"tts_{index}.wav"
            
            return StreamingResponse(
                generate_audio(),
                media_type=media_type,
                headers={
                    "Content-Disposition": f"inline; filename={filename}",
                    "X-Audio-Index": str(index),
                    "X-Audio-Format": target_format
                }
            )
        
        # ==========================================
        # Tetos ç»Ÿä¸€å¤„ç†é€»è¾‘ (Azure, Volc, Baidu, etc.)
        # ==========================================
        elif tts_engine in ['azure', 'volcengine', 'baidu', 'minimax', 'xunfei', 'fish', 'google']:
            import traceback # ç”¨äºæ‰“å°æŠ¥é”™å †æ ˆ
            import uuid
            # 1. å‡†å¤‡ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            unique_suffix = uuid.uuid4().hex[:8]
            os.makedirs(TOOL_TEMP_DIR, exist_ok=True)
            temp_filename = os.path.join(TOOL_TEMP_DIR, f"temp_tetos_{index}_{unique_suffix}.mp3")

            print(f"[DEBUG] å‡†å¤‡è°ƒç”¨ Tetos: å¼•æ“={tts_engine}, ä¸´æ—¶æ–‡ä»¶={temp_filename}")

            # 2. å®šä¹‰åŒæ­¥ç”Ÿæˆå‡½æ•° (å°†åœ¨çº¿ç¨‹æ± è¿è¡Œ)
            def run_tetos_sync():
                try:
                    speaker = None
                    
                    # === ç»Ÿä¸€è·å–éŸ³è‰² ===
                    # å¦‚æœå‰ç«¯ä¼ æ¥çš„ voice æ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œè®¾ä¸º Noneï¼Œå¦åˆ™ SDK å¯èƒ½æŠ¥é”™
                    selected_voice = tts_settings.get(f'{tts_engine}Voice', '')
                    if not selected_voice:
                        selected_voice = None
                        
                    print(f"[DEBUG] åˆå§‹åŒ– Speaker: {tts_engine}, éŸ³è‰²: {selected_voice}")

                    # === 1. Azure ===
                    if tts_engine == 'azure':
                        from tetos.azure import AzureSpeaker
                        speaker = AzureSpeaker(
                            speech_key=tts_settings.get('azureSpeechKey', ''),
                            speech_region=tts_settings.get('azureRegion', ''),
                            voice=selected_voice  # åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥
                        )
                    
                    # === 2. Volcengine (ç«å±±) ===
                    elif tts_engine == 'volcengine':
                        from tetos.volc import VolcSpeaker
                        speaker = VolcSpeaker(
                            access_key=tts_settings.get('volcAccessKey', ''),
                            secret_key=tts_settings.get('volcSecretKey', ''),
                            app_key=tts_settings.get('volcAppKey', ''),
                            voice=selected_voice  # åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥
                        )

                    # === 3. Baidu ===
                    elif tts_engine == 'baidu':
                        from tetos.baidu import BaiduSpeaker
                        speaker = BaiduSpeaker(
                            api_key=tts_settings.get('baiduApiKey', ''),
                            secret_key=tts_settings.get('baiduSecretKey', ''),
                            voice=selected_voice  # åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥
                        )

                    # === 4. Minimax ===
                    elif tts_engine == 'minimax':
                        from tetos.minimax import MinimaxSpeaker
                        speaker = MinimaxSpeaker(
                            api_key=tts_settings.get('minimaxApiKey', ''),
                            group_id=tts_settings.get('minimaxGroupId', ''),
                            voice=selected_voice  # åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥
                        )

                    # === 5. Xunfei (è®¯é£) ===
                    elif tts_engine == 'xunfei':
                        from tetos.xunfei import XunfeiSpeaker
                        speaker = XunfeiSpeaker(
                            app_id=tts_settings.get('xunfeiAppId', ''),
                            api_key=tts_settings.get('xunfeiApiKey', ''),
                            api_secret=tts_settings.get('xunfeiApiSecret', ''),
                            voice=selected_voice  # åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥
                        )
                    
                    # === 6. Fish Audio ===
                    elif tts_engine == 'fish':
                        from tetos.fish import FishSpeaker
                        speaker = FishSpeaker(
                            api_key=tts_settings.get('fishApiKey', ''),
                            voice=selected_voice  # åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥
                        )

                    # === 7. Google ===
                    elif tts_engine == 'google':
                        from tetos.google import GoogleSpeaker
                        # Google éœ€è¦å…ˆå¤„ç†é‰´æƒæ–‡ä»¶
                        sa_json = tts_settings.get('googleServiceAccount', '')
                        if sa_json:
                            import json
                            import tempfile
                            with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as tmp:
                                tmp.write(sa_json)
                                os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = tmp.name
                        
                        speaker = GoogleSpeaker(
                            voice=selected_voice # åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥
                        )

                    if not speaker:
                        raise Exception(f"æ— æ³•åˆå§‹åŒ– {tts_engine} Speaker (å¯¹è±¡ä¸ºç©º)")

                    # === æ‰§è¡Œåˆæˆ ===
                    # å› ä¸º voice å·²ç»åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥äº†ï¼Œè¿™é‡Œä¸å†ä¼  voice å‚æ•°
                    print(f"[DEBUG] å¼€å§‹åˆæˆæ–‡æœ¬ (é•¿åº¦: {len(text)})...")
                    speaker.say(text, temp_filename)
                    print(f"[DEBUG] åˆæˆå®Œæˆï¼Œæ–‡ä»¶å·²ç”Ÿæˆ: {temp_filename}")

                except Exception as e:
                    print(f"[ERROR] Tetos åˆæˆçº¿ç¨‹å†…éƒ¨æŠ¥é”™: {str(e)}")
                    raise e

            # 3. å¼‚æ­¥æ‰§è¡Œåˆæˆ
            try:
                await asyncio.to_thread(run_tetos_sync)
            except Exception as e:
                print(f"[ERROR] Tetos å¼‚æ­¥è°ƒç”¨å¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail=f"TTSåˆæˆå¤±è´¥: {str(e)}")

            # 4. è¯»å–æ–‡ä»¶å¹¶è¿”å›æµ
            if not os.path.exists(temp_filename):
                raise HTTPException(status_code=500, detail="åˆæˆæ–‡ä»¶æœªç”Ÿæˆ")

            async def generate_audio_from_file():
                try:
                    with open(temp_filename, "rb") as f:
                        file_data = f.read()
                    
                    if target_format == "opus":
                        # ã€ä¿®å¤ç‚¹ 4ã€‘æ”¾å…¥çº¿ç¨‹æ±  + è§£åŒ…å…ƒç»„
                        convert_result = await asyncio.to_thread(convert_to_opus_simple, file_data)
                        if isinstance(convert_result, tuple):
                            opus_data = convert_result[0]
                        else:
                            opus_data = convert_result
                            
                        chunk_size = 4096
                        for i in range(0, len(opus_data), chunk_size):
                            yield opus_data[i:i + chunk_size]
                    else:
                        chunk_size = 4096
                        for i in range(0, len(file_data), chunk_size):
                            yield file_data[i:i + chunk_size]
                except Exception as stream_e:
                     print(f"[ERROR] æµå¼è¯»å–/è½¬æ¢å¤±è´¥: {str(stream_e)}")
                finally:
                    if os.path.exists(temp_filename):
                        try:
                            os.remove(temp_filename)
                        except:
                            pass

            # è®¾ç½®å“åº”å¤´
            if target_format == "opus":
                media_type = "audio/ogg"
                filename = f"tts_{index}.opus"
            else:
                media_type = "audio/mpeg"
                filename = f"tts_{index}.mp3"

            return StreamingResponse(
                generate_audio_from_file(),
                media_type=media_type,
                headers={
                    "Content-Disposition": f"inline; filename={filename}",
                    "X-Audio-Index": str(index),
                    "X-Audio-Format": target_format
                }
            )

        raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„TTSå¼•æ“")
    
    except Exception as e:
        print(f"[ERROR] TTS åˆæˆå¤±è´¥: {str(e)}")
        return JSONResponse(status_code=500, content={"error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"})

@app.post("/tts/tetos/list_voices")
async def list_tetos_voices(request: Request):
    """
    é€šè¿‡ tetos è·å–éŸ³è‰²åˆ—è¡¨
    æµç¨‹: æ¥æ”¶é…ç½® -> å®ä¾‹åŒ– Speaker -> è°ƒç”¨ .list_voices()
    """
    try:
        data = await request.json()
        provider = data.get('provider', '').lower()
        config = data.get('config', {})  # ç”¨æˆ·å¡«å†™çš„é‰´æƒä¿¡æ¯

        if not provider:
            return JSONResponse(status_code=400, content={"error": "ç¼ºå°‘ 'provider' å‚æ•°"})

        # å®šä¹‰åŒæ­¥æ‰§è¡Œå‡½æ•°ï¼ˆåœ¨çº¿ç¨‹æ± è¿è¡Œï¼Œé¿å…é˜»å¡ï¼‰
        def _sync_fetch_voices():
            voices = []

            # ---------------------------
            # Azure TTS
            # ---------------------------
            if provider == 'azure':
                from tetos.azure import AzureSpeaker
                # å®ä¾‹åŒ–
                speaker = AzureSpeaker(
                    speech_key=config.get('speech_key') or config.get('api_key'),
                    speech_region=config.get('speech_region') or config.get('region')
                )
                # è·å–åˆ—è¡¨
                voices = speaker.list_voices()

            # ---------------------------
            # Volcengine (ç«å±±å¼•æ“)
            # ---------------------------
            elif provider == 'volcengine':
                from tetos.volc import VolcSpeaker
                speaker = VolcSpeaker(
                    access_key=config.get('access_key'),
                    secret_key=config.get('secret_key'),
                    app_key=config.get('app_key')
                )
                voices = speaker.list_voices()

            # ---------------------------
            # Baidu TTS
            # ---------------------------
            elif provider == 'baidu':
                from tetos.baidu import BaiduSpeaker
                speaker = BaiduSpeaker(
                    api_key=config.get('api_key'),
                    secret_key=config.get('secret_key')
                )
                voices = speaker.list_voices()

            # ---------------------------
            # Minimax TTS
            # ---------------------------
            elif provider == 'minimax':
                from tetos.minimax import MinimaxSpeaker
                speaker = MinimaxSpeaker(
                    api_key=config.get('api_key'),
                    group_id=config.get('group_id')
                )
                voices = speaker.list_voices()

            # ---------------------------
            # è®¯é£ (Xunfei)
            # ---------------------------
            elif provider == 'xunfei':
                from tetos.xunfei import XunfeiSpeaker
                speaker = XunfeiSpeaker(
                    app_id=config.get('app_id'),
                    api_key=config.get('api_key'),
                    api_secret=config.get('api_secret')
                )
                voices = speaker.list_voices()

            elif provider == 'fish':
                api_key = config.get('api_key')
                if not api_key:
                    raise ValueError("Fish Audio éœ€è¦é…ç½® API Key")

                # è¯·æ±‚ Fish Audio å®˜æ–¹ API
                # page_size è®¾ç½®ä¸º 30 ä»¥è·å–æ›´å¤šçƒ­é—¨éŸ³è‰²
                url = "https://api.fish.audio/model?page_size=30&page_number=1&sort_by=score"
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "User-Agent": "Mozilla/5.0" 
                }
                
                response = requests.get(url, headers=headers, timeout=60)
                response.raise_for_status() # æ£€æŸ¥ HTTP é”™è¯¯
                res_json = response.json()
                
                # è§£æè¿”å›çš„æ•°æ®ç»“æ„
                items = res_json.get("items", [])
                
                for item in items:
                    # å°† Fish Audio çš„æ•°æ®ç»“æ„è½¬æ¢ä¸ºå‰ç«¯é€šç”¨çš„ç»“æ„
                    # å‰ç«¯ getVoiceValue ä¼˜å…ˆæ‰¾ id
                    # å‰ç«¯ getVoiceLabel ä¼˜å…ˆæ‰¾ DisplayName æˆ– name
                    # å‰ç«¯ getVoiceDesc ä¼˜å…ˆæ‰¾ Locale
                    voices.append({
                        "id": item.get("_id"),            # å…³é”®ï¼šè¿™æ˜¯å®é™…çš„ voice ID
                        "name": item.get("title"),        # æ˜¾ç¤ºåç§°
                        "DisplayName": item.get("title"), # å…¼å®¹å­—æ®µ
                        "Locale": item.get("languages", ["Unknown"])[0] if item.get("languages") else "" # è¯­è¨€æ ‡ç­¾
                    })


            # ---------------------------
            # Google TTS
            # ---------------------------
            elif provider == 'google':
                from tetos.google import GoogleSpeaker
                # Google ç‰¹æ®Šå¤„ç†ï¼štetos ä¾èµ– GOOGLE_APPLICATION_CREDENTIALS ç¯å¢ƒå˜é‡
                # å¦‚æœ config ä¼ äº† service_account çš„ json å¯¹è±¡ï¼Œæˆ‘ä»¬éœ€è¦ä¸´æ—¶å†™å…¥æ–‡ä»¶
                
                service_account_data = config.get('service_account')
                temp_path = None
                
                try:
                    if service_account_data:
                        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                        with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as tmp:
                            if isinstance(service_account_data, dict):
                                json.dump(service_account_data, tmp)
                            else:
                                tmp.write(str(service_account_data))
                            temp_path = tmp.name
                        
                        # è®¾ç½®ç¯å¢ƒå˜é‡
                        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = temp_path
                    
                    # GoogleSpeaker åˆå§‹åŒ–é€šå¸¸ä¸éœ€è¦å‚æ•°ï¼Œå®ƒè‡ªå·±å»è¯»ç¯å¢ƒå˜é‡
                    speaker = GoogleSpeaker()
                    voices = speaker.list_voices()
                    
                finally:
                    # æ¸…ç†å·¥ä½œ
                    if temp_path:
                        if os.path.exists(temp_path):
                            os.remove(temp_path)
                        # å¦‚æœæ˜¯æˆ‘ä»¬è®¾ç½®çš„ç¯å¢ƒå˜é‡ï¼Œç”¨å®Œåˆ é™¤ï¼Œä»¥å…å½±å“å…¶ä»–è¯·æ±‚
                        if os.environ.get("GOOGLE_APPLICATION_CREDENTIALS") == temp_path:
                            del os.environ["GOOGLE_APPLICATION_CREDENTIALS"]

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ tetos æä¾›å•†: {provider}")

            return voices

        # ä½¿ç”¨ asyncio.to_thread æ”¾å…¥çº¿ç¨‹æ± æ‰§è¡Œï¼Œé˜²æ­¢é˜»å¡ FastAPI ä¸»å¾ªç¯
        voice_list = await asyncio.to_thread(_sync_fetch_voices)

        return JSONResponse(content={
            "status": "success",
            "provider": provider,
            "data": voice_list
        })

    except Exception as e:
        print(f"è·å– {provider} éŸ³è‰²åˆ—è¡¨å¤±è´¥: {e}")
        # æ•è·é‰´æƒå¤±è´¥ã€ç½‘ç»œé”™è¯¯ç­‰
        return JSONResponse(status_code=500, content={
            "status": "error", 
            "message": str(e),
            "detail": f"è·å– {provider} éŸ³è‰²åˆ—è¡¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¯†é’¥é…ç½®æ˜¯å¦æ­£ç¡®ã€‚"
        })

@app.get("/system/voices")
async def get_system_voices():
    """
    è·å–ç³»ç»Ÿå¯ç”¨çš„ pyttsx3 éŸ³è‰²åˆ—è¡¨ã€‚
    ä¼˜åŒ–ç‰ˆï¼š
    1. ä¼˜å…ˆå±•ç¤º Siri/Premium é«˜è´¨é‡éŸ³è‰²
    2. è‡ªåŠ¨ä» ID ä¸­è¡¥å…¨ç¼ºå¤±çš„è¯­è¨€æ ‡è¯†
    3. ä¸ºé«˜è´¨é‡éŸ³è‰²æ·»åŠ  [Siri] å‰ç¼€
    """
    import pyttsx3
    import sys
    import re

    def fetch_voices_sync():
        try:
            # 1. ä»ç„¶ä¿ç•™æ€ªè¯éŸ³è‰²é»‘åå• (è¿™äº›å£°éŸ³ç¡®å®æ²¡æ³•ç”¨)
            mac_novelty_voices = {
                'Albert', 'Bad News', 'Bahh', 'Bells', 'Boing', 'Bubbles', 'Cellos',
                'Deranged', 'Good News', 'Hysterical', 'Pipe Organ', 'Trinoids', 
                'Whisper', 'Zarvox', 'Organ'
            }

            engine = pyttsx3.init()
            voices = engine.getProperty('voices')
            
            processed_voices = []

            for v in voices:
                voice_name = v.name
                voice_id = str(v.id) # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                lower_id = voice_id.lower()

                # --- è¿‡æ»¤é€»è¾‘ ---
                if sys.platform == 'darwin':
                    if voice_name in mac_novelty_voices:
                        continue
                    
                    # ã€é‡è¦ä¿®æ”¹ã€‘ä¸è¦å†è¿‡æ»¤ 'siri' äº†ï¼
                    # æˆ‘ä»¬åªè¿‡æ»¤é‚£äº›å®Œå…¨æ— æ³•ä½¿ç”¨çš„ï¼ˆé€šå¸¸ id æå…¶ç®€çŸ­æˆ–æ˜¯æ— æ•ˆå¼•ç”¨ï¼‰
                    # ä½†ä¿ç•™åŒ…å« 'siri', 'premium', 'compact' çš„ ID

                # --- è¯­è¨€è§£æé€»è¾‘ (å¢å¼ºç‰ˆ) ---
                lang = "Unknown"
                
                # ä¼˜å…ˆå°è¯•ä» pyttsx3 å±æ€§è·å–
                if hasattr(v, 'languages') and v.languages:
                    raw_lang = v.languages[0] if isinstance(v.languages, list) else v.languages
                    if isinstance(raw_lang, bytes):
                        try:
                            lang = raw_lang.decode('utf-8', errors='ignore').replace('\x05', '')
                        except:
                            lang = str(raw_lang)
                    else:
                        lang = str(raw_lang)

                # ã€è¡¥å…¨é€»è¾‘ã€‘å¦‚æœå±æ€§é‡Œè¯»ä¸åˆ°è¯­è¨€ï¼Œå°è¯•ä» ID é‡Œæ­£åˆ™æå–
                # macOS çš„ ID é€šå¸¸é•¿è¿™æ ·: com.apple.speech.synthesis.voice.zh_CN.ting-ting.premium
                if lang == "Unknown" or lang == "":
                    # åŒ¹é… .zh_CN. æˆ– .en_US. è¿™ç§æ¨¡å¼
                    match = re.search(r'\.([a-z]{2}[_-][A-Z]{2})\.', voice_id)
                    if match:
                        lang = match.group(1).replace('_', '-') # ç»Ÿä¸€æ ¼å¼ä¸º zh-CN

                # --- åˆ¤æ–­æ˜¯å¦ä¸º Siri/é«˜è´¨é‡éŸ³è‰² ---
                # å…³é”®è¯ï¼šsiri, premium (é«˜å“è´¨), compact (å‹ç¼©çš„é«˜å“è´¨ï¼Œé€šå¸¸æ˜¯ç³»ç»Ÿé»˜è®¤ä¸‹è½½çš„)
                is_high_quality = False
                quality_tag = ""
                
                if any(k in lower_id for k in ['siri', 'premium', 'compact']):
                    is_high_quality = True
                    quality_tag = "[Siri/Premium] "
                
                # æœ‰äº›ç³»ç»Ÿç›´æ¥åœ¨åå­—é‡Œå°±å« "Siri Voice 1"
                if "siri" in voice_name.lower():
                    is_high_quality = True
                    quality_tag = "[Siri] "

                # ç»„è£…æ•°æ®
                processed_voices.append({
                    "id": voice_id,
                    "name": f"{quality_tag}{voice_name}", # åœ¨åå­—å‰åŠ ä¸Šæ ‡è¯†ï¼Œæ–¹ä¾¿å‰ç«¯å±•ç¤º
                    "original_name": voice_name,
                    "lang": lang,
                    "gender": getattr(v, 'gender', 'Unknown'),
                    "is_siri": is_high_quality # ç”¨äºæ’åºçš„æ ‡è®°
                })

            # --- æ’åºé€»è¾‘ ---
            # Python çš„ sort æ˜¯ç¨³å®šçš„ã€‚
            # key è§£é‡Š: (not x['is_siri']) -> True(1) æ’åé¢, False(0) æ’å‰é¢
            # æ‰€ä»¥ is_siri=True çš„ä¼šæ’åœ¨æœ€å‰é¢
            processed_voices.sort(key=lambda x: (not x['is_siri'], x['lang'], x['name']))

            return processed_voices
            
        except ImportError:
            print("é”™è¯¯: æœªæ‰¾åˆ° pyttsx3 é©±åŠ¨")
            return []
        except Exception as e:
            print(f"è·å–ç³»ç»ŸéŸ³è‰²é”™è¯¯: {str(e)}")
            return []

    try:
        available_voices = await asyncio.to_thread(fetch_voices_sync)
        return {
            "count": len(available_voices),
            "voices": available_voices
        }
    except Exception as e:
        from fastapi.responses import JSONResponse
        return JSONResponse(status_code=500, content={"error": str(e)})


# æ·»åŠ çŠ¶æ€å­˜å‚¨
mcp_status = {}
@app.post("/create_mcp")
async def create_mcp_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    mcp_id = data.get("mcpId")
    
    if not mcp_id:
        raise HTTPException(status_code=400, detail="Missing mcpId")
    
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
    background_tasks.add_task(process_mcp, mcp_id)
    
    return {"success": True, "message": "MCPæœåŠ¡å™¨åˆå§‹åŒ–å·²å¼€å§‹"}

@app.get("/mcp_status/{mcp_id}")
async def get_mcp_status(mcp_id: str):
    global mcp_client_list, mcp_status
    status = mcp_status.get(mcp_id, "not_found")
    if status == "ready":
        # ä¿è¯ _tools é‡Œéƒ½æ˜¯å¯åºåˆ—åŒ–çš„ dict / list / åŸºæœ¬ç±»å‹
        tools = await mcp_client_list[mcp_id].get_openai_functions(disable_tools=[])
        tools = json.dumps(mcp_client_list[mcp_id]._tools_list)
        return {"mcp_id": mcp_id, "status": status, "tools": tools}
    return {"mcp_id": mcp_id, "status": status, "tools": []}

async def process_mcp(mcp_id: str):
    """
    åˆå§‹åŒ–å•ä¸ª MCP æœåŠ¡å™¨ï¼Œå¸¦å¤±è´¥å›è°ƒåŒæ­¥ï¼Œæ— éœ€ sleepã€‚
    """
    global mcp_client_list, mcp_status

    # 1. åŒæ­¥åŸè¯­ï¼šäº‹ä»¶ + å¤±è´¥åŸå› 
    init_done = asyncio.Event()
    fail_reason: str | None = None

    async def on_failure(error_message: str):
        nonlocal fail_reason
        # ä»…ç¬¬ä¸€æ¬¡ç”Ÿæ•ˆ
        if fail_reason is not None:
            return
        fail_reason = error_message
        mcp_status[mcp_id] = f"failed: {error_message}"

        # å®¹é”™ï¼šåªæœ‰å®¢æˆ·ç«¯å·²åˆ›å»ºæ‰æ ‡è®° disabled
        if mcp_id in mcp_client_list:
            mcp_client_list[mcp_id].disabled = True
            await mcp_client_list[mcp_id].close()
            print(f"å…³é—­MCPæœåŠ¡å™¨: {mcp_id}")

        init_done.set()          # å”¤é†’ä¸»åç¨‹

    # 2. å¼€å§‹åˆå§‹åŒ–
    mcp_status[mcp_id] = "initializing"
    try:
        cur_settings = await load_settings()
        server_config = cur_settings["mcpServers"][mcp_id]

        mcp_client_list[mcp_id] = McpClient()
        init_task = asyncio.create_task(
            mcp_client_list[mcp_id].initialize(
                mcp_id,
                server_config,
                on_failure_callback=on_failure
            )
        )
        # 2.1 å…ˆç­‰åˆå§‹åŒ–æœ¬èº«ï¼ˆæœ€å¤š 6 ç§’ï¼‰
        await asyncio.wait_for(init_task, timeout=6)

        # 2.2 å†ç­‰çœ‹ on_failure ä¼šä¸ä¼šè¢«è§¦å‘ï¼ˆæœ€å¤š 5 ç§’ï¼‰
        try:
            await asyncio.wait_for(init_done.wait(), timeout=5)
        except asyncio.TimeoutError:
            # 5 ç§’å†…æ²¡æ”¶åˆ°å¤±è´¥å›è°ƒï¼Œè®¤ä¸ºæˆåŠŸ
            pass

        # 3. æœ€ç»ˆçŠ¶æ€åˆ¤å®š
        if fail_reason:
            # å›è°ƒé‡Œå·²ç»å…³è¿‡ clientï¼Œè¿™é‡Œåªéœ€ä¿è¯çŠ¶æ€ä¸€è‡´
            mcp_client_list[mcp_id].disabled = True
            return
        tool = []
        retry = 0 
        while tool == [] and retry < 10:
            try:
                tool = await mcp_client_list[mcp_id].get_openai_functions(disable_tools=[])
            except Exception as e:
                print(f"è·å–å·¥å…·å¤±è´¥: {str(e)}")
            finally:
                retry += 1
                await asyncio.sleep(0.5)
        mcp_status[mcp_id] = "ready"
        mcp_client_list[mcp_id].disabled = False

    except Exception as e:
        # ä»»ä½•å¼‚å¸¸ï¼ˆè¶…æ—¶ã€å´©æºƒï¼‰éƒ½èµ°è¿™é‡Œ
        mcp_status[mcp_id] = f"failed: {str(e)}"
        mcp_client_list[mcp_id].disabled = True
        await mcp_client_list[mcp_id].close()

    finally:
        # å¦‚æœä»»åŠ¡è¿˜æ´»ç€ï¼Œä¿é™©èµ·è§å–æ¶ˆæ‰
        if "init_task" in locals() and not init_task.done():
            init_task.cancel()
            try:
                await init_task
            except asyncio.CancelledError:
                pass

@app.delete("/remove_mcp")
async def remove_mcp_server(request: Request):
    global settings, mcp_client_list
    try:
        data = await request.json()
        server_name = data.get("serverName", "")

        if not server_name:
            raise HTTPException(status_code=400, detail="No server names provided")

        # ç§»é™¤æŒ‡å®šçš„MCPæœåŠ¡å™¨
        current_settings = await load_settings()
        if server_name in current_settings['mcpServers']:
            del current_settings['mcpServers'][server_name]
            await save_settings(current_settings)
            settings = current_settings

            # ä»mcp_client_listä¸­ç§»é™¤
            if server_name in mcp_client_list:
                mcp_client_list[server_name].disabled = True
                await mcp_client_list[server_name].close()
                del mcp_client_list[server_name]
                print(f"å…³é—­MCPæœåŠ¡å™¨: {server_name}")

            return JSONResponse({"success": True, "removed": server_name})
        else:
            raise HTTPException(status_code=404, detail="Server not found")
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON format")
    except Exception as e:
        logger.error(f"ç§»é™¤MCPæœåŠ¡å™¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/remove_memory")
async def remove_memory_endpoint(request: Request):
    data = await request.json()
    memory_id = data.get("memoryId")
    if memory_id:
        try:
            # åˆ é™¤MEMORY_CACHE_DIRç›®å½•ä¸‹çš„memory_idæ–‡ä»¶å¤¹
            memory_dir = os.path.join(MEMORY_CACHE_DIR, memory_id)
            shutil.rmtree(memory_dir)
            return JSONResponse({"success": True, "message": "Memory removed"})
        except Exception as e:
            return JSONResponse({"success": False, "message": str(e)})
    else:
        return JSONResponse({"success": False, "message": "No memoryId provided"})

@app.delete("/remove_agent")
async def remove_agent_endpoint(request: Request):
    data = await request.json()
    agent_id = data.get("agentId")
    if agent_id:
        try:
            # åˆ é™¤AGENT_CACHE_DIRç›®å½•ä¸‹çš„agent_idæ–‡ä»¶å¤¹
            agent_dir = os.path.join(AGENT_DIR, f"{agent_id}.json")
            shutil.rmtree(agent_dir)
            return JSONResponse({"success": True, "message": "Agent removed"})
        except Exception as e:
            return JSONResponse({"success": False, "message": str(e)})
    else:
        return JSONResponse({"success": False, "message": "No agentId provided"})

@app.post("/a2a")
async def initialize_a2a(request: Request):
    from python_a2a import A2AClient
    data = await request.json()
    try:
        client = A2AClient(data['url'])
        agent_card = client.agent_card.to_json()
        agent_card = json.loads(agent_card)
        return JSONResponse({
            **agent_card,
            "status": "ready",
            "enabled": True
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

@app.post("/start_HA")
async def start_HA(request: Request):
    data = await request.json()
    API_TOKEN = data['data']['api_key']
    ha_config = {
        "type": "sse",
        "url": data['data']['url'].rstrip('/') + "/mcp_server/sse",
        "headers": {"Authorization": f"Bearer {API_TOKEN}"}
    }

    global HA_client
    if HA_client is not None:
        # å·²åˆå§‹åŒ–è¿‡
        return JSONResponse({"status": "ready", "enabled": True})

    # ç”¨æ¥é€šçŸ¥â€œè¿æ¥å¤±è´¥â€çš„äº‹ä»¶
    conn_failed_event = asyncio.Event()
    failure_reason = None

    async def on_failure(error_message: str):
        nonlocal failure_reason
        failure_reason = error_message
        conn_failed_event.set()

    try:
        HA_client = McpClient()
        await HA_client.initialize("HA", ha_config, on_failure_callback=on_failure)

        # ç­‰ä¸€å°æ®µæ—¶é—´éªŒè¯è¿æ¥ç¡®å®æ´»äº†
        try:
            # 5 ç§’å†…å¦‚æœäº‹ä»¶è¢« setï¼Œè¯´æ˜è¿æ¥å¤±è´¥
            await asyncio.wait_for(conn_failed_event.wait(), timeout=5.0)
            # èµ°åˆ°è¿™é‡Œè¯´æ˜å¤±è´¥äº†
            raise RuntimeError(f"HA client connection failed: {failure_reason}")
        except asyncio.TimeoutError:
            # 2 ç§’æ— äº‹å‘ç”Ÿï¼Œè®¤ä¸ºè¿æ¥æˆåŠŸ
            pass

        return JSONResponse({"status": "ready", "enabled": True})

    except Exception as e:
        HA_client = None
        return JSONResponse(status_code=500, content={"error": str(e)})
    
@app.get("/stop_HA")
async def stop_HA():
    global HA_client
    try:
        if HA_client is not None:
            await HA_client.close()
            HA_client = None
            print(f"HA client stopped")
        return JSONResponse({
            "status": "stopped",
            "enabled": False
        })
    except Exception as e:
        HA_client = None
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

@app.post("/start_ChromeMCP")
async def start_ChromeMCP(request: Request):

    data = await request.json()

    chromeMCPSettings = data['data']

    if chromeMCPSettings.get('mcpName', 'browser-mcp') == 'browser-mcp':
        Chrome_config = {
            "type": "stdio",
            "command": "npx",
            "args": ["@browsermcp/mcp@latest"]
        }
    else:
        Chrome_config = {
            "type": "stdio",
            "command": "npx",
            "args": ["@playwright/mcp@latest"]
        }    

    global ChromeMCP_client
    if ChromeMCP_client is not None:
        # å·²åˆå§‹åŒ–è¿‡
        return JSONResponse({"status": "ready", "enabled": True})

    # ç”¨æ¥é€šçŸ¥â€œè¿æ¥å¤±è´¥â€çš„äº‹ä»¶
    conn_failed_event = asyncio.Event()
    failure_reason = None

    async def on_failure(error_message: str):
        nonlocal failure_reason
        failure_reason = error_message
        conn_failed_event.set()

    try:
        ChromeMCP_client = McpClient()
        await ChromeMCP_client.initialize("ChromeMCP", Chrome_config, on_failure_callback=on_failure)

        # ç­‰ä¸€å°æ®µæ—¶é—´éªŒè¯è¿æ¥ç¡®å®æ´»äº†
        try:
            # 5 ç§’å†…å¦‚æœäº‹ä»¶è¢« setï¼Œè¯´æ˜è¿æ¥å¤±è´¥
            await asyncio.wait_for(conn_failed_event.wait(), timeout=5.0)
            # èµ°åˆ°è¿™é‡Œè¯´æ˜å¤±è´¥äº†
            raise RuntimeError(f"ChromeMCP client connection failed: {failure_reason}")
        except asyncio.TimeoutError:
            # 2 ç§’æ— äº‹å‘ç”Ÿï¼Œè®¤ä¸ºè¿æ¥æˆåŠŸ
            pass

        return JSONResponse({"status": "ready", "enabled": True})
    except Exception as e:
        ChromeMCP_client = None
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.get("/stop_ChromeMCP")
async def stop_ChromeMCP():
    global ChromeMCP_client
    try:
        if ChromeMCP_client is not None:
            await ChromeMCP_client.close()
            ChromeMCP_client = None
            print(f"ChromeMCP client stopped")
        return JSONResponse({
            "status": "stopped",
            "enabled": False
        })
    except Exception as e:
        ChromeMCP_client = None
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )


@app.post("/start_sql")
async def start_sql(request: Request):
    data = await request.json()
    sql_args = []
    user = str(data['data'].get('user', '')).strip()
    password = str(data['data'].get('password', '')).strip()
    host = str(data['data'].get('host', '')).strip()
    port = str(data['data'].get('port', '')).strip()
    dbname = str(data['data'].get('dbname', '')).strip()
    dbpath = str(data['data'].get('dbpath', '')).strip()
    sql_url = ""
    if (data['data']['engine']=='sqlite'):
        sql_args = ["--from", "mcp-alchemy==2025.8.15.91819",
               "--refresh-package", "mcp-alchemy", "mcp-alchemy"]
        sql_url = f"sqlite:///{dbpath}"
        print(sql_url)
    elif (data['data']['engine']=='mysql'):
        sql_args = ["--from", "mcp-alchemy==2025.8.15.91819", "--with", "pymysql",
               "--refresh-package", "mcp-alchemy", "mcp-alchemy"]
        sql_url = f"mysql+pymysql://{user}:{password}@{host}:{port}/{dbname}"
    elif (data['data']['engine']=='postgres'):
        sql_args = ["--from", "mcp-alchemy==2025.8.15.91819", "--with", "psycopg2-binary",
               "--refresh-package", "mcp-alchemy", "mcp-alchemy"]
        sql_url = f"postgresql://{user}:{password}@{host}:{port}/{dbname}"
    elif (data['data']['engine']=='mssql'):
        sql_args = ["--from", "mcp-alchemy==2025.8.15.91819", "--with", "pymssql",
               "--refresh-package", "mcp-alchemy", "mcp-alchemy"]
        sql_url = f"mssql+pymssql://{user}:{password}@{host}:{port}/{dbname}"
    elif (data['data']['engine']=='oracle'):
        sql_args = ["--from", "mcp-alchemy==2025.8.15.91819", "--with", "oracledb",
               "--refresh-package", "mcp-alchemy", "mcp-alchemy"]
        sql_url = f"oracle+oracledb://{user}:{password}@{host}:{port}/{dbname}"

    sql_config = {
        "type": "stdio",
        "command": "uvx",
        "args": sql_args,
        "env": {
            "DB_URL": sql_url.strip(),
        }
    }

    global sql_client
    if sql_client is not None:
        # å·²åˆå§‹åŒ–è¿‡
        return JSONResponse({"status": "ready", "enabled": True})

    # ç”¨æ¥é€šçŸ¥â€œè¿æ¥å¤±è´¥â€çš„äº‹ä»¶
    conn_failed_event = asyncio.Event()
    failure_reason = None

    async def on_failure(error_message: str):
        nonlocal failure_reason
        failure_reason = error_message
        conn_failed_event.set()

    try:
        sql_client = McpClient()
        await sql_client.initialize("sqlMCP", sql_config, on_failure_callback=on_failure)

        # ç­‰ä¸€å°æ®µæ—¶é—´éªŒè¯è¿æ¥ç¡®å®æ´»äº†
        try:
            # 5 ç§’å†…å¦‚æœäº‹ä»¶è¢« setï¼Œè¯´æ˜è¿æ¥å¤±è´¥
            await asyncio.wait_for(conn_failed_event.wait(), timeout=5.0)
            # èµ°åˆ°è¿™é‡Œè¯´æ˜å¤±è´¥äº†
            raise RuntimeError(f"sqlMCP client connection failed: {failure_reason}")
        except asyncio.TimeoutError:
            # 2 ç§’æ— äº‹å‘ç”Ÿï¼Œè®¤ä¸ºè¿æ¥æˆåŠŸ
            pass

        return JSONResponse({"status": "ready", "enabled": True})
    except Exception as e:
        sql_client = None
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.get("/stop_sql")
async def stop_sql():
    global sql_client
    try:
        if sql_client is not None:
            await sql_client.close()
            sql_client = None
            print(f"sqlMCP client stopped")
        return JSONResponse({
            "status": "stopped",
            "enabled": False
        })
    except Exception as e:
        sql_client = None
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

# åœ¨ç°æœ‰è·¯ç”±ä¹‹åæ·»åŠ healthè·¯ç”±
@app.get("/health")
async def health_check():
    return {"status": "ok"}


@app.post("/load_file")
async def load_file_endpoint(request: Request, files: List[UploadFile] = File(None)):
    fastapi_base_url = str(request.base_url)
    logger.info(f"Received request with content type: {request.headers.get('Content-Type')}")
    file_links = []
    textFiles = []
    imageFiles = []
    content_type = request.headers.get('Content-Type', '')
    try:
        if 'multipart/form-data' in content_type:
            # å¤„ç†æµè§ˆå™¨ä¸Šä¼ çš„æ–‡ä»¶
            if not files:
                raise HTTPException(status_code=400, detail="No files provided")
            
            for file in files:
                file_extension = os.path.splitext(file.filename)[1]
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)
                
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                with open(destination, "wb") as buffer:
                    content = await file.read()
                    buffer.write(content)
                
                file_link = {
                    "path": f"{fastapi_base_url}uploaded_files/{unique_filename}",
                    "name": file.filename
                }
                file_links.append(file_link)
                file_meta = {
                    "unique_filename": unique_filename,
                    "original_filename": file.filename,
                }
                # file_extensionç§»é™¤ç‚¹å·
                file_extension = file_extension[1:]
                if file_extension in ALLOWED_EXTENSIONS:
                    textFiles.append(file_meta)
                elif file_extension in ALLOWED_IMAGE_EXTENSIONS:
                    imageFiles.append(file_meta)
        elif 'application/json' in content_type:
            # å¤„ç†Electronå‘é€çš„JSONæ–‡ä»¶è·¯å¾„
            data = await request.json()
            logger.info(f"Processing JSON data: {data}")
            
            for file_info in data.get("files", []):
                file_path = file_info.get("path")
                file_name = file_info.get("name", os.path.basename(file_path))
                
                if not os.path.isfile(file_path):
                    logger.error(f"File not found: {file_path}")
                    continue
                
                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                file_extension = os.path.splitext(file_name)[1]
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)
                
                # å¤åˆ¶æ–‡ä»¶åˆ°ä¸Šä¼ ç›®å½•
                with open(file_path, "rb") as src, open(destination, "wb") as dst:
                    dst.write(src.read())
                
                file_link = {
                    "path": f"{fastapi_base_url}uploaded_files/{unique_filename}",
                    "name": file_name
                }
                file_links.append(file_link)
                file_meta = {
                    "unique_filename": unique_filename,
                    "original_filename": file_name,
                }
                # file_extensionç§»é™¤ç‚¹å·
                file_extension = file_extension[1:]
                if file_extension in ALLOWED_EXTENSIONS:
                    textFiles.append(file_meta)
                elif file_extension in ALLOWED_IMAGE_EXTENSIONS:
                    imageFiles.append(file_meta)
        else:
            raise HTTPException(status_code=400, detail="Unsupported Content-Type")
        return JSONResponse(content={"success": True, "fileLinks": file_links , "textFiles": textFiles, "imageFiles": imageFiles})
    
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/delete_file")
async def delete_file_endpoint(request: Request):
    data = await request.json()
    file_name = data.get("fileName")
    file_path = os.path.join(UPLOAD_FILES_DIR, file_name)
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            return JSONResponse(content={"success": True})
        else:
            return JSONResponse(content={"success": False, "message": "File not found"})
    except Exception as e:
        return JSONResponse(content={"success": False, "message": str(e)})

class FileNames(BaseModel):
    fileNames: List[str]

@app.delete("/delete_files")
async def delete_files_endpoint(req: FileNames):
    success_files = []
    errors = []
    for name in req.fileNames:
        path = os.path.join(UPLOAD_FILES_DIR, name)
        try:
            if os.path.exists(path):
                os.remove(path)
                success_files.append(name)
            else:
                errors.append(f"{name} not found")
        except Exception as e:
            errors.append(f"{name}: {str(e)}")

    return JSONResponse(content={
        "success": len(success_files) > 0,   # åªè¦æœ‰æˆåŠŸå°±ç®—æˆåŠŸ
        "successFiles": success_files,
        "errors": errors
    })

ALLOWED_AUDIO_EXTENSIONS = ['wav', 'mp3', 'ogg', 'flac', 'aac']

@app.post("/upload_gsv_ref_audio")
async def upload_gsv_ref_audio(
    request: Request,
    file: UploadFile = File(...),
):
    fastapi_base_url = str(request.base_url)
    
    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    file_extension = file.filename.split('.')[-1].lower()
    if file_extension not in ALLOWED_AUDIO_EXTENSIONS:
        return JSONResponse(
            status_code=400,
            content={"success": False, "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}"}
        )
    
    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    unique_filename = f"{uuid.uuid4()}.{file_extension}"
    destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)
    
    try:
        # ä¿å­˜æ–‡ä»¶
        with open(destination, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # æ„å»ºå“åº”
        file_link = f"{fastapi_base_url}uploaded_files/{unique_filename}"
        
        return JSONResponse(content={
            "success": True,
            "message": "å‚è€ƒéŸ³é¢‘ä¸Šä¼ æˆåŠŸ",
            "file": {
                "path": file_link,
                "name": file.filename,
                "unique_filename": unique_filename
            }
        })
    
    except Exception as e:
        logger.error(f"å‚è€ƒéŸ³é¢‘ä¸Šä¼ å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}"}
        )

@app.delete("/delete_audio/{filename}")
async def delete_audio(filename: str):
    try:
        file_path = os.path.join(UPLOAD_FILES_DIR, filename)
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åæ˜¯UUIDæ ¼å¼ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»
        if not re.match(r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\.\w+$", filename):
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "Invalid filename"}
            )
        
        if os.path.exists(file_path):
            os.remove(file_path)
            return JSONResponse(content={
                "success": True,
                "message": "éŸ³é¢‘æ–‡ä»¶å·²åˆ é™¤"
            })
        else:
            return JSONResponse(
                status_code=404,
                content={"success": False, "message": "æ–‡ä»¶ä¸å­˜åœ¨"}
            )
            
    except Exception as e:
        logger.error(f"åˆ é™¤éŸ³é¢‘å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"åˆ é™¤å¤±è´¥: {str(e)}"}
        )

# å…è®¸çš„VRMæ–‡ä»¶æ‰©å±•å
ALLOWED_VRM_EXTENSIONS = {'vrm'}

@app.post("/upload_vrm_model")
async def upload_vrm_model(
    request: Request,
    file: UploadFile = File(...),
    display_name: str = Form(...)
):
    fastapi_base_url = str(request.base_url)
    
    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    file_extension = file.filename.split('.')[-1].lower()
    if file_extension not in ALLOWED_VRM_EXTENSIONS:
        return JSONResponse(
            status_code=400,
            content={"success": False, "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}ï¼Œåªæ”¯æŒ.vrmæ–‡ä»¶"}
        )
    
    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    unique_filename = f"{uuid.uuid4()}.{file_extension}"
    destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)
    
    try:
        # ä¿å­˜æ–‡ä»¶
        with open(destination, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # æ„å»ºå“åº”
        file_link = f"{fastapi_base_url}uploaded_files/{unique_filename}"
        
        return JSONResponse(content={
            "success": True,
            "message": "VRMæ¨¡å‹ä¸Šä¼ æˆåŠŸ",
            "file": {
                "path": file_link,
                "display_name": display_name,
                "original_name": file.filename,
                "unique_filename": unique_filename
            }
        })
    
    except Exception as e:
        logger.error(f"VRMæ¨¡å‹ä¸Šä¼ å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}"}
        )

@app.get("/get_default_vrm_models")
async def get_default_vrm_models(request: Request):
    try:
        fastapi_base_url = str(request.base_url)
        models = []
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(DEFAULT_VRM_DIR):
            os.makedirs(DEFAULT_VRM_DIR, exist_ok=True)
            return JSONResponse(content={
                "success": True,
                "models": []
            })
        
        # æ‰«æé»˜è®¤VRMç›®å½•ä¸­çš„æ‰€æœ‰.vrmæ–‡ä»¶
        vrm_files = glob.glob(os.path.join(DEFAULT_VRM_DIR, "*.vrm"))
        
        for vrm_file in vrm_files:
            file_name = os.path.basename(vrm_file)
            # ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºæ˜¾ç¤ºåç§°
            display_name = os.path.splitext(file_name)[0]
            
            # æ„å»ºæ–‡ä»¶è®¿é—®URL
            file_url = f"{fastapi_base_url}vrm/{file_name}"
            
            models.append({
                "id": os.path.splitext(file_name)[0].lower(),  # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºID
                "name": display_name,
                "path": file_url,
                "type": "default"
            })
        
        # æŒ‰åç§°æ’åº
        models.sort(key=lambda x: x['name'])
        print("models:",models)
        return JSONResponse(content={
            "success": True,
            "models": models
        })
        
    except Exception as e:
        logger.error(f"è·å–é»˜è®¤VRMæ¨¡å‹å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"è·å–é»˜è®¤æ¨¡å‹å¤±è´¥: {str(e)}"}
        )

# ä¿®æ”¹åˆ é™¤VRMæ¨¡å‹çš„æ¥å£ï¼Œæ·»åŠ å®‰å…¨æ£€æŸ¥
@app.delete("/delete_vrm_model/{filename}")
async def delete_vrm_model(filename: str):
    try:
        # ç¡®ä¿åªèƒ½åˆ é™¤ä¸Šä¼ ç›®å½•ä¸­çš„æ–‡ä»¶ï¼Œä¸èƒ½åˆ é™¤é»˜è®¤æ¨¡å‹
        file_path = os.path.join(UPLOAD_FILES_DIR, filename)
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åæ˜¯UUIDæ ¼å¼ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»
        if not re.match(r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\.vrm$", filename):
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "Invalid filename"}
            )
        
        # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶è·¯å¾„åœ¨ä¸Šä¼ ç›®å½•ä¸­ï¼Œé˜²æ­¢åˆ é™¤é»˜è®¤æ¨¡å‹
        if not file_path.startswith(os.path.abspath(UPLOAD_FILES_DIR)):
            return JSONResponse(
                status_code=403,
                content={"success": False, "message": "Cannot delete default models"}
            )
        
        if os.path.exists(file_path):
            os.remove(file_path)
            return JSONResponse(content={
                "success": True,
                "message": "VRMæ¨¡å‹æ–‡ä»¶å·²åˆ é™¤"
            })
        else:
            return JSONResponse(
                status_code=404,
                content={"success": False, "message": "æ–‡ä»¶ä¸å­˜åœ¨"}
            )
            
    except Exception as e:
        logger.error(f"åˆ é™¤VRMæ¨¡å‹å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"åˆ é™¤å¤±è´¥: {str(e)}"}
        )

ALLOWED_VRMA_EXTENSIONS = {"vrma"}

animation_dir = os.path.join(DEFAULT_VRM_DIR, "animations")

def make_file_url(request: Request, file_path: str) -> str:
    """å°†æœ¬åœ°æ–‡ä»¶è·¯å¾„è½¬æˆå¯¹å¤–å¯è®¿é—®çš„ URL"""
    return str(request.base_url) + file_path.lstrip("/")


def scan_motion_files(directory: str, allowed_ext: set) -> List[dict]:
    """
    æ‰«ææŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰ç¬¦åˆæ‰©å±•åçš„æ–‡ä»¶ï¼Œè¿”å›åˆ—è¡¨ï¼š
    [
      {
        "id": "æ–‡ä»¶å(ä¸å«æ‰©å±•å)",
        "name": "æ–‡ä»¶å(ä¸å«æ‰©å±•å)",
        "path": "å¯¹å¤–å¯è®¿é—®çš„å®Œæ•´ URL",
        "type": "default" | "user"
      }
    ]
    """
    files = []
    if not os.path.exists(directory):
        return files

    for f in os.listdir(directory):
        if f.lower().endswith(tuple(allowed_ext)):
            file_id = Path(f).stem
            file_path = os.path.join(directory, f)
            # æ³¨æ„ï¼šè¿™é‡Œç»Ÿä¸€è¿”å›ç›¸å¯¹è·¯å¾„ï¼Œåé¢å†ç»„è£…æˆ URL
            files.append({
                "id": file_id,
                "name": file_id,
                "path": file_path,
                "type": "default" if directory == animation_dir else "user"
            })
    # æŒ‰æ–‡ä»¶åæ’åº
    files.sort(key=lambda x: x["name"])
    return files

@app.get("/get_default_vrma_motions")
async def get_default_vrma_motions(request: Request):
    try:
        motions = scan_motion_files(animation_dir, ALLOWED_VRMA_EXTENSIONS)

        # æŠŠç£ç›˜è·¯å¾„è½¬æˆ URL
        for m in motions:
            file_name = os.path.basename(m["path"])
            m["path"] = str(request.base_url) + f"vrm/animations/{file_name}"

        return {"success": True, "motions": motions}

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"è·å–é»˜è®¤åŠ¨ä½œå¤±è´¥: {str(e)}"}
        )


@app.get("/get_user_vrma_motions")
async def get_user_vrma_motions(request: Request):
    try:
        motions = scan_motion_files(UPLOAD_FILES_DIR, ALLOWED_VRMA_EXTENSIONS)

        # æŠŠç£ç›˜è·¯å¾„è½¬æˆ URL
        for m in motions:
            file_name = os.path.basename(m["path"])
            m["path"] = str(request.base_url) + f"uploaded_files/{file_name}"

        return {"success": True, "motions": motions}

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"è·å–ç”¨æˆ·åŠ¨ä½œå¤±è´¥: {str(e)}"}
        )


@app.post("/upload_vrma_motion")
async def upload_vrma_motion(
    request: Request,
    file: UploadFile = File(...),
    display_name: str = Form(...)
):
    # æ£€æŸ¥æ‰©å±•å
    file_extension = Path(file.filename).suffix.lower().lstrip(".")
    if file_extension not in ALLOWED_VRMA_EXTENSIONS:
        return JSONResponse(
            status_code=400,
            content={"success": False, "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}"}
        )

    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    unique_filename = f"{uuid.uuid4()}.vrma"
    destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)

    try:
        # ä¿å­˜æ–‡ä»¶
        os.makedirs(UPLOAD_FILES_DIR, exist_ok=True)
        with open(destination, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        # æ„å»ºè¿”å›æ•°æ®
        file_url = make_file_url(request, f"uploaded_files/{unique_filename}")

        return JSONResponse(content={
            "success": True,
            "message": "åŠ¨ä½œä¸Šä¼ æˆåŠŸ",
            "file": {
                "unique_filename": unique_filename,
                "display_name": display_name,
                "path": file_url
            }
        })

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}"}
        )


@app.delete("/delete_vrma_motion/{filename}")
async def delete_vrma_motion(filename: str):
    try:
        # åªå…è®¸åˆ é™¤ UPLOAD_FILES_DIR ä¸­çš„æ–‡ä»¶
        if not re.match(r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\.vrma$", filename):
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "Invalid filename"}
            )

        file_path = os.path.join(UPLOAD_FILES_DIR, filename)
        abs_upload = os.path.abspath(UPLOAD_FILES_DIR)
        abs_file = os.path.abspath(file_path)

        if not abs_file.startswith(abs_upload):
            return JSONResponse(
                status_code=403,
                content={"success": False, "message": "ç¦æ­¢åˆ é™¤ç³»ç»Ÿæ–‡ä»¶"}
            )

        if os.path.exists(file_path):
            os.remove(file_path)
            return {"success": True, "message": "åŠ¨ä½œæ–‡ä»¶å·²åˆ é™¤"}
        else:
            return JSONResponse(
                status_code=404,
                content={"success": False, "message": "æ–‡ä»¶ä¸å­˜åœ¨"}
            )

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"åˆ é™¤å¤±è´¥: {str(e)}"}
        )

# -------------- GAUSS åœºæ™¯ç›¸å…³ --------------
GAUSS_DIR     = os.path.join(DEFAULT_VRM_DIR, "scene")       # é»˜è®¤åœºæ™¯ç›®å½•
ALLOWED_GAUSS = {"ply", "spz", "splat", "ksplat", "sog"}     # spark æ”¯æŒçš„æ‰©å±•å

@app.post("/upload_gauss_scene")
async def upload_gauss_scene(
    request: Request,
    file: UploadFile = File(...),
    display_name: str = Form(...)
):
    ext = Path(file.filename).suffix.lower().lstrip(".")
    if ext not in ALLOWED_GAUSS:
        return JSONResponse(status_code=400, content={
            "success": False,
            "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}"
        })
    unique = f"{uuid.uuid4()}.{ext}"
    destination = os.path.join(UPLOAD_FILES_DIR, unique)
    try:
        os.makedirs(UPLOAD_FILES_DIR, exist_ok=True)
        with open(destination, "wb") as f:
            f.write(await file.read())
        url = str(request.base_url) + f"uploaded_files/{unique}"
        return JSONResponse(content={
            "success": True,
            "file": {
                "unique_filename": unique,
                "display_name": display_name,
                "path": url
            }
        })
    except Exception as e:
        return JSONResponse(status_code=500, content={"success": False, "message": str(e)})

@app.get("/get_default_gauss_scenes")
async def get_default_gauss_scenes(request: Request):
    try:
        os.makedirs(GAUSS_DIR, exist_ok=True)
        scenes = []
        for f in os.listdir(GAUSS_DIR):
            ext = Path(f).suffix.lower().lstrip(".")
            if ext in ALLOWED_GAUSS:
                scenes.append({
                    "id":   Path(f).stem,
                    "name": Path(f).stem,
                    "path": str(request.base_url) + f"vrm/scene/{f}",
                    "type": "default"
                })
        scenes.sort(key=lambda x: x["name"])
        return {"success": True, "scenes": scenes}
    except Exception as e:
        return JSONResponse(status_code=500, content={"success": False, "message": str(e)})

@app.get("/get_user_gauss_scenes")
async def get_user_gauss_scenes(request: Request):
    try:
        scenes = []
        for f in os.listdir(UPLOAD_FILES_DIR):
            ext = Path(f).suffix.lower().lstrip(".")
            if ext in ALLOWED_GAUSS:
                scenes.append({
                    "id":   Path(f).stem,
                    "name": Path(f).stem,
                    "path": str(request.base_url) + f"uploaded_files/{f}",
                    "type": "user"
                })
        scenes.sort(key=lambda x: x["name"])
        return {"success": True, "scenes": scenes}
    except Exception as e:
        return JSONResponse(status_code=500, content={"success": False, "message": str(e)})

@app.delete("/delete_gauss_scene/{filename}")
async def delete_gauss_scene(filename: str):
    if not re.match(r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\.(ply|spz|splat|ksplat|sog)$", filename):
        return JSONResponse(status_code=400, content={"success": False, "message": "Invalid filename"})
    file_path = os.path.join(UPLOAD_FILES_DIR, filename)
    if os.path.exists(file_path):
        os.remove(file_path)
        return {"success": True, "message": "åœºæ™¯å·²åˆ é™¤"}
    return JSONResponse(status_code=404, content={"success": False, "message": "æ–‡ä»¶ä¸å­˜åœ¨"})


@app.get("/update_storage")
async def update_storage_endpoint(request: Request):
    settings = await load_settings()
    textFiles = settings.get("textFiles") or []
    imageFiles = settings.get("imageFiles") or []
    videoFiles = settings.get("videoFiles") or []
    # æ£€æŸ¥UPLOAD_FILES_DIRç›®å½•ä¸­çš„æ–‡ä»¶ï¼Œæ ¹æ®ALLOWED_EXTENSIONSã€ALLOWED_IMAGE_EXTENSIONSã€ALLOWED_VIDEO_EXTENSIONSåˆ†ç±»ï¼Œå¦‚æœä¸å­˜åœ¨äºtextFilesã€imageFilesã€videoFilesä¸­åˆ™æ·»åŠ è¿›å»
    # ä¸‰ä¸ªåˆ—è¡¨çš„å…ƒç´ æ˜¯å­—å…¸ï¼ŒåŒ…å«"unique_filename"å’Œ"original_filename"ä¸¤ä¸ªé”®
    
    for file in os.listdir(UPLOAD_FILES_DIR):
        file_path = os.path.join(UPLOAD_FILES_DIR, file)
        if os.path.isfile(file_path):
            file_extension = os.path.splitext(file)[1][1:]
            if file_extension in ALLOWED_EXTENSIONS:
                if file not in [item["unique_filename"] for item in textFiles]:
                    textFiles.append({"unique_filename": file, "original_filename": file})
            elif file_extension in ALLOWED_IMAGE_EXTENSIONS:
                if file not in [item["unique_filename"] for item in imageFiles]:
                    imageFiles.append({"unique_filename": file, "original_filename": file})
            elif file_extension in ALLOWED_VIDEO_EXTENSIONS:
                if file not in [item["unique_filename"] for item in videoFiles]:
                    videoFiles.append({"unique_filename": file, "original_filename": file})

    # å‘ç»™å‰ç«¯
    return JSONResponse(content={"textFiles": textFiles, "imageFiles": imageFiles, "videoFiles": videoFiles})

@app.get("/get_file_content")
async def get_file_content_endpoint(file_url: str):
    file_path = os.path.join(UPLOAD_FILES_DIR, file_url)
    content = await get_file_content(file_path)
    return JSONResponse(content={"content": content})

@app.post("/create_kb")
async def create_kb_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    kb_id = data.get("kbId")
    
    if not kb_id:
        raise HTTPException(status_code=400, detail="Missing kbId")
    
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
    background_tasks.add_task(process_kb, kb_id)
    
    return {"success": True, "message": "çŸ¥è¯†åº“å¤„ç†å·²å¼€å§‹ï¼Œè¯·ç¨åæŸ¥è¯¢çŠ¶æ€"}

@app.delete("/remove_kb")
async def remove_kb_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    kb_id = data.get("kbId")

    if not kb_id:
        raise HTTPException(status_code=400, detail="Missing kbId")
    try:
        background_tasks.add_task(remove_kb, kb_id)
    except Exception as e:
        return {"success": False, "message": str(e)}
    return {"success": True, "message": "çŸ¥è¯†åº“å·²åˆ é™¤"}

# åˆ é™¤çŸ¥è¯†åº“
async def remove_kb(kb_id):
    # åˆ é™¤KB_DIR/kb_idç›®å½•
    kb_dir = os.path.join(KB_DIR, str(kb_id))
    if os.path.exists(kb_dir):
        shutil.rmtree(kb_dir)
    else:
        print(f"KB directory {kb_dir} does not exist.")
    return

# æ·»åŠ çŠ¶æ€å­˜å‚¨
kb_status = {}
@app.get("/kb_status/{kb_id}")
async def get_kb_status(kb_id):
    status = kb_status.get(kb_id, "not_found")
    print (f"kb_status: {kb_id} - {status}")
    return {"kb_id": kb_id, "status": status}

# ä¿®æ”¹ process_kb
async def process_kb(kb_id):
    kb_status[kb_id] = "processing"
    try:
        from py.know_base import process_knowledge_base
        await process_knowledge_base(kb_id)
        kb_status[kb_id] = "completed"
    except Exception as e:
        kb_status[kb_id] = f"failed: {str(e)}"

@app.post("/create_sticker_pack")
async def create_sticker_pack(
    request: Request,
    files: List[UploadFile] = File(..., description="è¡¨æƒ…æ–‡ä»¶åˆ—è¡¨"),
    pack_name: str = Form(..., description="è¡¨æƒ…åŒ…åç§°"),
    descriptions: List[str] = Form(..., description="è¡¨æƒ…æè¿°åˆ—è¡¨")
):
    """
    åˆ›å»ºæ–°è¡¨æƒ…åŒ…
    - files: ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
    - pack_name: è¡¨æƒ…åŒ…åç§°
    - descriptions: æ¯ä¸ªè¡¨æƒ…çš„æè¿°åˆ—è¡¨
    """
    fastapi_base_url = str(request.base_url)
    imageFiles = []
    stickers_data = []
    
    try:
        # éªŒè¯è¾“å…¥æ•°æ®
        if not pack_name:
            raise HTTPException(status_code=400, detail="è¡¨æƒ…åŒ…åç§°ä¸èƒ½ä¸ºç©º")
        if len(files) == 0:
            raise HTTPException(status_code=400, detail="è‡³å°‘éœ€è¦ä¸Šä¼ ä¸€ä¸ªè¡¨æƒ…")
        if len(descriptions) != len(files):
            raise HTTPException(
                status_code=400, 
                detail=f"æè¿°æ•°é‡({len(descriptions)})ä¸æ–‡ä»¶æ•°é‡({len(files)})ä¸åŒ¹é…"
            )

        # å¤„ç†ä¸Šä¼ çš„è¡¨æƒ…æ–‡ä»¶
        for idx, file in enumerate(files):
            # è·å–æ–‡ä»¶æ‰©å±•å
            file_extension = os.path.splitext(file.filename)[1].lower()
            
            # éªŒè¯æ–‡ä»¶ç±»å‹
            if file_extension not in ['.png', '.jpg', '.jpeg', '.gif', '.webp']:
                raise HTTPException(
                    status_code=400, 
                    detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}"
                )
            
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            unique_filename = f"{uuid.uuid4()}{file_extension}"
            destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)

            # ä¿å­˜æ–‡ä»¶
            with open(destination, "wb") as buffer:
                content = await file.read()
                buffer.write(content)

            # æ„å»ºè¿”å›æ•°æ®
            imageFiles.append({
                "unique_filename": unique_filename,
                "original_filename": file.filename,
            })
            
            # è·å–å¯¹åº”çš„æè¿°ï¼ˆå¤„ç†å¯èƒ½çš„ç´¢å¼•è¶Šç•Œï¼‰
            description = descriptions[idx] if idx < len(descriptions) else ""

            # æ„å»ºè¡¨æƒ…æ•°æ®
            stickers_data.append({
                "unique_filename": unique_filename,
                "original_filename": file.filename,
                "url": f"{fastapi_base_url}uploaded_files/{unique_filename}",
                "description": description
            })

        # åˆ›å»ºè¡¨æƒ…åŒ…IDï¼ˆå¯æ›¿æ¢ä¸ºæ•°æ®åº“å­˜å‚¨é€»è¾‘ï¼‰
        sticker_pack_id = str(uuid.uuid4())
        
        return JSONResponse(content={
            "success": True,
            "id": sticker_pack_id,
            "name": pack_name,
            "stickers": stickers_data,
            "imageFiles": imageFiles,
            "cover": stickers_data[0]["url"] if stickers_data else None
        })
    
    except HTTPException as he:
        raise he
    except Exception as e:
        logger.error(f"åˆ›å»ºè¡¨æƒ…åŒ…æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨é”™è¯¯: {str(e)}")

from py.qq_bot_manager import QQBotConfig, QQBotManager
# å…¨å±€æœºå™¨äººç®¡ç†å™¨
qq_bot_manager = QQBotManager()

@app.post("/start_qq_bot")
async def start_qq_bot(config: QQBotConfig):
    try:
        qq_bot_manager.start_bot(config)
        return {
            "success": True,
            "message": "QQæœºå™¨äººå·²æˆåŠŸå¯åŠ¨",
            "environment": "thread-based"
        }
    except Exception as e:
        logger.error(f"å¯åŠ¨QQæœºå™¨äººå¤±è´¥: {e}")
        return JSONResponse(
            status_code=400,  # æ”¹ä¸º 400 è¡¨ç¤ºå®¢æˆ·ç«¯é”™è¯¯
            content={
                "success": False, 
                "message": f"å¯åŠ¨å¤±è´¥: {str(e)}",
                "error_type": "startup_error"
            }
        )

@app.post("/stop_qq_bot")
async def stop_qq_bot():
    try:
        qq_bot_manager.stop_bot()
        return {"success": True, "message": "QQæœºå™¨äººå·²åœæ­¢"}
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": str(e)}
        )

@app.get("/qq_bot_status")
async def qq_bot_status():
    status = qq_bot_manager.get_status()
    # å¦‚æœæœ‰å¯åŠ¨é”™è¯¯ï¼Œåœ¨çŠ¶æ€ä¸­åŒ…å«é”™è¯¯ä¿¡æ¯
    if status.get("startup_error") and not status.get("is_running"):
        status["error_message"] = f"å¯åŠ¨å¤±è´¥: {status['startup_error']}"
    return status

@app.post("/reload_qq_bot")
async def reload_qq_bot(config: QQBotConfig):
    try:
        # å…ˆåœæ­¢å†å¯åŠ¨
        qq_bot_manager.stop_bot()
        await asyncio.sleep(1)  # ç­‰å¾…å®Œå…¨åœæ­¢
        qq_bot_manager.start_bot(config)
        
        return {
            "success": True,
            "message": "QQæœºå™¨äººå·²é‡æ–°åŠ è½½",
            "config_changed": True
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": str(e)}
        )

# å…¥å£æ–‡ä»¶éƒ¨åˆ†ä»£ç 

from py.feishu_bot_manager import FeishuBotConfig, FeishuBotManager

# å…¨å±€é£ä¹¦æœºå™¨äººç®¡ç†å™¨
feishu_bot_manager = FeishuBotManager()

@app.post("/start_feishu_bot")
async def start_feishu_bot(config: FeishuBotConfig):
    try:
        feishu_bot_manager.start_bot(config)
        return {
            "success": True,
            "message": "é£ä¹¦æœºå™¨äººå·²æˆåŠŸå¯åŠ¨",
            "environment": "thread-based"
        }
    except Exception as e:
        logger.error(f"å¯åŠ¨é£ä¹¦æœºå™¨äººå¤±è´¥: {e}")
        return JSONResponse(
            status_code=400,
            content={
                "success": False, 
                "message": f"å¯åŠ¨å¤±è´¥: {str(e)}",
                "error_type": "startup_error"
            }
        )

@app.post("/stop_feishu_bot")
async def stop_feishu_bot():
    try:
        feishu_bot_manager.stop_bot()
        return {"success": True, "message": "é£ä¹¦æœºå™¨äººå·²åœæ­¢"}
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": str(e)}
        )

@app.get("/feishu_bot_status")
async def feishu_bot_status():
    status = feishu_bot_manager.get_status()
    # å¦‚æœæœ‰å¯åŠ¨é”™è¯¯ï¼Œåœ¨çŠ¶æ€ä¸­åŒ…å«é”™è¯¯ä¿¡æ¯
    if status.get("startup_error") and not status.get("is_running"):
        status["error_message"] = f"å¯åŠ¨å¤±è´¥: {status['startup_error']}"
    return status

@app.post("/reload_feishu_bot")
async def reload_feishu_bot(config: FeishuBotConfig):
    try:
        # å…ˆåœæ­¢å†å¯åŠ¨
        feishu_bot_manager.stop_bot()
        await asyncio.sleep(1)  # ç­‰å¾…å®Œå…¨åœæ­¢
        feishu_bot_manager.start_bot(config)
        
        return {
            "success": True,
            "message": "é£ä¹¦æœºå™¨äººå·²é‡æ–°åŠ è½½",
            "config_changed": True
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": str(e)}
        )
    
from py.dingtalk_bot_manager import DingtalkBotConfig, DingtalkBotManager

# å…¨å±€é’‰é’‰æœºå™¨äººç®¡ç†å™¨
dingtalk_bot_manager = DingtalkBotManager()

# è·¯ç”± 1: å¯åŠ¨
@app.post("/start_dingtalk_bot")
async def start_dingtalk_bot(config: DingtalkBotConfig):
    try:
        dingtalk_bot_manager.start_bot(config)
        return {"success": True, "message": "é’‰é’‰æœºå™¨äººå·²æˆåŠŸå¯åŠ¨"}
    except Exception as e:
        return JSONResponse(status_code=400, content={"success": False, "message": str(e)})

# è·¯ç”± 2: åœæ­¢
@app.post("/stop_dingtalk_bot")
async def stop_dingtalk_bot():
    try:
        dingtalk_bot_manager.stop_bot()
        return {"success": True, "message": "é’‰é’‰æœºå™¨äººå·²åœæ­¢"}
    except Exception as e:
        return JSONResponse(status_code=500, content={"success": False, "message": str(e)})

# è·¯ç”± 3: çŠ¶æ€æ£€æŸ¥
@app.get("/dingtalk_bot_status")
async def dingtalk_bot_status():
    return dingtalk_bot_manager.get_status()

# è·¯ç”± 4: é‡è½½é…ç½®
@app.post("/reload_dingtalk_bot")
async def reload_dingtalk_bot(config: DingtalkBotConfig):
    try:
        dingtalk_bot_manager.stop_bot()
        time.sleep(1)
        dingtalk_bot_manager.start_bot(config)
        return {"success": True, "message": "é’‰é’‰æœºå™¨äººé…ç½®å·²é‡è½½"}
    except Exception as e:
        return JSONResponse(status_code=400, content={"success": False, "message": str(e)})

from py.discord_bot_manager import DiscordBotManager, DiscordBotConfig

discord_bot_manager = DiscordBotManager()

@app.post("/start_discord_bot")
async def start_discord_bot(config: DiscordBotConfig):
    try:
        discord_bot_manager.start_bot(config)
        return {"success": True, "message": "Discord æœºå™¨äººå·²å¯åŠ¨"}
    except Exception as e:
        return JSONResponse(status_code=400, content={"success": False, "message": str(e)})

@app.post("/stop_discord_bot")
async def stop_discord_bot():
    discord_bot_manager.stop_bot()
    return {"success": True, "message": "Discord æœºå™¨äººå·²åœæ­¢"}

@app.get("/discord_bot_status")
async def discord_bot_status():
    return discord_bot_manager.get_status()

@app.post("/reload_discord_bot")
async def reload_discord_bot(config: DiscordBotConfig):
    discord_bot_manager.stop_bot()
    await asyncio.sleep(1)
    discord_bot_manager.start_bot(config)
    return {"success": True, "message": "Discord æœºå™¨äººå·²é‡è½½"}


from py.slack_bot_manager import SlackBotManager, SlackBotConfig

slack_bot_manager = SlackBotManager()

@app.post("/start_slack_bot")
async def start_slack_bot(config: SlackBotConfig):
    try:
        slack_bot_manager.start_bot(config)
        return {"success": True, "message": "Slack æœºå™¨äººå·²å¯åŠ¨"}
    except Exception as e:
        return JSONResponse(status_code=400, content={"success": False, "message": str(e)})

@app.post("/stop_slack_bot")
async def stop_slack_bot():
    slack_bot_manager.stop_bot()
    return {"success": True, "message": "Slack æœºå™¨äººå·²åœæ­¢"}

@app.get("/slack_bot_status")
async def slack_bot_status():
    return slack_bot_manager.get_status()

@app.post("/reload_slack_bot")
async def reload_slack_bot(config: SlackBotConfig):
    slack_bot_manager.stop_bot()
    await asyncio.sleep(1)
    slack_bot_manager.start_bot(config)
    return {"success": True, "message": "Slack æœºå™¨äººå·²é‡è½½"}

from py.telegram_bot_manager import TelegramBotManager, TelegramBotConfig

# å…¨å±€ Telegram æœºå™¨äººç®¡ç†å™¨
telegram_bot_manager = TelegramBotManager()

@app.post("/start_telegram_bot")
async def start_telegram_bot(config: TelegramBotConfig):
    """
    å¯åŠ¨ Telegram æœºå™¨äººï¼ˆä¸é£ä¹¦æ¥å£å®Œå…¨å¯¹ç§°ï¼‰
    """
    try:
        telegram_bot_manager.start_bot(config)
        return {
            "success": True,
            "message": "Telegram æœºå™¨äººå·²æˆåŠŸå¯åŠ¨",
            "environment": "thread-based"
        }
    except Exception as e:
        logger.error(f"å¯åŠ¨ Telegram æœºå™¨äººå¤±è´¥: {e}")
        return JSONResponse(
            status_code=400,
            content={
                "success": False,
                "message": f"å¯åŠ¨å¤±è´¥: {str(e)}",
                "error_type": "startup_error"
            }
        )


@app.post("/stop_telegram_bot")
async def stop_telegram_bot():
    """
    åœæ­¢ Telegram æœºå™¨äºº
    """
    try:
        telegram_bot_manager.stop_bot()
        return {"success": True, "message": "Telegram æœºå™¨äººå·²åœæ­¢"}
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": str(e)}
        )


@app.get("/telegram_bot_status")
async def telegram_bot_status():
    """
    è·å– Telegram æœºå™¨äººçŠ¶æ€
    """
    status = telegram_bot_manager.get_status()
    if status.get("startup_error") and not status.get("is_running"):
        status["error_message"] = f"å¯åŠ¨å¤±è´¥: {status['startup_error']}"
    return status


@app.post("/reload_telegram_bot")
async def reload_telegram_bot(config: TelegramBotConfig):
    """
    é‡æ–°åŠ è½½ Telegram æœºå™¨äººï¼ˆå…ˆåœåå¯ï¼‰
    """
    try:
        telegram_bot_manager.stop_bot()
        await asyncio.sleep(1)  # ç­‰å¾…å®Œå…¨åœæ­¢
        telegram_bot_manager.start_bot(config)
        return {
            "success": True,
            "message": "Telegram æœºå™¨äººå·²é‡æ–°åŠ è½½",
            "config_changed": True
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": str(e)}
        )


@app.post("/add_workflow")
async def add_workflow(file: UploadFile = File(...), workflow_data: str = Form(...)):
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦ä¸º JSON
    if file.content_type != "application/json":
        raise HTTPException(
            status_code=400,
            detail="Only JSON files are allowed."
        )

    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼Œuuid.uuid4()ï¼Œæ²¡æœ‰è¿è¯ç¬¦
    unique_filename = str(uuid.uuid4()).replace('-', '')

    # æ‹¼æ¥æ–‡ä»¶è·¯å¾„
    file_path = os.path.join(UPLOAD_FILES_DIR, unique_filename + ".json")

    # ä¿å­˜æ–‡ä»¶
    try:
        with open(file_path, "wb") as buffer:
            buffer.write(await file.read())
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to save file: {str(e)}"
        )

    # è§£æ workflow_data
    workflow_data_dict = json.loads(workflow_data)

    # è¿”å›æ–‡ä»¶ä¿¡æ¯
    return JSONResponse(
        status_code=200,
        content={
            "success": True,
            "message": "File uploaded successfully",
            "file": {
                "unique_filename": unique_filename,
                "original_filename": file.filename,
                "url": f"/uploaded_files/{unique_filename}",
                "enabled": True,
                "text_input": workflow_data_dict.get("textInput"),
                "text_input_2": workflow_data_dict.get("textInput2"),
                "image_input": workflow_data_dict.get("imageInput"),
                "image_input_2": workflow_data_dict.get("imageInput2"),
                "seed_input": workflow_data_dict.get("seedInput"),
                "seed_input2": workflow_data_dict.get("seedInput2"),
                "description": workflow_data_dict.get("description")
            }
        }
    )

@app.delete("/delete_workflow/{filename}")
async def delete_workflow(filename: str):
    file_path = os.path.join(UPLOAD_FILES_DIR, filename + ".json")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="File not found")

    # åˆ é™¤æ–‡ä»¶
    try:
        os.remove(file_path)
        return JSONResponse(
            status_code=200,
            content={"success": True, "message": "File deleted successfully"}
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete file: {str(e)}"
        )

@app.get("/cur_language")
async def cur_language():
    settings = await load_settings()
    target_language = settings["currentLanguage"]
    return {"language": target_language}

@app.get("/vrm_config")
async def vrm_config():
    settings = await load_settings()
    return {"VRMConfig": settings.get("VRMConfig", {})}

from py.live_router import router as live_router, ws_router as live_ws_router

# 2. åˆ†åˆ«æŒ‚è½½
app.include_router(live_router)     # /api/live/*
app.include_router(live_ws_router)  # /ws/live/*


# ---------- å·¥å…· ----------
def get_dir(mid: str) -> str:
    return os.path.join(MEMORY_CACHE_DIR, mid)

def get_faiss_path(mid: str) -> str:
    return os.path.join(get_dir(mid), "agent-party.faiss")

def get_pkl_path(mid: str) -> str:
    return os.path.join(get_dir(mid), "agent-party.pkl")

def load_index_and_meta(mid: str):
    import faiss
    fpath, ppath = get_faiss_path(mid), get_pkl_path(mid)
    if not (os.path.exists(fpath) and os.path.exists(ppath)):
        raise HTTPException(status_code=404, detail="memory not found")
    index = faiss.read_index(fpath)
    with open(ppath, "rb") as f:
        raw = pickle.load(f)          # å¯èƒ½æ˜¯ tuple ä¹Ÿå¯èƒ½æ˜¯ dict
    # å…¼å®¹æ—§æ•°æ®ï¼šå¦‚æœæ˜¯ tuple å–ç¬¬ 0 ä¸ªï¼Œå¦åˆ™ç›´æ¥ç”¨
    meta_dict = raw[0] if isinstance(raw, tuple) else raw
    return index, meta_dict

def save_index_and_meta(mid: str, index, meta: List[Dict[Any, Any]]):
    import faiss
    faiss.write_index(index, get_faiss_path(mid))
    with open(get_pkl_path(mid), "wb") as f:
        pickle.dump(meta, f)


def fmt_iso8605_to_local(iso: str) -> str:
    """
    ISO-8601 -> æœåŠ¡å™¨æœ¬åœ°æ—¶åŒº yyyy-MM-dd HH:mm:ss
    """
    try:
        dt = datetime.fromisoformat(iso)      # è¯»å…¥ï¼ˆå¯èƒ½å¸¦æ—¶åŒºï¼‰
        dt = dt.astimezone()                  # è½åˆ°æœåŠ¡å™¨å½“å‰æ—¶åŒº
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        return iso        # è§£æå¤±è´¥å°±åŸæ ·è¿”å›


def flatten_records(meta: Dict[str, Any]) -> List[Dict[str, Any]]:
    flat = []
    for uuid, rec in meta.items():
        flat.append({
            "idx"        : len(flat),
            "uuid"       : uuid,
            "text"       : rec["data"],
            "created_at" : fmt_iso8605_to_local(rec["created_at"]),
            "timetamp"   : rec["timetamp"],
        })
    return flat


# æ–°å¢ï¼š dict â†” list äº’è½¬å·¥å…·
def dict_to_list(meta: Dict[str, Any]) -> List[Dict[str, Any]]:
    """æœ‰åºåŒ–ï¼Œä¿è¯é¡ºåºä¸ Faiss ç´¢å¼•ä¸€è‡´"""
    return [{uuid: rec} for uuid, rec in meta.items()]

def list_to_dict(meta_list: List[Dict[str, Any]]) -> Dict[str, Any]:
    """åˆ—è¡¨å†å‹å› dict"""
    new_meta = {}
    for item in meta_list:
        uuid, rec = next(iter(item.items()))
        new_meta[uuid] = rec
    return new_meta

# ---------- æ¨¡å‹ ----------
class TextUpdate(BaseModel):
    new_text: str

# ---------- 1. è¯»å–ï¼ˆå¹³é“ºï¼‰ ----------
@app.get("/memory/{memory_id}")
async def read_memory(memory_id: str) -> List[Dict[str, Any]]:
    _, meta_dict = load_index_and_meta(memory_id)   # æ‹†åŒ…
    return flatten_records(meta_dict)               # ä¼ å­—å…¸

# ---------- 2. ä¿®æ”¹ï¼ˆåªæ”¹ dataï¼‰ ----------
@app.put("/memory/{memory_id}/{idx}")
async def update_text(
    memory_id: str,
    idx: int,
    body: TextUpdate = Body(...)
) -> dict:
    index, meta_dict = load_index_and_meta(memory_id)
    meta_list = dict_to_list(meta_dict)
    if not (0 <= idx < len(meta_list)):
        raise HTTPException(status_code=404, detail="index out of range")
    # å®šä½ â†’ æ”¹ data
    uuid, rec = next(iter(meta_list[idx].items()))
    rec["data"] = body.new_text
    # å†™å›
    save_index_and_meta(memory_id, index, list_to_dict(meta_list))
    return {"message": "updated", "idx": idx}


# ---------- 3. åˆ é™¤ï¼ˆæŒ‰è¡Œå·ï¼‰ ----------
@app.delete("/memory/{memory_id}/{idx}")
async def delete_text(memory_id: str, idx: int) -> dict:
    import faiss
    import numpy as np
    index, meta_dict = load_index_and_meta(memory_id)
    meta_list = dict_to_list(meta_dict)
    if not (0 <= idx < len(meta_list)):
        raise HTTPException(status_code=404, detail="index out of range")

    ntotal = index.ntotal
    print("index.ntotal",index.ntotal)
    print("len(meta_list)",len(meta_list))
    if ntotal != len(meta_list):
        raise RuntimeError("index ä¸ meta é•¿åº¦ä¸ä¸€è‡´")

    # 1. é‡å»º Faiss ç´¢å¼•ï¼ˆå»æ‰ idxï¼‰
    ids_to_keep = np.array([i for i in range(ntotal) if i != idx], dtype=np.int64)
    vecs = np.vstack([index.reconstruct(i) for i in range(ntotal)])
    new_index = faiss.IndexFlatL2(index.d)   # è·Ÿä½ å»ºç´¢å¼•æ—¶ä¿æŒä¸€è‡´
    if vecs.shape[0] - 1 > 0:
        new_index.add(vecs[ids_to_keep].astype("float32"))

    # 2. åˆ é™¤åˆ—è¡¨å…ƒç´ 
    del meta_list[idx]

    # 3. è½ç›˜
    save_index_and_meta(memory_id, new_index, list_to_dict(meta_list))
    return {"message": "deleted", "idx": idx}

@app.get("/api/update_proxy")
async def update_proxy():
    try:
        settings = await load_settings()
        if settings:
            if settings["systemSettings"]["proxy"] and settings["systemSettings"]["proxyMode"] == "manual":
                # è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡
                os.environ['http_proxy'] = settings["systemSettings"]["proxy"].strip()
                os.environ['https_proxy'] = settings["systemSettings"]["proxy"].strip()
            elif settings["systemSettings"]["proxyMode"] == "system":
                os.environ.pop('http_proxy', None)
                os.environ.pop('https_proxy', None)
            else:
                os.environ['http_proxy'] = ""
                os.environ['https_proxy'] = ""
        return {"message": "Proxy updated successfully", "success": True}
    except Exception as e:
        return {"message": str(e), "success": False}

@app.get("/api/get_userfile")
async def get_userfile():
    try:
        userfile = USER_DATA_DIR
        return {"message": "Userfile loaded successfully", "userfile": userfile, "success": True}
    except Exception as e:
        return {"message": str(e), "success": False}

@app.get("/api/get_extfile")
async def get_extfile():
    try:
        extfile = EXT_DIR
        return {"message": "Extfile loaded successfully", "extfile": extfile, "success": True}
    except Exception as e:
        return {"message": str(e), "success": False}

def get_internal_ip():
    """è·å–æœ¬æœºå†…ç½‘ IP åœ°å€"""
    try:
        # åˆ›å»ºä¸€ä¸ª socket è¿æ¥ï¼Œç›®æ ‡å¯ä»¥æ˜¯ä»»ä½•å…¬ç½‘åœ°å€ï¼ˆä¸çœŸè¿æ¥ï¼‰ï¼Œåªæ˜¯ç”¨æ¥è·å–å‡ºå£ IP
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.settimeout(0)
        s.connect(("8.8.8.8", 80))  # ä½¿ç”¨ Google DNSï¼Œä¸å®é™…å‘é€æ•°æ®
        internal_ip = s.getsockname()[0]
        s.close()
        return internal_ip
    except Exception:
        return "127.0.0.1"

@app.get("/api/ip")
def get_ip():
    ip = get_internal_ip()
    return {"ip": ip}


settings_lock = asyncio.Lock()
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    active_connections.append(websocket)

    # [å…³é”®ç‚¹ 1] ä¸ºå½“å‰è¿æ¥ç”Ÿæˆå”¯ä¸€ID
    connection_id = str(shortuuid.ShortUUID().random(length=8))
    # æ ‡è®°è¯¥è¿æ¥æ˜¯å¦å‘é€è¿‡æç¤ºè¯ï¼ˆç”¨äºåˆ¤æ–­æ–­å¼€æ—¶æ˜¯å¦éœ€è¦å‘é€ç§»é™¤æŒ‡ä»¤ï¼‰
    has_sent_prompt = False
    has_start_tts = False

    try:
        async with settings_lock:
            current_settings = await load_settings()
            if current_settings.get("conversations", None):
                await save_covs({"conversations": current_settings["conversations"]})
                del current_settings["conversations"]
                await save_settings(current_settings)
            covs = await load_covs()
            current_settings["conversations"] = covs.get("conversations", [])
        
        await websocket.send_json({"type": "settings", "data": current_settings})
        
        while True:
            data = await websocket.receive_json()
            
            # --- å¸¸è§„é€»è¾‘ä¿æŒä¸å˜ ---
            if data.get("type") == "ping":
                await websocket.send_json({"type": "pong"})
            elif data.get("type") == "save_settings":
                await save_settings(data.get("data", {}))
                await websocket.send_json({
                    "type": "settings_saved",
                    "correlationId": data.get("correlationId"),
                    "success": True
                })
                for connection in [conn for conn in active_connections if conn != websocket]:
                    await connection.send_json({
                        "type": "settings_update",
                        "data": data.get("data", {})
                    })

            elif data.get("type") == "save_conversations":
                await save_covs(data.get("data", {}))
                await websocket.send_json({
                    "type": "conversations_saved",
                    "correlationId": data.get("correlationId"),
                    "success": True
                })
            elif data.get("type") == "get_settings":
                settings = await load_settings()
                if settings.get("conversations", None):
                    await save_covs({"conversations": settings["conversations"]})
                    del settings["conversations"]
                    await save_settings(settings)
                covs = await load_covs()
                settings["conversations"] = covs.get("conversations", [])
                await websocket.send_json({"type": "settings", "data": settings})
            elif data.get("type") == "save_agent":
                current_settings = await load_settings()
                agent_id = str(shortuuid.ShortUUID().random(length=8))
                config_path = os.path.join(AGENT_DIR, f"{agent_id}.json")
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(current_settings, f, indent=4, ensure_ascii=False)
                current_settings['agents'][agent_id] = {
                    "id": agent_id,
                    "name": data['data']['name'],
                    "system_prompt": data['data']['system_prompt'],
                    "config_path": config_path,
                    "enabled": False,
                }
                await save_settings(current_settings)
                await websocket.send_json({"type": "settings", "data": current_settings})
            
            elif data.get("type") == "set_user_input":
                user_input = data.get("data", {}).get("text", "")
                for connection in active_connections:
                    await connection.send_json({
                        "type": "update_user_input",
                        "data": {"text": user_input}
                    })

            # --- [å…³é”®ä¿®æ”¹] å¤„ç†æ‰©å±•é¡µé¢å‘é€çš„ç³»ç»Ÿæç¤º ---
            elif data.get("type") == "set_system_prompt":
                has_sent_prompt = True # æ ‡è®°è¯¥è¿æ¥ä¸ºæ‰©å±•æº
                extension_system_prompt = data.get("data", {}).get("text", "")
                
                # å¹¿æ’­æ—¶æºå¸¦ connection_id
                for connection in active_connections:
                    await connection.send_json({
                        "type": "update_system_prompt",
                        "data": {
                            "id": connection_id,      # è¿™é‡Œä¼ å…¥è¿æ¥ID
                            "text": extension_system_prompt
                        }
                    })

            elif data.get("type") == "set_tool_input":
                tool_input = data.get("data", {}).get("text", "")
                for connection in active_connections:
                    await connection.send_json({
                        "type": "update_tool_input",
                        "data": {"text": tool_input}
                    })
            # æŠŠæ–‡å­—ä¼ ç»™ä¸»ç•Œé¢TTSå¹¶æ’­æ”¾
            elif data.get("type") == "start_read":
                has_start_tts = True
                read_input = data.get("data", {}).get("text", "")
                for connection in active_connections:
                    await connection.send_json({
                        "type": "start_tts",
                        "data": {"text": read_input}
                    })

            # åœæ­¢ä¸»ç•Œé¢TTSå¹¶æ¸…ç©ºè¦æ’­æ”¾çš„å†…å®¹
            elif data.get("type") == "stop_read":
                for connection in active_connections:
                    await connection.send_json({
                        "type": "stop_tts",
                        "data": {}
                    })

            elif data.get("type") == "trigger_close_extension":
                for connection in active_connections:
                    await connection.send_json({
                        "type": "trigger_close_extension",
                        "data": {}
                    })

            elif data.get("type") == "trigger_send_message":
                for connection in active_connections:
                    await connection.send_json({
                        "type": "trigger_send_message",
                        "data": {}
                    })
                    
            elif data.get("type") == "trigger_clear_message":
                for connection in active_connections:
                    await connection.send_json({
                        "type": "trigger_clear_message",
                        "data": {}
                    })

            elif data.get("type") == "get_messages":
                for connection in active_connections:
                    await connection.send_json({
                        "type": "request_messages",
                        "data": {}
                    })

            elif data.get("type") == "broadcast_messages":
                messages_data = data.get("data", {})
                for connection in [conn for conn in active_connections if conn != websocket]:
                    await connection.send_json({
                        "type": "messages_update",
                        "data": messages_data
                    })

    except Exception as e:
        print(f"WebSocket error: {e}")
    finally:
        if websocket in active_connections:
            active_connections.remove(websocket)
        
        # --- [å…³é”®ä¿®æ”¹] è¿æ¥æ–­å¼€æ—¶çš„å¤„ç† ---
        # åªæœ‰å½“è¯¥è¿æ¥æ›¾ç»å‘é€è¿‡ update_system_prompt æ—¶æ‰è§¦å‘
        # é¿å…æ™®é€šå®¢æˆ·ç«¯æ–­å¼€æ—¶è¯¯åˆ å†…å®¹
        if has_sent_prompt:
            print(f"Extension {connection_id} disconnected. Removing prompt.")
            for connection in active_connections:
                try:
                    # å‘é€ç§»é™¤æŒ‡ä»¤ï¼Œåªæºå¸¦ ID
                    await connection.send_json({
                        "type": "remove_system_prompt",
                        "data": {
                            "id": connection_id 
                        }
                    })
                except Exception:
                    pass
        if has_start_tts:
            print(f"Extension {connection_id} disconnected. Removing tts.")
            for connection in active_connections:
                try:
                    # å‘é€ç§»é™¤æŒ‡ä»¤ï¼Œåªæºå¸¦ ID
                    await connection.send_json({
                        "type": "stop_tts",
                        "data": {}
                    })
                except Exception:
                    pass

from py.uv_api import router as uv_router
app.include_router(uv_router)

from py.node_api import router as node_router 
app.include_router(node_router)

from py.git_api import router as git_router
app.include_router(git_router)

from py.extensions import router as extensions_router

app.include_router(extensions_router)

from py.sherpa_model_manager import router as sherpa_model_router
app.include_router(sherpa_model_router)

from py.ebd_model_manager import router as ebd_model_router
app.include_router(ebd_model_router)

from py.minilm_router import router as minilm_router
app.include_router(minilm_router)

from py.ebd_api import router as embedding_router
app.include_router(embedding_router)

mcp = FastApiMCP(
    app,
    name="Agent party MCP - chat with multiple agents",
    include_operations=["get_agents", "chat_with_agent_party"],
)

mcp.mount()

app.mount("/vrm", StaticFiles(directory=DEFAULT_VRM_DIR), name="vrm")
app.mount("/tool_temp", StaticFiles(directory=TOOL_TEMP_DIR), name="tool_temp")
app.mount("/uploaded_files", StaticFiles(directory=UPLOAD_FILES_DIR), name="uploaded_files")
app.mount("/ext", StaticFiles(directory=EXT_DIR), name="ext")
app.mount("/", StaticFiles(directory=os.path.join(base_path, "static"), html=True), name="static")

# ç®€åŒ–mainå‡½æ•°
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        app,
        host=HOST,
        port=PORT
    )